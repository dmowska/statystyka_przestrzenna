[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statystyka przestrzenna",
    "section": "",
    "text": "O skrypcie",
    "crumbs": [
      "O skrypcie"
    ]
  },
  {
    "objectID": "index.html#struktura-skryptu",
    "href": "index.html#struktura-skryptu",
    "title": "Statystyka przestrzenna",
    "section": "Struktura skryptu",
    "text": "Struktura skryptu\nSkrypt zawiera materiały do ćwiczeń z przedmiotu “Statystyka przestrzenna” prowadzonego na III roku studiów inżynierskich Geoinformacja na Wydziale Nauk Geograficznych i Geologicznych UAM.\nSkrypt składa się z kilku części dotyczących:\n\npracy z danymi przestrzennymi w R\neksploracyjnej analizy danych\nanalizy przestrzennego rozkładu danych punktowych\nanalizy danych obszarowych\nanalizy danych ciągłych (geostatystyka)",
    "crumbs": [
      "O skrypcie"
    ]
  },
  {
    "objectID": "index.html#dane-wykorzystywane-w-skrypcie",
    "href": "index.html#dane-wykorzystywane-w-skrypcie",
    "title": "Statystyka przestrzenna",
    "section": "Dane wykorzystywane w skrypcie",
    "text": "Dane wykorzystywane w skrypcie\nDane wykorzystywane w skrypcie można pobrać TUTAJ",
    "crumbs": [
      "O skrypcie"
    ]
  },
  {
    "objectID": "index.html#zadania-samodzielne",
    "href": "index.html#zadania-samodzielne",
    "title": "Statystyka przestrzenna",
    "section": "Zadania samodzielne",
    "text": "Zadania samodzielne\nRozdział 14 w części ZADANIA zawiera zestawienie zadań do samodzielnego rozwiązania. Zadania te będą rozwiązywane na kolejnych ćwiczeniach.\nRozdział 15 zawiera opis projektu zaliczeniowego obejmującego materiał dotyczący eksploracyjnej analizy danych, analizy przestrzennego rozkładu danych punktowych oraz analizy danych obszarowych.",
    "crumbs": [
      "O skrypcie"
    ]
  },
  {
    "objectID": "index.html#wykorzystanie-skryptu-na-ćwiczeniach",
    "href": "index.html#wykorzystanie-skryptu-na-ćwiczeniach",
    "title": "Statystyka przestrzenna",
    "section": "Wykorzystanie skryptu na ćwiczeniach",
    "text": "Wykorzystanie skryptu na ćwiczeniach\n\nPrzed zajęciami należy zapoznać się z materiałem zamieszczonym w rozdziale zadanym na kolejne ćwiczenia.\nNa ćwiczeniach samodzielnie rozwiązywane będą zadania z rozdziału 14 odnoszące się do zadanej części materiału.",
    "crumbs": [
      "O skrypcie"
    ]
  },
  {
    "objectID": "01_wstep.html",
    "href": "01_wstep.html",
    "title": "WSTĘP",
    "section": "",
    "text": "Termin statystyka przestrzenna odnosi się do stosowania pojęć i metod statystycznych do danych, które mają wyraźną strukturę przestrzenną, która jest ważna dla zrozumienia tych danych (https://www.spatialanalysisonline.com/). Statystyka przestrzenna dostarcza narzędzi badawczych oraz podstaw teoretycznych związanych ze zbieraniem danych, analizą danych, oraz wnioskowaniem statystycznym dla danych przestrzennych (Suchecka 2014).\n\n\n\nGłówne nurty statystyki przestrzennej ze względu na rodzaj analizowanych danych\n\n\nZe względu na możliwości zastosowania metod statystycznych można wyróżnić trzy rodzaje danych przestrzennych (Cressie, 1993):\n\ndane punktowe (point data) - reprezentują wartości zmiennych mające charakter punktów w przestrzeni geograficznej;\ndane obszarowe (area data) - powstają poprzez podzielenie obszaru badań na jednostki przestrzenne, w których agregowane są wyniki, np. podział na jednostki administracyjne;\ndane powierzchniowe ciągłe przestrzennie (surface data, spatially continous data) - określane poprzez ciągłą zmienność na podstawie funkcji odległości (np. temperatura powietrza, opady atmosferyczne, występowanie złóż surowców naturalnych).\n\nZe względu na rodzaj analizowanych danych w statystyce przestrzennej można wyróżnić trzy główne nurty (Suchecka 2014):\n\nanalizę danych punktowych (ang. point pattern analysis) - stosuje się do określenia, w jaki sposób dane obiekty (zdarzenia) są rozmieszczone w przestrzeni (losowo, regularnie, skupione w klastry).\nmetody analizy danych obszarowych - stosuje się do oceny autokorelacji przestrzennej i sprawdzenia, czy obszary bliskie mają podobne, czy różne wartości.\ngeostatystykę - stosuje się do zrozumienie zmienności przestrzennej lub czasowej zjawisk ciągłych przestrzennie (np. rozkład opadów atmosferycznych, zanieczyszczeń powietrza).\n\nW kolejnych rozdziałach omówione zostaną metody wchodzące w skład w/w nurtów statystyki przestrzennej.",
    "crumbs": [
      "WSTĘP"
    ]
  },
  {
    "objectID": "02_wprowadzenie.html",
    "href": "02_wprowadzenie.html",
    "title": "1  Wprowadzenie",
    "section": "",
    "text": "1.1 Podstawowe pojęcia",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "02_wprowadzenie.html#podstawowe-pojęcia",
    "href": "02_wprowadzenie.html#podstawowe-pojęcia",
    "title": "1  Wprowadzenie",
    "section": "",
    "text": "1.1.1 Statystyka przestrzenna\n\nTermin statystyka przestrzenna odnosi się do stosowania pojęć i metod statystycznych do danych, które mają wyraźną strukturę przestrzenną, która jest ważna dla zrozumienia tych danych (https://www.spatialanalysisonline.com/)\nStatystyka przestrzenna dostarcza narzędzi badawczych oraz podstaw teoretycznych związanych ze zbieraniem danych, analizą danych, oraz wnioskowaniem danych dla danych przestrzennych (Suchecka 2014).\n\n\n\n1.1.2 Zależność przestrzenna\n\npodstawowa koncepcja analiz przestrzennych\npojęcie zależności przestrzennej (ang. spatial dependence) zostało ujęte przez Toblera (1970): “Wszystkie rzeczy są ze sobą powiązane, ale rzeczy bliskie są bardziej powiązane niż rzeczy odległe”.\nsformułowanie Toblera jest znane jako “Pierwsze prawo geografii” (ang. First Law of Geography)\n“wielkość” zależności przestrzenej może być mierzona za pomocą statystyk autokorelacji przestrzennej.\nzależność przestrzenna stanowi także podstawę metod stosowanych w ramach geostatystyki, która opisuje zmienność przestrzenną za pomocą funkcji pokazującej, jak autokorelacja przestrzenna maleje wraz ze wzrostem odległości.\n\n\n\n\n\n1.1.3 Autokorelacja przestrzenna\n\nŹródło: https://mgimond.github.io/Spatial/spatial-autocorrelation.html\n\nokreślana jako sytuacja, w której wystepowanie jednego zjawiska w jednej jednostce przestrzennej powoduje zmniejszenie lub zwiększenie się tego zjawiska w jednostkach sąsiednich (Bivand, 1980)\nmoże być obserowana jako klastry podobnych wartości lub systematyczny wzorzec przestrzenny (spatial pattern)\njest to konsekwencja występowania zależności przestrzennych - oznacza, że bliskie geograficznie obserwacje są bardziej do siebie podobne niż te odległe\npojęcie poraz pierwszy wprowadzone w 1968 roku przez Cliffa i Orda. Do tego czasu używano pojęć: zależność przestrzenna (spatial dependency), interakcje przestrzenne (spatial interactions), związek przestrzenny (spatial association)\npierwsze miary autokorelacji przestrzennej wprowadzone zostały w latach 50. XX w. w pracach Morana (1950) oraz Gearyego (1954), a następnie rozszerzone o miary lokalne w latach 90.",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "02_wprowadzenie.html#rozwój-metod-statystyki-przestrzennej",
    "href": "02_wprowadzenie.html#rozwój-metod-statystyki-przestrzennej",
    "title": "1  Wprowadzenie",
    "section": "1.2 Rozwój metod statystyki przestrzennej",
    "text": "1.2 Rozwój metod statystyki przestrzennej\n\nlata 30. XX w.\n\nzastosowanie teorii procesów punktowych w ramach analiz przestrzennych do analizy przestrzennego rozmieszczenia zjawisk (pierwsze zastosowanie: ekologia - badanie rozkładu gatunków drzew w przestrzeni)\npraca Stephana (1934): dane geograficzne są ze sobą powiązane jak kiście winogron, a nie oddzielne jak kule w urnie\neksperyment Fishera (1935): wyniki eksperymentu pokazały zbliżone poziomy wydajności z upraw pól leżących obok siebie.\n\n\n\n\nlata 50. XX w. (Moran, 1950, Geary 1954) -\n\nopracowanie obecnie stosowanych testów statystycznych do weryfikacji istotności autokorelacji przestrzennej wartości atrybutów dla regularnych obiektów obszarowych (regular lattice system). Testy te służą do weryfikacji hipotezy zerowej o braku autokorelacji przestrzennej (braku struktury przestrzennej), wobec nieokreślonej hipotezy alternatywnej\nwyprowadzone statystyki autokorelacji przestrzennej były określane miernikami sąsiedztwa\n\n\n\n\nlata 70. XX w.\n\ndynamiczny rozwój metod statytyki przestrzennej\ndwa kierunki badań:\n\nrozwój podstaw teorii statystyki dla danych przestrzennych\nrozwój nowych metod i technik badawczych związanych głównie z pojawieniem się nowych technik komputerowych\n\nwprowadzenie terminu autokorelacji przestrzennej przez Cliffa i Orda (Monografia Spatial autocorrelation, 1973)\n\n\n\n\nlata 90. XX w\n\nposzerzenie globalnych miar autokorelacji przestrzennej o miary lokalne które wyznacza się oddzielnie dla każdego obszaru (statystykę G Getisa [Getis, Ord, 1992], statystykę LISA (Local Indicator of Spatial Autocorrelation) [Anselin, 1994] oraz statystykę Ci Geary’ego.)",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "02_wprowadzenie.html#statystyka-przestrzenna-a-inne-dyscypliny",
    "href": "02_wprowadzenie.html#statystyka-przestrzenna-a-inne-dyscypliny",
    "title": "1  Wprowadzenie",
    "section": "1.3 Statystyka przestrzenna a inne dyscypliny",
    "text": "1.3 Statystyka przestrzenna a inne dyscypliny\nStatystyka przestrzenna jest odrębną dyscypliną wywodzącą się z analiz przestrzennych oraz ściśle związaną z tradycyjnymi metodami statystycznymi i nowocześniejszą statystyką obliczeniową (ang. computational statistics).\nStatystyka przestrzenna dostarcza formalnych narzędzi statystycznych w ramach szerszego procesu badania danych przestrzennych.\nMetody statystyki przestrzennej są także implementowane w ramach oprogramowania GIS:\n\nQGIS: https://docs.qgis.org/3.34/en/docs/training_manual/vector_analysis/spatial_statistics.html\nArcGIS: https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/an-overview-of-the-spatial-statistics-toolbox.htm\n\n\n1.3.1 Statystyka przestrzenna a analizy przestrzenne i GIS\n\n\n\n\n\n\n\n\n\nOpis\n\n\n\n\nSystemy Informacji Geograficznej (GIS)\n\nSystem/platforma do tworzenia, gromadzenia, zarządzania, analizowania i wizualizacji różnych typów danych\nDostarcza podstawowych narzędzi i technologii do pracy z danymi przestrzennymi, w tym do ich tworzenia, edycji i przechowywania.\n\n\n\nAnalizy przestrzenne\n\nDziedzina obejmująca szeroki zestaw technik wykorzystywanych do wizualizacji, przetwarzania i analizy danych przestrzennych.\nProces wykorzystywania narzędzi i technologii GIS do rozwiązywania problemów przestrzennych i zrozumienia danych przestrzennych.\nObejmuje wiele metod i narzędzi, takich jak nakładanie, selekcja obiektów na podstawie atrybutów, buforowanie, agregacja danych, obliczanie odległości, algebra map.\n\nZastosowanie:\n\nokreślania relacji przestrzennych (bliskość, przecięcie, nakładanie się, dostępność)\nanalizy sieciowe: znajdowania najkrótszej/najszybszej drogi\nznajdowania najlepszych lokalizacji (na podstawie dowolnego zestawu kryteriów opartych o atrybuty danych przestrzennych lub wykorzystując unikalne właściwości danych przestrzennych, takie jak odległość lub relacje z innymi miejscami.)\n\n\n\nStatystyka przestrzenna\n\nWyspecjalizowany podzbiór metod analizy przestrzennej, który koncentruje się na stosowaniu pojęć i metod statystycznych do danych przestrzennych.\nUwzględnia ona zależności przestrzenne, heterogeniczność przestrzenną oraz położenie geograficzne obserwacji w celu modelowania wzorców i relacji za pomocą narzędzi takich jak autokorelacja przestrzenna, interpolacja przestrzenna, geostatystyka i regresja przestrzenna.\nMetody statystyki przestrzennej mogą być zaimplementowane jako rozszerzenia oprogramownia GIS.\n\nZastosowanie:\n\nwykrywanie i kwantyfikacja wzorców w rozkładzie przestrzennym danych,\nwykrywanie obszarów koncentracjia zjawisk poprzez wyszukanie statystycznie istotnych skupisk wysokich lub niskich wartości)\nprognozowania - szacowanie wartości dla całego obszaru (dane ciągłe) na podstawie danych punktowych\n\n\n\n\n\n\n1.3.2 Statystyka przestrzenna a statystyka\n\nStatystyka przestrzenna w odróżnieniu od klasycznej statystyki umożliwia analizę danych geograficzny oraz informacji zlokalizowanych w przestrzeni.\nStatystyka przestrzenna i klasyczna statystyka różnią się jednym zasadniczym założeniem:\n\nKlasyczna statystyka: zakłada niezależność obserwacji, tzn. elementy próby sa dobierane niezależnie od siebie (tj. losowo)\nStatystyka przestrzenna: założenie o niezależności obserwacji nie jest spełnione. Zgodnie z regułą Toblera (nazywaną też pierwszym prawem geografii) obieky sąsiadające w przestrzeni/czasie są do siebie bardziej podobne niż obiekty dalej od siebie położone. Oznacza to, że próbkowanie w przestrzeni/czasie może nie mieć charakteru losowego.\n\n\n\n1.3.2.1 Analogie między statystyką a statystyką przestrzenną\nMetodologia badań statystyki przestrzennej w większości powstała na podstawie metod statystyki klasycznej. W ramach statystyki przestrzennej także wyróżnia się statystykę opisową oraz wnioskowanie statystyczne.\n\nŹródło: Suchecka (2014)\n\n\n\n\n\n\nKlasyczna statystyka\nStatystyka przestrzenna\n\n\n\n\nStatystyka opisowa\nPrzestrzenne statystyki opisowe: średnia centralna, mediana centralna, odległość standardowa, względna odległość\n\n\nKorelacja\nAutokorelacja przestrzenna: statystyki join-count, Morana I, Geary’ego C, Getisa-Orda\n\n\nPredykcja liniowa\nInterpolacja przestrzenna: np. IDW, kriging\n\n\nProcesy punktowe\nAnaliza przestrzennych danych punktowych (SPPA): model kompletnej losowości przestrzennej CSR, metoda najbliższego sąsiada, metoda funkcji K, estymacja jądrowa\n\n\n\n\n\n\n1.3.3 Statystyka przestrzenna a ekonometria przestrzenna\n\nŹródło: Suchecka (2014)\n\n\n\n\n\n\nStatystyka przestrzenna\nEkonometria przestrzenna\n\n\n\n\nopiera się na analizie danych odnoszących się do różnych dyscyplin\nopiera się na modelowaniu, zajmuje się specyfikacją, estymacją i weryfikacją modeli uwzględniajacych aspekt przestrzenny\n\n\nwiększość opracowań z zakresu statystyki przestrzennej dotyczy zjawisk w dziedzinie biologii, ekologii, epidemiologii, geologii\nanaliza przestrzennych danych ekonomicznych oraz związanych z nimi problemów\nopisuje przede wszystkim procesy ekonomiczne na poziomie regionalnym",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "03_zastosowanie.html",
    "href": "03_zastosowanie.html",
    "title": "2  Zastosowania statystyki przestrzennej",
    "section": "",
    "text": "2.1 Główne nurty metodologiczne w statystyce przestrzennej\nZe względu na rodzaj analizowanych danych w statystyce przestrzennej można wyróżnić trzy główne nurty:\nGriffith (1999) wśród metod statystyki przestrzennej wyróżnia:",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zastosowania statystyki przestrzennej</span>"
    ]
  },
  {
    "objectID": "03_zastosowanie.html#główne-nurty-metodologiczne-w-statystyce-przestrzennej",
    "href": "03_zastosowanie.html#główne-nurty-metodologiczne-w-statystyce-przestrzennej",
    "title": "2  Zastosowania statystyki przestrzennej",
    "section": "",
    "text": "analizę danych punktowych (ang. point pattern analysis)\ngeostatystykę\nmetody analizy danych obszarowych i punktowych atrybutowych.\n\n\n\nanalizę danych punktowych\ngeostatystykę\nautoregresję przestrzenną\nmetody wizualizacji danych",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zastosowania statystyki przestrzennej</span>"
    ]
  },
  {
    "objectID": "03_zastosowanie.html#analiza-danych-punktowych-bezatybutowych",
    "href": "03_zastosowanie.html#analiza-danych-punktowych-bezatybutowych",
    "title": "2  Zastosowania statystyki przestrzennej",
    "section": "2.2 Analiza danych punktowych (bezatybutowych)",
    "text": "2.2 Analiza danych punktowych (bezatybutowych)\n\n2.2.1 Cel\n\nokreślenie w jaki sposób dane obiekty/zdarzenia są rozmieszczone w przestrzeni.\n\nCzy punkty są rozmieszczone losowo? (tzn. na ich rozmieszczenie nie działa żaden czynnik lub zależą od wielu czynników które się wzajemnie znoszą)\nCzy punkty są romieszczone regularnie? (tj. efekt świafomego działania, nie jest to proces naturalny)\nCzy występują skupiska (klastry) punktów? (tzn. na rozmieszczenie punktów wpływa jakiś czynnik)\n\nrozpoznanie wzorca rozmieszczenia punktów w przestrzeni jest podstawą do wykrycia zależności przestrzennych.\n\n\n\n2.2.2 Dane punktowe\nPrzykładami danych punktowych bezatrybutowych są:\n\ndane epidemiologiczne dotyczące występowania przypadków zachorowań\nrozkłady występowania określonych gatunków roślin czy zwierząt.\n\n\n\n2.2.3 Przykłady danych punktowych: zbiory danych z pakietu spatstat\n\n\n\n\n2.2.4 Zastosowanie\nAnaliza przestrzennych danych punktowych jest stosowana w wielu dziedzinach:\n\nbiologia, ekologia, botanika -\n\nrozmieszczenie gatunków roślin,\nrozmieszczenie gatunków ptaków i ich lęgowisk\nbadanie rozkładu gatunków drzew w przestrzeni (badanie rozmieszczenia roślin stało się pierwowzorem technik analizy przestrzennych danych punktowych)\n\nepidemiologia - rozprzestrzenianie się chorób (wirusów, bakterii)\nochrona zdrowia - obszary występowania chorób zakaźnych, nowotworów\nnauki ekonomiczne - przestrzenne zróżnicowanie zjawisk ekonomicznych, np. dochodów, bezrobocia\narcheologia\n\nzróżnicowanie lokalizacji stanowisk archeologicznych w kontekście rozwoju społeczno-środowiskowego\nanaliza wzorców stanowisk stanowi mocną metodę zrozumienia preferencji lokalizacji stanowisk w archeologii.\n\ngeografia\n\nrozprzestrzenianie się pożarów",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zastosowania statystyki przestrzennej</span>"
    ]
  },
  {
    "objectID": "03_zastosowanie.html#geostatystyka",
    "href": "03_zastosowanie.html#geostatystyka",
    "title": "2  Zastosowania statystyki przestrzennej",
    "section": "2.3 Geostatystyka",
    "text": "2.3 Geostatystyka\n\nGeostatystyka to gałąź statystyki skupiająca się na przestrzennych lub czasoprzestrzennych zbiorach danych.\nUwzględnia w analizie przestrzenną i czasową lokalizację.\nPodstawą opisu struktury przestrzennej jest semiwariogram - funkcja określająca zależność pomiędzy średnim zróżnicowaniem wartości analizowanej cechy i odległością między punktami pomiaru (zakłada się, że podobieństwo wartości maleje wraz z odleglością).\nPoczątki geostatystyki: lata 60.XX w. (francuski matematyk G. Matheron w 1963 roku zaproponował metodę krigingu opartą mająca na celu optymalizacje szacowania parametrów geologicznych).\n\n\n2.3.1 Cel\n\nzrozumienie zmienności przestrzennej lub czasowej zjawiska\nszacowanie wartości dla całego obszaru (dane ciągłe) na podstawie danych punktowych (estymacja)\nokreślenie prawdopodobieństwa przekroczenia w danym punkcie lub obszarze wartości progowej\nOptymalizacja próbkowania oraz sieci monitoringowych\n\n\n\n2.3.2 Przykłady zastosowania\nModelowanie zmienności przestrzennej opadów na podstawie zbioru danych sic97 (Spatial Interpolation Comparison 1997 data set: Swiss Rainfall) z pakietu gstat przedstawiającego 100 pomiarów wysokości opadów w obszarze Szwajcarii.\n\nModelowanie zmienności przestrzennej zanieczyszczenia gleb (zmienna Co) na podstawie zbioru danych jura z pakietu gstat. Zbiór danych przedstawia pomiary zanieczyszczenia gleby metalami ciężkimi w 259 lokalizacjach. Zbiór danych jura pochodzi z ksiązki Pierre Goovaerts’ (Goovaerts, P. 1997. Geostatistics for Natural Resources Evaluation. Oxford Univ. Press, New-York, 483)\n\nModelowanie zmienności przestrzennej zanieczyszczenia powietrza w Kalifornii (średnia zawartość Ozonu w latach 1980-2009) na podstawie zbioru danych airqual z pakietu rspat.\n\n\n\n2.3.3 Zastosowanie geostatystyki\nGeostatystyka jest stosowana obecnie w wielu dyscyplinach, takich jak: geologia naftowa, oceanografia, geochemia,logistyka, leśnictwo, gleboznawstwo, hydrologia, meteorologia, epidemiologia.\n\nGórnictwo, geologia, eksploracja zasobów naturalnych:\n\nszacowanie rezerw mineralnych,\ncharakterystyki złoża,\noptymalizacji miejsc wierceń.\nhttps://geojournals.pgi.gov.pl/bp/article/view/28824/pdf\n\nZarządzanie kryzysowe\n\nocena ryzyka związanego z klęskami żywiołowymi, takimi jak trzęsienia ziemi, osuwiska i powodzie, w celu opracowania strategii zarządzania katastrofami i łagodzenia ryzyka.\n\nRolnictwo\n\nanalizę jakości gleby,\nprognozowanie plonów\nprecyzyjne rolnictwo poprzez pomiar zmienności przestrzennej właściwości gleby.\n\nGeografia\n\nocena zanieczyszczeń, np. mapy zanieczyszczeń powietrza.\n\nHydrologia\n\nmodelowanie głębokości wód gruntowych,\nprognozowanie występowania opadów\nocena ryzyka powodzi poprzez analizę wzorców przestrzennych zmiennych hydrologicznych.\n\nEpidemiologia i geografia zdrowia\n\nbadanie przestrzennego rozmieszczenia chorób,\nanaliza dostępu do opieki zdrowotnej i\nprzewidywania wybuchów chorób w różnych lokalizacjach.\n\nUbezpieczenia i finanse:\n\nwykorzystywane w branży ubezpieczeniowej do oceny i zarządzania ryzykiem związanym z klęskami żywiołowymi lub rozprzestrzenianiem się chorób w celu decydowania o odpowiednich składkach i skutecznego zarządzania portfelami.\n\n\nhttps://desktop.arcgis.com/en/arcmap/latest/extensions/geostatistical-analyst/geostatistical-analyst-example-applications.htm",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zastosowania statystyki przestrzennej</span>"
    ]
  },
  {
    "objectID": "03_zastosowanie.html#autokorelacja-przestrzenna-danych-obszarowych",
    "href": "03_zastosowanie.html#autokorelacja-przestrzenna-danych-obszarowych",
    "title": "2  Zastosowania statystyki przestrzennej",
    "section": "2.4 Autokorelacja przestrzenna danych obszarowych",
    "text": "2.4 Autokorelacja przestrzenna danych obszarowych\n\n2.4.1 Cel\n\nanaliza podobieństwa i różnic między obszarami/regionami\nwyszukanie obszarów podobnych do siebie\nznalezienie obszarów znacząco różnych od sąsiednich\nkluczowe zadanie: określenie, które obszary ze sobą sąsiadują.\n\n\n\n2.4.2 Przykład zastosowania",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Zastosowania statystyki przestrzennej</span>"
    ]
  },
  {
    "objectID": "04_daneprzestrzenne.html",
    "href": "04_daneprzestrzenne.html",
    "title": "3  Dane przestrzenne w R",
    "section": "",
    "text": "3.1 Dane przestrzenne - pakiety\nR jest językiem programowania oraz środowiskiem obliczeniowym pozwalającym na analizę statystyczną danych oraz wizualizację danych. R dostarcza także wielu funkcji do pracy z danymi przestrzennymi. Poniżej przedstawione zostały podstawowe informacje dotyczące pracy z danymi przestrzennymi w R. Więcej informacji znajduje się w książce https://r.geocompx.org/\nR zawiera wiele funkcji pozwalających na przetwarzanie, wizualizację i analizowanie danych przestrzennych. Funkcje te zawarte są w pakietach:\nWięcej szczegółów na temat pakietów R służących do analizy przestrzennej można znaleźć pod adresem https://cran.r-project.org/view=Spatial.",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dane przestrzenne w R</span>"
    ]
  },
  {
    "objectID": "04_daneprzestrzenne.html#dane-przestrzenne---pakiety",
    "href": "04_daneprzestrzenne.html#dane-przestrzenne---pakiety",
    "title": "3  Dane przestrzenne w R",
    "section": "",
    "text": "obsługa danych przestrzennych: sf, terra, stars, rastrer, sp\nwizualizacja danych przestrzennych: tmap\nstatystyka przestrzenna\n\nstatystyka punktów: spatstat, sfdep\nstatystyka sieci: sfnetworks, tidygraph\nstatystyka powierzchni (dane poligonowe): sfdep\ngeostatystyka: gstat\n\n\n\n\n3.1.1 Pakiet sf\n\noparty jest o standard OGC o nazwie Simple Features (https://r-spatial.github.io/sf/articles/sf1.html);\ndefiniuje klasę obiektów sf, która jest sposobem reprezentacji przestrzennych danych wektorowych. Pozwala on na stosowanie dodatkowych typów danych wektorowych (np. poligon i multipoligon to dwie oddzielne klasy), łatwiejsze przetwarzanie danych, oraz obsługę przestrzennych baz danych takich jak PostGIS;\njest używany przez kilkadziesiąt dodatkowych pakietów R;\nwiększość funkcji zawartych w pakiecie zaczyna się od prefiksu st_;\nwięcej informacji:\n\nhttps://cran.r-project.org/web/packages/sf/index.html\nhttps://r-spatial.github.io/sf/\n\n\n\n\n3.1.2 Pakiet terra\n\ndostarcza funkcji do analizy danych przestrzennych w formacie rastrowym oraz wektorowym;\nmetody dla danych wektorowych obejmują operacje geometryczne, takie jak intersect i buffer;\nmetody analizy danych rastrowych obejmują funkcje lokalne, sąsiedztwa, globalne, strefowe oraz operacje geometryczne;\nwięcej informacji:\n\nhttps://github.com/rspatial/terra\nhttps://rspatial.org/pkg/index.html\nhttps://cran.r-project.org/web/packages/terra/index.html\n\n\n\n\n3.1.3 Pakiet tmap\n\npakiet do wizualizacji danych przestrzennych;\nsposób tworzenia map jest podobny do składni stosowanej w pakiecie ggplot2, została jednak dostosowana do tworzenia map a nie wykresów;\nwięcej informacji:\n\nhttps://r-tmap.github.io/tmap/index.html\nhttps://r-tmap.github.io/tmap/articles/tmap-getstarted.html\nhttps://github.com/r-tmap/tmap\nhttps://cran.r-project.org/web/packages/tmap/\n\n\n\n\n3.1.4 Biblioteki zewnętrzne\n\nGDAL/OGR\n\nGDAL to biblioteka zawierająca funkcje służące do odczytywania i zapisywania danych w formatach rastrowych.\nOGR to biblioteka służąca to odczytywania i zapisywania danych w formatach wektorowych.\nWykorzytywana przez pakiety R do wczytywania i zapisywania danych przestrzennych.\n\nGEOS\n\nBiblioteka GEOS jest używana przez pakiety R (np. sf) do wykonywania operacji przestrzennych.\nPrzykładowe funkcje tej biblioteki to tworzenie buforów, wyliczanie centroidów, określanie relacji topologicznych (np. przecina, zawiera, etc.) i wiele innych.\n\nPROJ\n\nBiblioteka PROJ jest używana w R do określania i konwersji układów współrzędnych.\nBardzo popularnym zapisem układu współrzędnych jest wykorzytanie systemu kodów EPSG (ang. European Petroleum Survey Group), który pozwala on na łatwe identyfikowanie układów współrzędnych. Przykładowo, układ PL 1992 może być określony jako “EPSG:2180”.\nDane przestrzenne moga też przechowywać opis układu współrzędnych w bardziej złożonej formie - tzw. WKT2.\nStrona https://epsg.org/ zawiera bazę danych układów współrzędnych.",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dane przestrzenne w R</span>"
    ]
  },
  {
    "objectID": "04_daneprzestrzenne.html#reprezentacja-danych-przestrzennych-w-r",
    "href": "04_daneprzestrzenne.html#reprezentacja-danych-przestrzennych-w-r",
    "title": "3  Dane przestrzenne w R",
    "section": "3.2 Reprezentacja danych przestrzennych w R",
    "text": "3.2 Reprezentacja danych przestrzennych w R\nDane przestrzenne mogą być reprezentowane w R poprzez wiele różnych klas obiektów z użyciem różnych pakietów R:\n\ndane wektorowe:\n\npakiet sf: klasy sf\npakiet terra: klasa spatVector\n\ndane rastrowe:\n\npakiet terra: klasa spatRaster",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dane przestrzenne w R</span>"
    ]
  },
  {
    "objectID": "04_daneprzestrzenne.html#import-i-eksport-danych",
    "href": "04_daneprzestrzenne.html#import-i-eksport-danych",
    "title": "3  Dane przestrzenne w R",
    "section": "3.3 Import i eksport danych",
    "text": "3.3 Import i eksport danych\nR pozwala na odczytywanie danych przestrzennych z wielu formatów. Do najpopularniejszych należą:\n\ndane tekstowe z plików o rozszerzeniu .csv,\ndane wektorowe z plików .shp,\ndane rastrowe z plików w formacie GeoTIFF,\nbazy danych przestrzennych z plików rozszerzeniu .gpkg. Geopaczka (geopackage) może przechowywać zarówno pliki tekstowe, wektorowe oraz rastrowe.\n\n\n3.3.1 Format .csv (dane punktowe)\nDane z plików tekstowych (np. rozszerzenie .csv) można odczytać za pomocą uogólnionej funkcji read.table() lub też funkcji szczegółowych - read.csv() lub read.csv2().\n\npunkty = read.csv(\"data/punkty.csv\")\n\n\nhead(punkty)\n\n      srtm clc      temp      ndvi      savi        x        y\n1 175.7430   1 13.852222 0.6158061 0.4189449 750298.0 716731.6\n2 149.8111   1 15.484209 0.5558816 0.3794864 753482.9 717331.4\n3 272.8583  NA 12.760814 0.6067462 0.3745572 747242.5 720589.0\n4 187.2777   1 14.324648 0.3756170 0.2386246 755798.9 718828.1\n5 260.1366   1 15.908549 0.4598393 0.3087599 746963.5 717533.5\n6 160.1416   2  9.941118 0.5600288 0.3453627 756801.6 720474.1\n\n\nPo wczytaniu pliku tekstowego za pomocą funkcji read.csv(), nowy obiekt (np. punkty) jest reprezentowany za pomocą klasy nieprzestrzennej data.frame. Aby obiekt został przetworzony do klasy przestrzennej, konieczne jest określenie które kolumny zawierają informacje o współrzędnych (w tym przykładzie są to kolumny x oraz y).\nDo przekształcenia obiektu typu data.frame do obiektu przestrzennego klasy sf wykorzystuje się funkcję st_as_sf() z pakietu sf. Funkcja st_as_sf() wymaga zdefniowania obiektu zawierającego dane (w tym wypadku punkty), nazw kolumn zawierających współrzędne x oraz y (coords = c(“x”, “y”)). Warto także na tym etapie zdefiniować układ współrzędnych (crs = “EPSG:2180”).\n\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE\n\npunkty_sf = st_as_sf(punkty, coords = c(\"x\", \"y\"), crs = \"EPSG:2180\")\npunkty_sf\n\nSimple feature collection with 248 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 745592.5 ymin: 712642.4 xmax: 756967.8 ymax: 721238.7\nProjected CRS: ETRF2000-PL / CS92\nFirst 10 features:\n       srtm clc      temp      ndvi      savi                  geometry\n1  175.7430   1 13.852222 0.6158061 0.4189449   POINT (750298 716731.6)\n2  149.8111   1 15.484209 0.5558816 0.3794864 POINT (753482.9 717331.4)\n3  272.8583  NA 12.760814 0.6067462 0.3745572   POINT (747242.5 720589)\n4  187.2777   1 14.324648 0.3756170 0.2386246 POINT (755798.9 718828.1)\n5  260.1366   1 15.908549 0.4598393 0.3087599 POINT (746963.5 717533.5)\n6  160.1416   2  9.941118 0.5600288 0.3453627 POINT (756801.6 720474.1)\n7  192.9223   2 13.514751 0.6009588 0.3737249   POINT (752698 718623.6)\n8  234.9989   1 12.919225 0.4979195 0.2978503 POINT (749399.8 716955.2)\n9  228.7312   1 19.247132 0.3819634 0.2077981   POINT (748766.6 713889)\n10 240.5474   2 10.529105 0.5795159 0.3805268   POINT (749290.4 718861)\n\n\nNależy zwrócić uwagę, że po przekształceniu danych wczytanych z pliku tekstowego (punkty.csv) na obiekt przestrzenny (punkty_sf) w tabeli atrybutów nie ma już kolumn “x” oraz “y”. Obiekt przestrzenny zawiera natomiast kolumnę geometry z informacją o współrzędnych punktów. Ponadto po wyświetleniu obiektu punkty_sf w nagłówku mamy podane informacje o układzie współrzędnych, zasięgu danych (obwiedni, bounding box).\nProstą wizualizacje danych klasy przestrzennej można uzyskać za pomocą funkcji plot().\n\nplot(punkty_sf)\n\n\n\n\n\n\n\n\nFunkcja plot() wyświetli wszystkie zmienne. Wyświetlenie wybranej zmiennej wymaga zdefiniowania jej nazwy.\n\nplot(punkty_sf[\"temp\"])\n\n\n\n\n\n\n\n\n\n\n3.3.2 Import i eksport danych wektorowych\nDane przestrzenne zapisane w pliku (.gpkg, .shp) można wczytać do R wykorzystując funkcje read_sf() z pakietu sf.\n\nlibrary(sf)\npowiaty_wlkp = read_sf(\"data/wlkp_powiaty.gpkg\")\n\nJeśli geopaczka zawiera kilka warstw na etapie wczytywania pliku można podać także nazwę warstwy.\n\nlibrary(sf)\npowiaty_wlkp = read_sf(\"data/wlkp_powiaty.gpkg\", layer = \"powiaty\")\n\nDane przestrzenne można zapisać do pliku wykorzystując funkcję write_sf() z pakietu sf (funkcja ta nadpisuje plik). Dane mogą być zapisane do różnych formatów (np. geopaczka lub plik ESRI Shapefile).\n\n#create output directory out \ndir.create(\"out\")\nwrite_sf(powiaty_wlkp, dsn = \"out/out_powiaty_wlkp.shp\")\n\nR pozwala także na zapisanie kilku warstw do istniejącej geopaczki. Użyjemy w tym celu funkcji st_write(), która automatycznie nie nadpisuje pliku. W przypadku, gdy chcemy stworzyć geopaczkę składająca się z kilku warstw na etapie zapisu należy podać także nazwę warstwy (argument layer). Argument delete_dsn = TRUE usuwa istniejący wcześniej plik.\n\nst_write(powiaty_wlkp, dsn = \"out/out_powiaty_wlkp.gpkg\", layer = \"powiaty2\", delete_dsn = TRUE)\n\nDeleting source `out/out_powiaty_wlkp.gpkg' using driver `GPKG'\nWriting layer `powiaty2' to data source \n  `out/out_powiaty_wlkp.gpkg' using driver `GPKG'\nWriting 35 features with 2 fields and geometry type Multi Polygon.\n\nst_write(powiaty_wlkp, dsn = \"out/out_powiaty_wlkp.gpkg\", layer = \"powiaty3\")\n\nWriting layer `powiaty3' to data source \n  `out/out_powiaty_wlkp.gpkg' using driver `GPKG'\nWriting 35 features with 2 fields and geometry type Multi Polygon.\n\n\nAby dowiedzieć się, jakie warstwy są zapisane w geopaczce można użyć funkcji st_layers().\n\nst_layers(\"out/out_powiaty_wlkp.gpkg\")\n\nDriver: GPKG \nAvailable layers:\n  layer_name geometry_type features fields           crs_name\n1   powiaty2 Multi Polygon       35      2 ETRF2000-PL / CS92\n2   powiaty3 Multi Polygon       35      2 ETRF2000-PL / CS92\n\n\n\n\n3.3.3 Import i eksport danych rastrowych\nDo obsługi danych rastrowych zostanie wykorzystany pakiet terra. Funkcja rast() pozwala na wczytanie danych rastrowych do klasy SpatRaster. Funkcja ta obsługuje zarówno jednokanałowe jak i wielokanałowe (wielowarstwowe) pliki rastrowe. Przykładem pliku rastrowego, w którym zostało zapisanych kilka kanałów - warstw jest plik siatka-all.tif.\n\nlibrary(terra)\nsrtm = rast(\"data/srtm.tif\")\nsrtm\n\nclass       : SpatRaster \nsize        : 96, 127, 1  (nrow, ncol, nlyr)\nresolution  : 90, 90  (x, y)\nextent      : 745541.7, 756971.7, 712616.2, 721256.2  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \nsource      : srtm.tif \nname        : srtm \n\n\n\nsiatka_all = rast(\"data/siatka-all.tif\")\nsiatka_all\n\nclass       : SpatRaster \nsize        : 96, 127, 5  (nrow, ncol, nlyr)\nresolution  : 90, 90  (x, y)\nextent      : 745541.7, 756971.7, 712616.2, 721256.2  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=tmerc +lat_0=0 +lon_0=19 +k=0.9993 +x_0=500000 +y_0=-5300000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsource      : siatka-all.tif \nnames       : siatka-all_1, siatka-all_2, siatka-all_3, siatka-all_4, siatka-all_5 \n\n\nProstą wizualizację danych rastrowych można uzyskać za pomocą funkcji plot().\n\nplot(srtm)\n\n\n\n\n\n\n\n\n\nplot(siatka_all)\n\n\n\n\n\n\n\n\n\nplot(siatka_all[\"siatka-all_2\"])\n\n\n\n\n\n\n\n\nDane można zapisać do pliku używając funkcji writeRaster() z pakietu terra.\n\nwriteRaster(srtm, \"out/out_srtm.tif\", overwrite = TRUE)\n\n\nwriteRaster(siatka_all, \"out/out_siatka_all.tif\", overwrite = TRUE)\n\n\nwriteRaster(siatka_all[\"siatka-all_2\"], \"out/out_clc.tif\", overwrite = TRUE)",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dane przestrzenne w R</span>"
    ]
  },
  {
    "objectID": "04_daneprzestrzenne.html#struktura-danych-przestrzennych-w-r",
    "href": "04_daneprzestrzenne.html#struktura-danych-przestrzennych-w-r",
    "title": "3  Dane przestrzenne w R",
    "section": "3.4 Struktura danych przestrzennych w R",
    "text": "3.4 Struktura danych przestrzennych w R\n\n3.4.1 Dane wektorowe\nPodstawowe informacje o obiekcie można uzyskać poprzez wpisanie jego nazwy:\n\npunkty_sf\n\nSimple feature collection with 248 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 745592.5 ymin: 712642.4 xmax: 756967.8 ymax: 721238.7\nProjected CRS: ETRF2000-PL / CS92\nFirst 10 features:\n       srtm clc      temp      ndvi      savi                  geometry\n1  175.7430   1 13.852222 0.6158061 0.4189449   POINT (750298 716731.6)\n2  149.8111   1 15.484209 0.5558816 0.3794864 POINT (753482.9 717331.4)\n3  272.8583  NA 12.760814 0.6067462 0.3745572   POINT (747242.5 720589)\n4  187.2777   1 14.324648 0.3756170 0.2386246 POINT (755798.9 718828.1)\n5  260.1366   1 15.908549 0.4598393 0.3087599 POINT (746963.5 717533.5)\n6  160.1416   2  9.941118 0.5600288 0.3453627 POINT (756801.6 720474.1)\n7  192.9223   2 13.514751 0.6009588 0.3737249   POINT (752698 718623.6)\n8  234.9989   1 12.919225 0.4979195 0.2978503 POINT (749399.8 716955.2)\n9  228.7312   1 19.247132 0.3819634 0.2077981   POINT (748766.6 713889)\n10 240.5474   2 10.529105 0.5795159 0.3805268   POINT (749290.4 718861)\n\n\nObiekty klasy sf() są zbudowane z tabeli (data frame) wraz z dodatkową kolumną zawierającą geometrię (często nazywaną geometry lub geom) oraz szeregu atrybutów przestrzennych.\nStrukturę obiektu sf można też sprawdzić za pomocą funkcji str():\n\nstr(punkty_sf)\n\nClasses 'sf' and 'data.frame':  248 obs. of  6 variables:\n $ srtm    : num  176 150 273 187 260 ...\n $ clc     : int  1 1 NA 1 1 2 2 1 1 2 ...\n $ temp    : num  13.9 15.5 12.8 14.3 15.9 ...\n $ ndvi    : num  0.616 0.556 0.607 0.376 0.46 ...\n $ savi    : num  0.419 0.379 0.375 0.239 0.309 ...\n $ geometry:sfc_POINT of length 248; first list element:  'XY' num  750298 716732\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA\n  ..- attr(*, \"names\")= chr [1:5] \"srtm\" \"clc\" \"temp\" \"ndvi\" ...\n\n\n\n3.4.1.1 Konwersja danych przestrzennych do danych nieprzestrzennych\nObiekt klasy sf można przetworzyć na obiekt nieprzestrzenny klasy data.frame używając funkcji st_drop_geometry(). Funkcja st_drop_geometry() pozwala na pozbycie się informacji przestrzennych z obiektu klasy sf i uzyskanie jedynie obiektu klasy data.frame zawierającego nieprzestrzenne informacje o kolejnych punktach w tym zbiorze.\n\npunkty_df = st_drop_geometry(punkty_sf)\nhead(punkty_df)\n\n      srtm clc      temp      ndvi      savi\n1 175.7430   1 13.852222 0.6158061 0.4189449\n2 149.8111   1 15.484209 0.5558816 0.3794864\n3 272.8583  NA 12.760814 0.6067462 0.3745572\n4 187.2777   1 14.324648 0.3756170 0.2386246\n5 260.1366   1 15.908549 0.4598393 0.3087599\n6 160.1416   2  9.941118 0.5600288 0.3453627\n\n\nProszę zwrócić uwagę, że powyższy obiekt nie zawiera już informacji o współrzęnych (kolumn x oraz y).\n\n\n3.4.1.2 Współrzędne\nFunkcja st_coordinates() pozwala na wydobycie współrzędnych z obiektu punktowego klasy sf.\n\nxy = st_coordinates(punkty_sf)\nhead(xy)\n\n            X        Y\n[1,] 750298.0 716731.6\n[2,] 753482.9 717331.4\n[3,] 747242.5 720589.0\n[4,] 755798.9 718828.1\n[5,] 746963.5 717533.5\n[6,] 756801.6 720474.1\n\n\nInformację o współrzędnych można dołączyć do ramki danych nieprzestrzennych.\n\npunkty_df2 = cbind(punkty_df, xy)\nhead(punkty_df2)\n\n      srtm clc      temp      ndvi      savi        X        Y\n1 175.7430   1 13.852222 0.6158061 0.4189449 750298.0 716731.6\n2 149.8111   1 15.484209 0.5558816 0.3794864 753482.9 717331.4\n3 272.8583  NA 12.760814 0.6067462 0.3745572 747242.5 720589.0\n4 187.2777   1 14.324648 0.3756170 0.2386246 755798.9 718828.1\n5 260.1366   1 15.908549 0.4598393 0.3087599 746963.5 717533.5\n6 160.1416   2  9.941118 0.5600288 0.3453627 756801.6 720474.1\n\n\n\n\n3.4.1.3 Obwiednia\nFunkcja st_bbox() określa zasięg przestrzenny danych w jednostkach mapy.\n\nst_bbox(punkty_sf)\n\n    xmin     ymin     xmax     ymax \n745592.5 712642.4 756967.8 721238.7 \n\n\n\n\n3.4.1.4 Układ współrzędnych\nFunkcja st_crs() wyświetla definicję układu współrzędnych w formacie WKT2.\n\nst_crs(punkty_sf)\n\nCoordinate Reference System:\n  User input: EPSG:2180 \n  wkt:\nPROJCRS[\"ETRF2000-PL / CS92\",\n    BASEGEOGCRS[\"ETRF2000-PL\",\n        DATUM[\"ETRF2000 Poland\",\n            ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",9702]],\n    CONVERSION[\"Poland CS92\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",19,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9993,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",-5300000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (x)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (y)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Topographic mapping (medium and small scale).\"],\n        AREA[\"Poland - onshore and offshore.\"],\n        BBOX[49,14.14,55.93,24.15]],\n    ID[\"EPSG\",2180]]\n\n\n\n\n\n3.4.2 Dane rastrowe\nPo wczytaniu danych rastrowych do R otrzymamy następujące informacje: wielkość rastra zdefiniowaną liczbą wierszy i kolumn (dimensions), rozdzielczość danych (resolution), zasięg warstwy (extent), informację o układzie współrzędnych warstwy (coord. ref.), nazwę pliku źródłowego wczytanego do R (source) oraz nazwę warstwy (name).\n\nsrtm\n\nclass       : SpatRaster \nsize        : 96, 127, 1  (nrow, ncol, nlyr)\nresolution  : 90, 90  (x, y)\nextent      : 745541.7, 756971.7, 712616.2, 721256.2  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \nsource      : srtm.tif \nname        : srtm \n\n\nW przypadku pliku rastrowego zawierającego kilka kanałów (bands) zostaną podane nazwy (names) wszystkich kanałów.\n\nsiatka_all\n\nclass       : SpatRaster \nsize        : 96, 127, 5  (nrow, ncol, nlyr)\nresolution  : 90, 90  (x, y)\nextent      : 745541.7, 756971.7, 712616.2, 721256.2  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=tmerc +lat_0=0 +lon_0=19 +k=0.9993 +x_0=500000 +y_0=-5300000 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs \nsource      : siatka-all.tif \nnames       : siatka-all_1, siatka-all_2, siatka-all_3, siatka-all_4, siatka-all_5 \n\n\nPrzetworzenie obiektu klasy SpatRaster na data.frame odbywa się natomiast używając funkcji as.data.frame().\n\nsrtm_df = as.data.frame(srtm)\n\nFunkcje st_bbox() oraz st_crs() można także zastsować do obiektu klasy SpatRaster.\n\nst_bbox(srtm)\n\n    xmin     ymin     xmax     ymax \n745541.7 712616.2 756971.7 721256.2 \n\n\n\nst_crs(siatka_all)",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dane przestrzenne w R</span>"
    ]
  },
  {
    "objectID": "04_daneprzestrzenne.html#przetwarzanie-danych-wektorowych-w-r",
    "href": "04_daneprzestrzenne.html#przetwarzanie-danych-wektorowych-w-r",
    "title": "3  Dane przestrzenne w R",
    "section": "3.5 Przetwarzanie danych wektorowych w R",
    "text": "3.5 Przetwarzanie danych wektorowych w R\n\n3.5.1 Łączenie danych przestrzennych i atrybutowych\nPlik wlkp_powiaty.gpkg zawiera granice powiatów województwa wielkopolskiego, a plik bezrobocie_wlkp.csv zawiera informacje o stopie bezrobocia w powiatach województwa wielkopolskiego w 2004, 2010 oraz 2023 roku.\n\nlibrary(sf)\npowiaty_wlkp = read_sf(\"data/wlkp_powiaty.gpkg\")\nhead(powiaty_wlkp)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 290692.6 ymin: 427189.4 xmax: 431664.5 ymax: 588338.5\nProjected CRS: ETRF2000-PL / CS92\n# A tibble: 6 × 3\n  Nazwa                   TERYT                                             geom\n  &lt;chr&gt;                   &lt;chr&gt;                               &lt;MULTIPOLYGON [m]&gt;\n1 chodzieski              3001  (((365822.6 579030.7, 365850.1 579032.1, 365909…\n2 czarnkowsko-trzcianecki 3002  (((342585.5 579834.4, 342574.5 579840.3, 342578…\n3 gnieźnieński            3003  (((385846.8 531930.4, 385847.9 531933.4, 385852…\n4 gostyński               3004  (((367318.3 453501.8, 367470.1 453670.5, 367470…\n5 grodziski               3005  (((326493 471503, 326484.8 471495.6, 326449.4 4…\n6 jarociński              3006  (((394140.5 464050.1, 394117.8 464090.4, 394120…\n\n\n\nbezrobocie_wlkp = read.csv(\"data/bezrobocie_wlkp.csv\", sep = \";\", dec = \",\")\nhead(bezrobocie_wlkp)\n\n      Kod                          Nazwa sb2004 sb2010 sb2023\n1 3001000              Powiat chodzieski   23.6   15.2    7.5\n2 3002000 Powiat czarnkowsko-trzcianecki   23.8   15.0    4.8\n3 3003000            Powiat gnieźnieński   24.0   12.7    4.5\n4 3004000               Powiat gostyński   18.6   12.5    5.1\n5 3005000               Powiat grodziski   10.8    8.1    4.6\n6 3006000              Powiat jarociński   24.2   15.2    4.3\n\n\nAby połączyć dane przestrzenne z informacją o stopie bezrobocia wykorzystane zostaną pola TERYT oraz Kod. Kolumna TERYT w pliku wlkp_powiaty.gpkg zawiera identyfikator powiatu w postaci 4 cyfr. Do pola TERYT trzeba dodać “000”, aby identyfikator powiatu w obu obiektach był taki sam.\n\npowiaty_wlkp$TERYT = paste(powiaty_wlkp$TERYT, \"000\", sep=\"\")\n\nDo połączenia danych przestrzennych i nieprzestrzennych zostanie wykorzystana funkcja merge(). Funkcja ta nie będzie działać, jeśli będziemy chcieli połączyć dwa obiekty klasy przestrzennej.\n\npowiaty_wlkp_attr = merge(powiaty_wlkp, bezrobocie_wlkp[, -2], by.x = \"TERYT\", by.y = \"Kod\")\nhead(powiaty_wlkp_attr)\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 290692.6 ymin: 427189.4 xmax: 431664.5 ymax: 588338.5\nProjected CRS: ETRF2000-PL / CS92\n    TERYT                   Nazwa sb2004 sb2010 sb2023\n1 3001000              chodzieski   23.6   15.2    7.5\n2 3002000 czarnkowsko-trzcianecki   23.8   15.0    4.8\n3 3003000            gnieźnieński   24.0   12.7    4.5\n4 3004000               gostyński   18.6   12.5    5.1\n5 3005000               grodziski   10.8    8.1    4.6\n6 3006000              jarociński   24.2   15.2    4.3\n                            geom\n1 MULTIPOLYGON (((365822.6 57...\n2 MULTIPOLYGON (((342585.5 57...\n3 MULTIPOLYGON (((385846.8 53...\n4 MULTIPOLYGON (((367318.3 45...\n5 MULTIPOLYGON (((326493 4715...\n6 MULTIPOLYGON (((394140.5 46...\n\n\n\n\n3.5.2 Tworzenie obiektów wektorowych\nFunkcja st_point() z pakietu sf tworzy obiekt punktowy Simple Feautures na podstawie wektora, listy lub macierzy zawierającej współrzędne x oraz y.\n\nst_point(c(16.941820, 52.464279))\n\nPOINT (16.94182 52.46428)\n\n\nFunkcja st_sfc() z pakietu sf tworzy kolumnę z geometrią oraz dodaje informację o układzie współrzędnych dla obiektu typu Simple Feautures.\n\np1 = st_sfc(st_point(c(16.941820, 52.464279)), crs = 4326)\np1\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 16.94182 ymin: 52.46428 xmax: 16.94182 ymax: 52.46428\nGeodetic CRS:  WGS 84\n\n\nPOINT (16.94182 52.46428)\n\n\n\nplot(p1)\n\n\n\n\n\n\n\n\nDo utworzenia obiektu punktowego klasy sf na podstawie ramki danych (data.frame) używa się funkcji st_as_sf().\n\n\n3.5.3 Zmiana układu współrzędnych\nObiekt p1 został utworzony podając współrzędne w układzie WGS84 (EPSG: 4326). Do zmiany układu współrzędnych służy funkcja st_transform(). Poniżej zmienimy układ na PUWG1992 (EPSG: 2180).\n\np1a = st_transform(p1, 2180)\np1a\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 360220.9 ymin: 512925.4 xmax: 360220.9 ymax: 512925.4\nProjected CRS: ETRF2000-PL / CS92\n\n\nPOINT (360220.9 512925.4)\n\n\n\n\n3.5.4 Strefa buforowa\nFunkcja st_buffer() pozwala na wyznaczenie strefy bufforowej (otoczki) o zadanej odległości w jednostkach układu współrzędnych (w tym wypadku 50000 metrów).\n\nbfr = st_buffer(p1a, dist = 50000)\nbfr\n\nGeometry set for 1 feature \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 310220.9 ymin: 462925.4 xmax: 410220.9 ymax: 562925.4\nProjected CRS: ETRF2000-PL / CS92\n\n\nPOLYGON ((410220.9 512925.4, 410152.4 510308.6,...\n\n\n\nplot(bfr)\n\n\n\n\n\n\n\n\n\n\n3.5.5 Centroidy poligonów\nFunkcja st_centroid() wyznacza centroidy poligonów.\n\npowiaty_centroidy = st_centroid(powiaty_wlkp)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\nhead(powiaty_centroidy)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 319500.7 ymin: 442156.9 xmax: 404928.2 ymax: 568496.7\nProjected CRS: ETRF2000-PL / CS92\n# A tibble: 6 × 3\n  Nazwa                   TERYT                  geom\n  &lt;chr&gt;                   &lt;chr&gt;           &lt;POINT [m]&gt;\n1 chodzieski              3001000 (365156.4 568496.7)\n2 czarnkowsko-trzcianecki 3002000 (323039.8 563992.2)\n3 gnieźnieński            3003000 (404928.2 520088.9)\n4 gostyński               3004000 (366508.5 442156.9)\n5 grodziski               3005000 (319500.7 482387.3)\n6 jarociński              3006000   (398122.1 459364)\n\n\n\nplot(powiaty_centroidy[\"geom\"])\n\n\n\n\n\n\n\n\n\ncentroidy_attr = st_centroid(powiaty_wlkp_attr)\n\nWarning: st_centroid assumes attributes are constant over geometries\n\ncentroidy_attr\n\nSimple feature collection with 35 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 298759.2 ymin: 375275.2 xmax: 484054.5 ymax: 618120.6\nProjected CRS: ETRF2000-PL / CS92\nFirst 10 features:\n     TERYT                   Nazwa sb2004 sb2010 sb2023\n1  3001000              chodzieski   23.6   15.2    7.5\n2  3002000 czarnkowsko-trzcianecki   23.8   15.0    4.8\n3  3003000            gnieźnieński   24.0   12.7    4.5\n4  3004000               gostyński   18.6   12.5    5.1\n5  3005000               grodziski   10.8    8.1    4.6\n6  3006000              jarociński   24.2   15.2    4.3\n7  3007000                 kaliski   17.2    8.5    3.1\n8  3008000                kępiński    8.9    5.6    1.6\n9  3009000                  kolski   24.1   14.4    3.9\n10 3010000                koniński   27.5   18.1    8.3\n                        geom\n1  POINT (365156.4 568496.7)\n2  POINT (323039.8 563992.2)\n3  POINT (404928.2 520088.9)\n4  POINT (366508.5 442156.9)\n5  POINT (319500.7 482387.3)\n6    POINT (398122.1 459364)\n7  POINT (446068.2 436904.3)\n8  POINT (427638.9 375275.2)\n9  POINT (484054.5 485762.4)\n10 POINT (449786.4 490670.6)\n\n\n\n\n3.5.6 Selekcja na podstawie lokalizacji\nUżywając indeksowania można w R wykonywać selekcje na podstawie lokalizacji. Poniżej wybrano wszystkie centroidy przecinające się z obiektem bfr. Możliwe jest stosowanie także innych metod opisujących relacje przestrzenne.\n\ncentroidy_bfr = powiaty_centroidy[bfr, ]\n\n\ncentroidy_bfr\n\nSimple feature collection with 9 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 325004.7 ymin: 467381.3 xmax: 404928.2 ymax: 551033.6\nProjected CRS: ETRF2000-PL / CS92\n# A tibble: 9 × 3\n  Nazwa        TERYT                  geom\n  &lt;chr&gt;        &lt;chr&gt;           &lt;POINT [m]&gt;\n1 gnieźnieński 3003000 (404928.2 520088.9)\n2 kościański   3011000 (342649.8 467381.3)\n3 obornicki    3016000 (354281.4 540430.6)\n4 Poznań       3064000 (358768.1 506529.9)\n5 poznański    3021000 (360114.6 504966.5)\n6 szamotulski  3024000 (325004.7 529234.5)\n7 średzki      3025000 (385041.2 480378.2)\n8 śremski      3026000   (367149 468692.8)\n9 wągrowiecki  3028000 (383478.6 551033.6)\n\n\n\nplot(centroidy_bfr[\"geom\"])\n\n\n\n\n\n\n\n\n\npowiaty_centroidy[bfr, , op = st_within]\n\n\n\n3.5.7 Selekcja na podstawie atrybutów\nObiekty klasy sf() są zbudowane z tabeli (data frame) zawierajacej szereg atrybutów oraz dodatkową kolumnę zawierającą geometrie (często nazywaną geometry lub geom). Tabelę atrybutów można przeszukiwać używając dowolnych nprzeznaczonych do danych nieprzestrzennych (np. indeksowania).\n\npoznan = powiaty_wlkp[powiaty_wlkp$Nazwa == \"Poznań\",]\nplot(st_geometry(poznan))\n\n\n\n\n\n\n\n\n\n\n3.5.8 Tworzenie siatki - obiekt wektorowy\nTworzenie siatki składa się z 3 kroków:\n\nUtworzenie poligonu o zasięgu warstwy (w przykładzie poligon o zasięgu warstwy powiaty_wlkp).\n\n\nwlkp_bb = st_sf(geom = st_as_sfc(st_bbox(powiaty_wlkp)))\nplot(wlkp_bb)\n\n\n\n\n\n\n\n\n\nUtworzenie siatki używając funkcji st_make_grid(). Pierwszym argumentem jest obiekt definiujący zasięg siatki, a drugi (n) definiuje liczbę kolumn oraz wierszy. W przykładzie zostanie utworzona siatka o wymiarach 10x10.\n\n\nwlkp_grid = st_make_grid(wlkp_bb, n = 10)\nwlkp_grid\n\nGeometry set for 100 features \nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 281949.4 ymin: 360241.8 xmax: 507167.3 ymax: 645513.7\nProjected CRS: ETRF2000-PL / CS92\nFirst 5 geometries:\n\n\nPOLYGON ((281949.4 360241.8, 304471.2 360241.8,...\n\n\nPOLYGON ((304471.2 360241.8, 326993 360241.8, 3...\n\n\nPOLYGON ((326993 360241.8, 349514.8 360241.8, 3...\n\n\nPOLYGON ((349514.8 360241.8, 372036.6 360241.8,...\n\n\nPOLYGON ((372036.6 360241.8, 394558.3 360241.8,...\n\n\n\nplot(wlkp_grid)\n\n\n\n\n\n\n\n\nFunkcja st_sf() przekształca utworzoną siatkę na obiekt klasy sf składający się z ramki danych (data.frame) oraz kolumny zawierającej geometrię.\n\nwlkp_grid_sf = st_sf(geometry = wlkp_grid)\nwlkp_grid_sf\n\nSimple feature collection with 100 features and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 281949.4 ymin: 360241.8 xmax: 507167.3 ymax: 645513.7\nProjected CRS: ETRF2000-PL / CS92\nFirst 10 features:\n                         geometry\n1  POLYGON ((281949.4 360241.8...\n2  POLYGON ((304471.2 360241.8...\n3  POLYGON ((326993 360241.8, ...\n4  POLYGON ((349514.8 360241.8...\n5  POLYGON ((372036.6 360241.8...\n6  POLYGON ((394558.3 360241.8...\n7  POLYGON ((417080.1 360241.8...\n8  POLYGON ((439601.9 360241.8...\n9  POLYGON ((462123.7 360241.8...\n10 POLYGON ((484645.5 360241.8...\n\n\nPodsumowując, do utworzenia siatki potrzebne jest wykonanie 3 poniższych kroków.\n\nwlkp_bb = st_sf(geom = st_as_sfc(st_bbox(powiaty_wlkp)))\nwlkp_grid = st_make_grid(wlkp_bb, n = 10)\nwlkp_grid_sf = st_sf(geometry = wlkp_grid)",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dane przestrzenne w R</span>"
    ]
  },
  {
    "objectID": "04_daneprzestrzenne.html#przetwarzanie-danych-rastrowych-w-r",
    "href": "04_daneprzestrzenne.html#przetwarzanie-danych-rastrowych-w-r",
    "title": "3  Dane przestrzenne w R",
    "section": "3.6 Przetwarzanie danych rastrowych w R",
    "text": "3.6 Przetwarzanie danych rastrowych w R\n\n3.6.1 Tworzenie siatki rastrowej\nFunkcja rast() z pakietu terra pozwala na wczytanie danych rastrowych zapisanych w zewnętrznym pliku lub na utworzenie obiektu rastrowego o zadanym zasięgu, rozdzielczości oraz układzie współrzędnych.\n\nlibrary(terra)\nmoja_siatka = rast(ext = poznan, res = 90, crs = crs(poznan))\nmoja_siatka\n\nclass       : SpatRaster \nsize        : 269, 255, 1  (nrow, ncol, nlyr)\nresolution  : 90, 90  (x, y)\nextent      : 345943.2, 368893.2, 493660, 517870  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \n\n\n\nmoja_siatka2 = rast(xmin = 345943.2, ymin = 493660, \n                    xmax = 368893.2, ymax = 517870, \n                    res = 90, crs = crs(poznan))\nmoja_siatka2\n\nclass       : SpatRaster \nsize        : 269, 255, 1  (nrow, ncol, nlyr)\nresolution  : 90, 90  (x, y)\nextent      : 345943.2, 368893.2, 493660, 517870  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \n\n\nDo utworzonego obiektu rastrowego należy przypisać wartości.\n\nmoja_siatka &lt;- setValues(moja_siatka, 1:ncell(moja_siatka))\n\n\nplot(moja_siatka)\n\n\n\n\n\n\n\n\n\nwriteRaster(moja_siatka, \"out/out_moja_siatka.tif\")\n\n\n\n3.6.2 Przycięcie danych rastrowych do zasięgu warstwy\nFunkcja crop() pozwala na przycięcie warstwy rastrowej do granic wyznaczonych przez warstwę wektorową.\n\npoznan_rst = crop(moja_siatka, poznan, mask = TRUE)\n\n\nplot(poznan_rst)\n\n\n\n\n\n\n\n\n\n\n3.6.3 Zmiana rozdzielczości\nFunkcja aggregate() pozwala na agregację warstwy rastrowej. Warstwa rastrowa poznan_rst ma rozdzielczość 90m. Argument fact = 2 agreguję warstwę dwukrotnie zmniejszając rozdzielczość, po agregacji warstwa będzie miała rozdzielczość 180m. Argument fun oznacza funkcję jaka ma być wykorzystana do agregacji danych (np. suma, wartość minimalna lub maksymalna, średnia, itp.). W przykładzie nowa warstwa rastrowa będzie miała rozdzielczość 180m, a wartość komórek będzie sumą wartości z komórek 90m.\n\npoznan_rst_agg = aggregate(poznan_rst, fact = 2, fun = \"sum\")\npoznan_rst_agg\n\nclass       : SpatRaster \nsize        : 135, 128, 1  (nrow, ncol, nlyr)\nresolution  : 180, 180  (x, y)\nextent      : 345943.2, 368983.2, 493570, 517870  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \nsource(s)   : memory\nname        :  lyr.1 \nmin value   :   1236 \nmax value   : 272612 \n\n\n\nZagreguj warstwę rastrową srtm do rozdzielczości 360m. Jaką wartość musi przyjąć argument fact? Warstwa rastrowa srtm to cyfrowy model wysokościowy (DEM) zawierający w komórkach wartości wysokości n.p.m. Jaką funkcję należy użyć do aggregacji tych danych?",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dane przestrzenne w R</span>"
    ]
  },
  {
    "objectID": "04_daneprzestrzenne.html#wizualizacja-danych-przestrzennych-w-r",
    "href": "04_daneprzestrzenne.html#wizualizacja-danych-przestrzennych-w-r",
    "title": "3  Dane przestrzenne w R",
    "section": "3.7 Wizualizacja danych przestrzennych w R",
    "text": "3.7 Wizualizacja danych przestrzennych w R\nIstnieje wiele pakietów pozwalajacych na wizualizację danych przestrzennych. Poniżej pokazane są przykłady użycia pakietu tmap.\nGłówna idea stojąca za pakietem tmap mówi o łączeniu kolejnych linii kodu znakiem +, i te kolejne elementy będą wyświetlane po sobie. Złożona wizualizacja danych przestrzennych składa się z następujących elementów:\n\ntm_shape() - podstawowa funkcja pakietu,która pozwala na zdefiniowanie jaki obiekt przestrzenny chcemy zwizualizować.\nElementy odnoszące się do typów obiektów (np. dane punktowe, poligonowe, rastrowe, itp.)\n\ntm_dots(), tm_symbols() - dane punktowe\ntm_polygons(), tm_borders() - dane poligonowe\ntm_raster() - dane rastrowe.\n\nDodatkowe elementy kompozycji mapy:\n\ntm_scale_bar() - podziałka liniowa\ntm_compass() - strzałka północy\n\n\nKażda z powyższych składowych funkcji jest też łatwo modyfikowalna, pozwalając na wybranie zmiennej do przestawienia na mapie, stylu legendy, jej tytułu, itd.\n\n3.7.1 Dane poligonowe\nPodstawowa wizualizacja danych poligonowych składa się z dwóch elementów: tm_shape() oraz tm_polygons() (lub tm_borders()).\n\nlibrary(tmap)\ntm_shape(powiaty_wlkp) +\n    tm_polygons()\n\n\n\n\n\n\n\n\n\ntm_shape(powiaty_wlkp) +\n    tm_borders(lwd = 2)\n\n\n\n\n\n\n\n\nNastępnie możemy definiować dodatkowe elementy: wizualizowana zmienna, wykorzystana paleta itp.\n\nlibrary(tmap)\ntm_shape(powiaty_wlkp_attr) +\n    tm_polygons(fill = \"sb2023\", palette = \"YlOrBr\", title = \"Stopa bezrobocia [%]\")\n\n\n\n\n\n\n\n\n\n\n3.7.2 Dane punktowe\nPodstawowa wizualizacja danych punktowych składa się z dwóch elementów: tm_shape() oraz tm_dots().\n\ntm_shape(centroidy_attr) +\n    tm_dots()\n\n\n\n\n\n\n\n\n\ntm_shape(centroidy_attr) + \n    tm_dots(fill = \"sb2023\")\n\n\n\n\n\n\n\n\n\ntm_shape(centroidy_attr) + \n    tm_dots(fill = \"sb2023\", \n            fill.scale = tm_scale_continuous(values = \"viridis\", label.na = \"Brak danych\"),\n            fill.legend = tm_legend(title = \"Stopa bezrobocia\"),\n            size = 1)\n\n\n\n\n\n\n\n\n\n\n3.7.3 Dane rastrowe\nPodstawowa wizualizacja danych rastrowych składa się z dwóch elementów: tm_shape() oraz tm_raster().\n\ntm_shape(siatka_all) +\n    tm_raster(\"siatka-all_1\") +\n    tm_scalebar() +\n    tm_compass(position = c(\"left\", \"top\"))\n\n\n\n\n\n\n\n\n\ntm_shape(siatka_all) +\n    tm_raster(col = \"siatka-all_1\",\n              col.scale = tm_scale_continuous(values = \"Spectral\"),\n              col.legend = tm_legend(title = \"m npm\")) \n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Spectral\" is named\n\"brewer.spectral\"\n\n\n\n\n\n\n\n\n\n\n\n3.7.4 Łączenie warstw\nPakiet tmap pozwala także na tworzenie złożonych wizualizacji, łącząc obiekty różnego typu (np. dane poligonowe oraz punktowe lub dane rastrowe i wektorowe).\n\nlibrary(tmap)\ntm_shape(powiaty_wlkp_attr) +\n    tm_polygons(fill = \"sb2023\", palette = \"YlOrBr\") + \ntm_shape(centroidy_attr) + \n  tm_dots(size = 0.7)\n\n\n\n\n\n\n\n\n\ntm_shape(siatka_all) +\n    tm_raster(col = \"siatka-all_1\",\n              col.scale = tm_scale_continuous(values = \"Spectral\"),\n              col.legend = tm_legend(title = \"m npm\")) + \n    tm_shape(punkty_sf) + \n    tm_symbols(fill = \"temp\", \n            fill.scale = tm_scale_continuous(values = \"viridis\", label.na = \"Brak danych\"),\n            fill.legend = tm_legend(title = \"Temperatura [*C]\"),\n            col = \"white\",\n            size = 0.75) +\n    tm_scalebar()\n\n[cols4all] color palettes: use palettes from the R package cols4all. Run\n`cols4all::c4a_gui()` to explore them. The old palette name \"Spectral\" is named\n\"brewer.spectral\"\n[plot mode] legend/component: Some components or legends are too \"high\" and are\ntherefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\n3.7.5 Zapisywanie map\n\ntm = tm_shape(powiaty_wlkp) +\n    tm_polygons()\ntmap_save(tm, \"out/mapa.png\", width = 600, height = 600)",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dane przestrzenne w R</span>"
    ]
  },
  {
    "objectID": "05_eksploracja.html",
    "href": "05_eksploracja.html",
    "title": "4  Eksploracyjna analiza danych przestrzennych",
    "section": "",
    "text": "4.1 Eksploracyjna analiza danych",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eksploracyjna analiza danych przestrzennych</span>"
    ]
  },
  {
    "objectID": "05_eksploracja.html#eksploracyjna-analiza-danych",
    "href": "05_eksploracja.html#eksploracyjna-analiza-danych",
    "title": "4  Eksploracyjna analiza danych przestrzennych",
    "section": "",
    "text": "Każda analiza danych przestrzennych powinna być poprzedzona eksploracyjną analizą danych. Celem eksploracyjnej analizy danych jest uzyskanie podstawowych informacji o analizowanych danych.\nSposób wykonania eksploracyjnej analizy danych (np. dobór wykorzystywanych metod) różni się w zależności od posiadanego zbioru danych, jak i od postawionego pytania.\n\n\n4.1.1 Cele eksploracyjnej analizy danych\nOgólne cele eksploracyjnej analizy danych obejmują:\n\nPrzygotowanie ogólnej charakterystyki danych oraz badanego zjawiska.\nIdentyfikacja przestrzennego/czasowego typu próbkowania.\nOcena relacji zachodzących pomiędzy lokalizacją pomiaru/obserwacji, a czynnikami wpływającymi na zmienność przestrzenną badanych cech.\n\n\n\n4.1.2 Eksploracyjna analiza nieprzestrzenna i przestrzenna\nEksploracyjna analiza danych dzieli się na analizę nieprzestrzenną i przestrzenną.\n\n\n\n\n\n\n\nNieprzestrzenna\nPrzestrzenna\n\n\n\n\nw analizie uwzględniane są wartości atrybutów, a nie ich lokalizacja\npodstawą analizy danych jest lokalizacja obserwacji\n\n\nanaliza statystyczna danych wykonywana w oparciu o tabelę atrybutów dołączoną do danych geoprzestrzennych\n\n\n\npodstawowe statystyki opisowe, typ rozkładu danych, analiza korelacji, zmienność cechy w grupach\ntyp próbkowania, ogólny pogląd na zmienność przestrzenną zjawiska",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eksploracyjna analiza danych przestrzennych</span>"
    ]
  },
  {
    "objectID": "05_eksploracja.html#eksploracyjna-analiza-dla-różnych-typów-danych",
    "href": "05_eksploracja.html#eksploracyjna-analiza-dla-różnych-typów-danych",
    "title": "4  Eksploracyjna analiza danych przestrzennych",
    "section": "4.2 Eksploracyjna analiza dla różnych typów danych",
    "text": "4.2 Eksploracyjna analiza dla różnych typów danych\n\n4.2.1 Statystyka punktów\nCel:\n\nokreślenie w jaki sposób obiekty/zdarzenia są rozmieszczone w przestrzeni\n\nEksploracyjna analiza danych:\n\nSprawdzenie poprawności współrzędnych\nWgląd w typ próbkowania\nOkreślenie zależności między lokalizacją, a innymi czynnikami (np. więcej włamań notuje się w obszarach z wysokimi cenami nieruchomości)\n\n\n\n4.2.2 Statystyka powierzchni (dane poligonowe)\nCel:\n\nanaliza podobieństwa i różnic między obszarami/regionami\nwyszukanie obszarów podobnych do siebie\nznalezienie obszarów znacząco różnych od sąsiednich\n\nEksploracyjna analiza danych:\n\nCharakterystyka statystyczna danych (statystyki podstawowe, wartości odstające globalnie, rozkład danych)\nSprawdzenie poprawności danych, w tym między innymi identyfikacja danych odstających lokalnie\n\n\n\n4.2.3 Geostatystyka\nCel:\n\nzrozumienie zmienności przestrzennej lub czasowej zjawiska\nszacowanie wartości dla całego obszaru (dane ciągłe) na podstawie danych punktowych (estymacja)\n\nEksploracyjna analiza danych:\n\nCharakterystyka statystyczna danych (statystyki podstawowe, wartości odstające globalnie, rozkład danych)\nSprawdzenie poprawności współrzędnych\nSprawdzenie poprawności danych, w tym między innymi identyfikacja danych odstających lokalnie\nWgląd w tym próbkowania\nOgólny pogląd na zmienność przestrzenną, wykorzystanie prostej automatycznej procedury interpolacji",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eksploracyjna analiza danych przestrzennych</span>"
    ]
  },
  {
    "objectID": "05_eksploracja.html#przykład-1.-analiza-rozkładu-temperatury-powietrza",
    "href": "05_eksploracja.html#przykład-1.-analiza-rozkładu-temperatury-powietrza",
    "title": "4  Eksploracyjna analiza danych przestrzennych",
    "section": "4.3 Przykład 1. Analiza rozkładu temperatury powietrza",
    "text": "4.3 Przykład 1. Analiza rozkładu temperatury powietrza\n\n4.3.1 Dane i cel analizy\n\n4.3.1.1 Cel\nCelem analizy jest uzyskanie informacji o przestrzennym rozkładzie wartości temperatury w analizowanym obszarze. Dane punktowe przedstawiające lokalizacje pomiaru temperatury powietrza zostaną wykorzystane następnie w analizie geostatystycznej mającej w celu oszacowania wartości dla całego obszaru z wykorzystaniem jednej z geostatystycznych metod estymacji.\n\n\n4.3.1.2 Dane\nDla analizowanego obszaru dysponujemy:\n\ndanymi punktowymi (punkty.csv) zapisanymi w pliku tekstowym zawierającymi lokalizacje, w których wykonano pomiary temperatury powietrza. Dla każdej lokalizacji dysponujemy także zestawem zmiennych dodatkowych (ndvi, wysokość n.p.m (srtm), klasa pokrycia terenu (clc)).\nplikiem wektorowym w formacie .gpkg (granica.gpkg) zawierającym granicę obszaru analizy.\n\n\ngranica = read_sf(\"data/granica.gpkg\")\n#wczytanie danych oraz konwersja do klasy sf\npkt = read.csv(\"data/punkty.csv\")\npkt = pkt[!is.na(pkt$temp), ]\npunkty = st_as_sf(pkt, coords = c(\"x\", \"y\"), crs = \"EPSG:2180\")\n\n\nstr(punkty)\n\nClasses 'sf' and 'data.frame':  247 obs. of  6 variables:\n $ srtm    : num  176 150 273 187 260 ...\n $ clc     : int  1 1 NA 1 1 2 2 1 1 2 ...\n $ temp    : num  13.9 15.5 12.8 14.3 15.9 ...\n $ ndvi    : num  0.616 0.556 0.607 0.376 0.46 ...\n $ savi    : num  0.419 0.379 0.375 0.239 0.309 ...\n $ geometry:sfc_POINT of length 247; first list element:  'XY' num  750298 716732\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA\n  ..- attr(*, \"names\")= chr [1:5] \"srtm\" \"clc\" \"temp\" \"ndvi\" ...\n\n\n\n\n\n4.3.2 Eksploracyjna nieprzestrzenna analiza danych\n\n4.3.2.1 Podsumowanie numeryczne\nPodstawowe informacje o analizowanej zmiennej (w tym przykładzie temperatura powietrza temp) można uzyskać za pomocą funkcji summary(). Dodatkowo można obliczyć inne statystyki opisowe, np. odchylenie standardowe (funkcja sd).\n\nsummary(punkty$temp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  7.883  12.003  14.941  15.273  17.630  24.945 \n\n\n\nsd(punkty$temp)\n\n[1] 3.973076\n\n\n\n\n4.3.2.2 Rozkład danych\nRozkład danych można przedstawić graficznie wykorzystując histogram (geom_histogram) lub estymator jądrowy gęstości (geom_density).\n\nlibrary(ggplot2)\n\nggplot(punkty, aes(temp)) + \n  geom_histogram() + \n  labs(x = \"Zmienna temp\", y = \"Liczba obserwacji\") + \n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nggplot(punkty, aes(temp)) + geom_density() + theme_bw()\n\n\n\n\n\n\n\n\nDodatkowo rozkład danych można opisać za pomocą dwóch statystyk opisowych: skośności oraz kurtozy.\n\nlibrary(e1071)\nskewness(punkty$temp)\nkurtosis(punkty$temp)\n\n\n\n4.3.2.3 Dane globalnie odstające\nDane globalnie odstające to bardzo niskie lub bardzo wysokie wartości względem pozostałych w zbiorze danych. Można je zidentyfikować poprzez wykonanie histogramu oraz obejrzenie rozkładu danych.\n\n\n4.3.2.4 Zależność między temperaturą, a zmiennymi ndvi i srtm\nDo określenia związku między zmiennymi ilościowymi (np. temperaturą a wartościami zmiennych srtm oraz ndvi) można wykorzystać wykres rozrzutu oraz miary korelacji (współczynnik korelacji liniowej lub współczynnik korelacji rang Spearmana). Poniżej wykonano wykresy rozrzutu pokazujące zależnośc między zmienną temp, a ndvi oraz zmienną temp, a srtm.\n\nggplot(punkty, aes(temp, ndvi)) +\n  geom_point() +\n  labs(x = \"Zmienna temp\", y = \"Zmienna ndvi\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCzy istnieje zależność między zmienną temp, a ndvi?\n\n\nggplot(punkty, aes(temp, srtm)) +\n  geom_point() +\n  labs(x = \"Zmienna temp\", y = \"Zmienna srtm\") + \n  theme_bw()\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nCzy istnieje zależność między zmienną temp, a srtm?\n\nDo liczbowego określenia zależności między zmiennymi wykorzystano test korelacji dla współczynnika korelacji rang Spearmana. Test korelacji wyliczany jest z wykorzystaniem funkcji cor.test(), w której jako metodę (argument method) można podać współczynnik korelacji rang Spearmana (method = “spearman”) lub współczynnik korelacji liniowej Pearsona (method = “pearson”). W wyniku otrzymamy wartość współczynnika korelacji oraz poziom istotności.\n\ncor.test(punkty$temp, punkty$ndvi, method = \"spearman\")\n\nWarning in cor.test.default(punkty$temp, punkty$ndvi, method = \"spearman\"):\nCannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  punkty$temp and punkty$ndvi\nS = 2378524, p-value = 0.4074\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n0.05294538 \n\n\n\ncor.test(punkty$temp, punkty$srtm, method = \"spearman\")\n\nWarning in cor.test.default(punkty$temp, punkty$srtm, method = \"spearman\"):\nCannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  punkty$temp and punkty$srtm\nS = 2866567, p-value = 0.00782\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.1695597 \n\n\n\nJaka jest wartość korelacji dla zmiennych temp/ndvi, oraz temp/srtm? Czy w każdym z przypadków uzyskano wynik istotny statystycznie?\n\n\n\nZmienność temperatury w klasach pokrycia terenu\nZależność między zmienną ilościową a jakościową można przedstawić na wykresie pudełkowym.\n\npunkty$clc = as.factor(punkty$clc)\nggplot(punkty, aes(x = clc, y = temp)) + \n  geom_boxplot() + \n  labs(x = \"Zmienna clc\", y = \"Zmienna temp\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nCzy temperatura różnicuje się w zależności od klasy pokrycia terenu?\n\nDo określenia istotności różnić średniej pomiędzy grupami można wykorzystać analizę wariancji. Analiza wariancji (ang. Analysis of Variance - ANOVA) służy do testowania istotności różnic między średnimi w wielu grupach. Metoda ta służy do oceny czy średnie wartości cechy Y różnią się istotnie pomiędzy grupami wyznaczonymi przez zmienną X.\n\naov_test = aov(temp ~ clc, data = punkty)\nsummary(aov_test)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nclc           2   1272   636.0   60.86 &lt;2e-16 ***\nResiduals   239   2498    10.4                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n5 observations deleted due to missingness\n\n\nANOVA nie pozwala natomiast na stwierdzenie między którymi grupami występują różnice. Aby to stwierdzić konieczne jest wykonanie porównań wielokrotnych (post-hoc), np. testu Tukeya.\n\ntukey = TukeyHSD(aov_test, \"clc\")\nplot(tukey, las = 1)\n\n\n\n\n\n\n\n\n\nCzy temperatura różnicuje się istotnie w zależności od klasy pokrycia terenu?\n\n\n\n\n4.3.3 Eksploracyjna przestrzenna analiza danych\nDo sprawdzenia poprawności współrzędnych, określenia typu próbkowania oraz identyfikacji danych odstających lokalnie można wykorzystać mapę lokalizacyjną pokazującą lokalizację wykonania pomiarów. Mapę lokalizacyjną można wykonać wykorzystując pakiet tmap.\n\n4.3.3.1 Sprawdzenie poprawności współrzędnych oraz określenie typu próbkowania\n\nCzy współrzędne są w odpowiednim porządku (X-Y, Y-X)?\nCzy współrzędne są w odpowiednim układzie?\nCzy wszystkie punkty znajdują się na badanym obszarze?\n\n\ntm_shape(granica) +\n        tm_polygons() +\n        tm_shape(punkty) +\n        tm_symbols() +\n        tm_grid()\n\n\n\n\n\n\n\n\n\n\nSprawdzenie poprawności danych oraz identyfikacja danych odstających lokalnie\n\ntm_shape(punkty) +\n  tm_graticules(labels.show = FALSE) +\n  tm_symbols(col = \"temp\",\n             style = \"cont\", \n             n = 10,\n             palette = \"-Spectral\",\n             title.col = \"Zmienna temp\")\n\n\n\n\nRozkład wartości temperatury powietrza w analizowanym obszarze.",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eksploracyjna analiza danych przestrzennych</span>"
    ]
  },
  {
    "objectID": "05_eksploracja.html#przykład-2-analiza-przestępczości-w-hrabstwie-cook",
    "href": "05_eksploracja.html#przykład-2-analiza-przestępczości-w-hrabstwie-cook",
    "title": "4  Eksploracyjna analiza danych przestrzennych",
    "section": "4.4 Przykład 2: Analiza przestępczości w hrabstwie Cook",
    "text": "4.4 Przykład 2: Analiza przestępczości w hrabstwie Cook\n\n4.4.1 Dane\n\nPlik socio_economic_variables.gpkg dostarcza danych społeczno-ekonomicznych dla obszaru hrabstwa Cook (centralna część miasta Chicago). Dane te pochodzą ze zbioru danych American Community Survey: ACS 2016-2020.\nPlik crimes_vehicle_theft.gpkg zawiera lokalizacje przestępstw (kradzieży samochodów) dla centralnej części Chicago dla 2020 roku.\n\n\nsocio_data = read_sf(\"data/socio_economic_variables.gpkg\")\ncrimes = read_sf(\"data/crimes_vehicle_theft.gpkg\")\n\n\ntm_shape(socio_data) + \n  tm_borders(col = \"grey\") + \ntm_shape(crimes) +\n  tm_dots(fill = \"red\")\n\n\n\n\n\n\n\n\nKolorem czerwonym zaznaczono lokalizacje przestępstw (kradzieży samochodów), a szarym granice obszarów spisowych w hrabstwie Cook, dla których pozyskano dane ekonomiczne.\nWykorzystując funkcję str() możemy uzyskać podstawowe informacje o analizowanych zmiennych.\n\nstr(socio_data)\n\nsf [1,332 × 23] (S3: sf/tbl_df/tbl/data.frame)\n $ GEOID            : chr [1:1332] \"17031822400\" \"17031740100\" \"17031530501\" \"17031830300\" ...\n $ POP              : num [1:1332] 6214 2962 5073 4536 2523 ...\n $ WHITE            : num [1:1332] 0.6999 0.8383 0.0195 0.2277 0.8922 ...\n $ BLACK            : num [1:1332] 0.00724 0 0.94067 0.74757 0.04201 ...\n $ ASIAN            : num [1:1332] 0.0439 0.0135 0 0.0117 0.0103 ...\n $ HISPANIC         : num [1:1332] 0.242 0.1435 0.0264 0.0126 0.0289 ...\n $ FOREIGN_BORN     : num [1:1332] 0.2948 0.023 0.0199 0.0423 0.0258 ...\n $ UNEMPL           : num [1:1332] 0.0339 0.0554 0.2593 0.1521 0.0249 ...\n $ PUBLIC_ASSIST    : num [1:1332] 0.1664 0.0565 0.4314 0.183 0.0387 ...\n $ VACANT           : num [1:1332] 0.1591 0.0884 0.2364 0.1074 0.0459 ...\n $ OCCUPIED         : num [1:1332] 1 1 1 1 1 ...\n $ OWNER            : num [1:1332] 0.662 0.851 0.567 0.416 0.828 ...\n $ RENTER           : num [1:1332] 0.338 0.149 0.433 0.584 0.172 ...\n $ MEDIAN_INCOME    : num [1:1332] 51782 84800 45651 48491 102543 ...\n $ MEDIAN_BUILT_YEAR: num [1:1332] 1972 1954 0 1958 1947 ...\n $ MEDIAN_HOME_VALUE: num [1:1332] 204900 245200 121100 26000 285700 ...\n $ OTHER            : num [1:1332] 0.993 0.995 0.987 1 0.973 ...\n $ SEX_RATIO        : num [1:1332] 0.000974 0.001032 0.0008 0.000715 0.000941 ...\n $ EDU_HIGH         : num [1:1332] 0.209 0.33 0.142 0.321 0.619 ...\n $ HOME_VALUES_CAT  : chr [1:1332] \"medium\" \"medium\" \"low\" \"low\" ...\n $ INCOME_CAT       : chr [1:1332] \"medium\" \"medium\" \"low\" \"medium\" ...\n $ BUILT_YEAR_CAT   : chr [1:1332] \"medium\" \"medium\" \"old\" \"medium\" ...\n $ geom             :sfc_MULTIPOLYGON of length 1332; first list element: List of 1\n  ..$ :List of 1\n  .. ..$ : num [1:37, 1:2] -87.8 -87.8 -87.8 -87.8 -87.8 ...\n  ..- attr(*, \"class\")= chr [1:3] \"XY\" \"MULTIPOLYGON\" \"sfg\"\n - attr(*, \"sf_column\")= chr \"geom\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA NA NA NA NA ...\n  ..- attr(*, \"names\")= chr [1:22] \"GEOID\" \"POP\" \"WHITE\" \"BLACK\" ...\n\n\nRamka danych socio_data zawiera szereg zmiennych społeczno-ekonomicznych i demograficznych:\n\nliczbę ludności w każdym obszarze spisowym (POP)\nodsetek ludności w podziale na grupy rasowo-etniczne (WHITE, BLACK, ASIAN, HISPANIC, OTHER)\nodsetek osób urodzonych za granicą (FOREIGN_BORN)\nodsetek osób bezrobotnych (UNEMPL)\nodsetek osób korzystających z pomocy społecznej (PUBLIC_ASSIST)\nodsetek niezamieszkłaych domów oraz mieszkań (VACANT)\nodsetek zamieszkałych domów oraz mieszkań (OCCUPPIED)\nodsetek domów/mieszkań zamieszkałych przez właściciela (OWNER)\nodsetek domów/mieszkań zamieszkałych przez osoby wynajmujące (RENTER)\nmediana dochodów w obszarze spisowym (MEDIAN_INCOME)\nmediana wieku zabudowy w obszarze spisowym (MEDIAN_BUILT_YEAR)\nmediana wartości domów w obszarze spisowym (MEDIAN_HOME_VALUE)\nodsetek osób w wyższym wykształceniem (EDU_HIGH)\nwartość domów: niska, średnia, wysoka (HOME_VALUES_CAT)\ndochód: niski, średni, wysoki (INCOME_CAT)\nwiek zabudowy: nowa, średnia, stara zabudowa (BUILT_YEAR_CAT)\n\n\n\n4.4.2 Cel analizy\nCelem analizy jest znalezienie czynników społeczno-ekonomicznych wpływających na poziom przestępczości (kradzieży samochodów). Czy istnieje zależność między poziomem przestępczości, a zmiennymi społeczno-ekonomicznymi?\n\n\n4.4.3 Przygotowanie danych do analizy\n\nUjednolicenie układów współrzędnych między warstwami.\n\nLokalizacja przestępstw jest w układzie WGS84, warstwę trzeba przekształcić do układu warstwy z danymi społeczno-ekonomicznymi.\n\ncrimes = st_transform(crimes, st_crs(socio_data))\n\n\nselekcja obszarow spisowych, w których zlokalizowane są przestępstwa.\n\n\nsocio_data = socio_data[crimes, ]\n\n\nzliczenie przestępstw w obszarach spisowych\n\n\nsocio_data$NB_CRIMES &lt;- lengths(st_intersects(socio_data, crimes))\n\n\nobliczenie odsetka przestępst na 1000 mieszkańców (ang. crime rates)\n\n\nsocio_data$CRIME_RATE = (socio_data$NB_CRIMES/socio_data$POP)*1000\n\n\nutworzenie ramki danych bez kolumny z geometrią (będzie potrzebne przy części analiz statystycznych)\n\n\nsocio_data_attr = socio_data\nsocio_data_attr$geom &lt;-NULL\n\n\n\n4.4.4 Eksploracyjna nieprzestrzenna analiza danych\n\nPodsumowanie numeryczne\n\nsummary(socio_data_attr)\n\n    GEOID                POP            WHITE             BLACK        \n Length:788         Min.   :    0   Min.   :0.00000   Min.   :0.00000  \n Class :character   1st Qu.: 2044   1st Qu.:0.03004   1st Qu.:0.02311  \n Mode  :character   Median : 3142   Median :0.22915   Median :0.09334  \n                    Mean   : 3440   Mean   :0.30523   Mean   :0.34852  \n                    3rd Qu.: 4570   3rd Qu.:0.57202   3rd Qu.:0.81515  \n                    Max.   :10974   Max.   :0.92883   Max.   :1.00000  \n                                    NA's   :3         NA's   :3        \n     ASIAN            HISPANIC        FOREIGN_BORN        UNEMPL       \n Min.   :0.00000   Min.   :0.00000   Min.   :0.0000   Min.   :0.00000  \n 1st Qu.:0.00000   1st Qu.:0.04447   1st Qu.:0.0458   1st Qu.:0.03864  \n Median :0.02261   Median :0.13664   Median :0.1531   Median :0.07256  \n Mean   :0.06003   Mean   :0.26169   Mean   :0.1803   Mean   :0.09981  \n 3rd Qu.:0.07913   3rd Qu.:0.40534   3rd Qu.:0.2974   3rd Qu.:0.14334  \n Max.   :0.84913   Max.   :0.98900   Max.   :0.6585   Max.   :0.46776  \n NA's   :3         NA's   :3         NA's   :3        NA's   :3        \n PUBLIC_ASSIST         VACANT           OCCUPIED          OWNER       \n Min.   :0.00000   Min.   :0.00000   Min.   :0.9983   Min.   :0.0000  \n 1st Qu.:0.07705   1st Qu.:0.06515   1st Qu.:0.9998   1st Qu.:0.2993  \n Median :0.17330   Median :0.09682   Median :0.9999   Median :0.4352  \n Mean   :0.21870   Mean   :0.11825   Mean   :0.9999   Mean   :0.4516  \n 3rd Qu.:0.33914   3rd Qu.:0.14966   3rd Qu.:1.0000   3rd Qu.:0.5860  \n Max.   :0.77023   Max.   :0.47222   Max.   :1.0000   Max.   :0.9745  \n NA's   :3         NA's   :3         NA's   :3        NA's   :3       \n     RENTER        MEDIAN_INCOME    MEDIAN_BUILT_YEAR MEDIAN_HOME_VALUE\n Min.   :0.02548   Min.   : 12328   Min.   :   0      Min.   :  65500  \n 1st Qu.:0.41399   1st Qu.: 38394   1st Qu.:   0      1st Qu.: 168600  \n Median :0.56484   Median : 55062   Median :1945      Median : 248700  \n Mean   :0.54843   Mean   : 63882   Mean   :1148      Mean   : 288966  \n 3rd Qu.:0.70067   3rd Qu.: 80872   3rd Qu.:1957      3rd Qu.: 368300  \n Max.   :1.00000   Max.   :216667   Max.   :2006      Max.   :1245500  \n NA's   :3         NA's   :8        NA's   :3         NA's   :19       \n     OTHER          SEX_RATIO            EDU_HIGH       HOME_VALUES_CAT   \n Min.   :0.8020   Min.   :0.0004454   Min.   :0.01092   Length:788        \n 1st Qu.:0.9636   1st Qu.:0.0008190   1st Qu.:0.15302   Class :character  \n Median :0.9821   Median :0.0009405   Median :0.30406   Mode  :character  \n Mean   :0.9755   Mean   :0.0009558   Mean   :0.38004                     \n 3rd Qu.:0.9960   3rd Qu.:0.0010612   3rd Qu.:0.59072                     \n Max.   :1.0000   Max.   :0.0088278   Max.   :0.93945                     \n NA's   :3        NA's   :3           NA's   :3                           \n  INCOME_CAT        BUILT_YEAR_CAT       NB_CRIMES       CRIME_RATE    \n Length:788         Length:788         Min.   : 1.00   Min.   :0.1563  \n Class :character   Class :character   1st Qu.: 5.00   1st Qu.:1.5722  \n Mode  :character   Mode  :character   Median : 9.00   Median :2.9283  \n                                       Mean   :12.53   Mean   :   Inf  \n                                       3rd Qu.:16.00   3rd Qu.:6.1173  \n                                       Max.   :92.00   Max.   :   Inf  \n                                                                       \n\n\n\ntable(socio_data_attr$INCOME_CAT)\n\n\n  high    low medium \n   151    282    347 \n\n\n\ntable(socio_data_attr$HOME_VALUES_CAT)\n\n\n  high    low medium \n   207    198    364 \n\n\n\ntable(socio_data_attr$BUILT_YEAR_CAT)\n\n\nmedium    new    old \n   312     16    457 \n\n\n\n\n4.4.4.1 Rozkład danych o przestępczości\n\nggplot(socio_data_attr, aes(x = CRIME_RATE)) + geom_histogram() + theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n4.4.4.2 Zależność między odsetkiem przestępstw a zmiennymi społeczno-ekonomicznymi\n\ncor = cor(socio_data_attr[24], socio_data_attr[3:19], method = \"spearman\", use = \"complete.obs\")\nround(cor, 2)\n\n           WHITE BLACK ASIAN HISPANIC FOREIGN_BORN UNEMPL PUBLIC_ASSIST VACANT\nCRIME_RATE -0.58  0.64 -0.49    -0.41        -0.54   0.48          0.55   0.53\n           OCCUPIED OWNER RENTER MEDIAN_INCOME MEDIAN_BUILT_YEAR\nCRIME_RATE    -0.54 -0.35   0.35          -0.5             -0.01\n           MEDIAN_HOME_VALUE OTHER SEX_RATIO EDU_HIGH\nCRIME_RATE             -0.41  0.15     -0.26    -0.37\n\n\nW kolejnym kroku zostaną wyselekcjonowane wartości współczynnika korelacji +/- 0,5, a następnie dla nich zostaną wykonane wykresy rozrzutu.\n\ncor_v = as.vector(cor)\nnames(cor_v) = colnames(cor)\nsel_cor = cor_v[abs(cor)&gt;0.5]\nround(sel_cor, 2)\n\n        WHITE         BLACK  FOREIGN_BORN PUBLIC_ASSIST        VACANT \n        -0.58          0.64         -0.54          0.55          0.53 \n     OCCUPIED \n        -0.54 \n\n\nObjaśnienie zmiennych:\n- WHITE - odsetek ludności białych\n- BLACK - odsetek ludności czarnej \n- FOREIGN_BORN - odsetek ludności urodzonej zagranicą\n- PUBLIC ASSIST - odestek gospodarstw domowych korzystających z pomocy społecznej\n- VACANT - odsetek niezamieszkałych budynków\n- OCCUPIED - odsetek zamieszkałych budynków\nWizualizacja zależności między zmiennymi ze współczynnikiem korelacji +/- 0,5.\n\nlibrary(PerformanceAnalytics)\nlist_variables = names(sel_cor)\nchart.Correlation(socio_data_attr[, c(\"CRIME_RATE\", list_variables)], histogram=TRUE, pch=19, method = \"spearman\")\n\n\n\n\n\n\n\n\n\n\n4.4.4.3 Zmienność odsetka przestępczości w klasach poziomu dochodu, cen domów oraz wieku zabudowy\n\nZależność między odsetkiem przestępczości a poziomem dochodów.\n\nWyróżniono 3 klasy używając zmiennej MEDIAN_INCOME:\n\nlow - wartości dochodów niższe od kwartyla 1.\nmedium - wartości dochodów między 1 i 3 kwartylem.\nhigh - wartości dochodów wyższe do 3. kwartyla.\n\n\nlibrary(dplyr)\nsocio_data_attr %&gt;% na.omit() %&gt;%  \n  mutate(INCOME_CAT = factor(INCOME_CAT,level=c(\"low\", \"medium\", \"high\"))) %&gt;% \nggplot(., aes(x = INCOME_CAT, y = CRIME_RATE)) + \n  geom_boxplot()  + \n  xlab(\"Household income\") + ylab(\"Crime rates\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nZależność między odsetkiem przestępczości a cenami domów.\n\nWyróżniono 3 klasy używając zmiennej MEDIAN_HOME_VALUE:\n\nlow - wartości cen domów niższe od kwartyla 1.\nmedium - wartości cen domów między 1 i 3 kwartylem.\nhigh - wartości cen domów wyższe do 3. kwartyla.\n\n\nsocio_data_attr %&gt;% na.omit() %&gt;%  \n  mutate(HOME_VALUES_CAT = factor(HOME_VALUES_CAT,level=c(\"low\", \"medium\", \"high\"))) %&gt;% \nggplot(., aes(x = HOME_VALUES_CAT, y = CRIME_RATE)) + \n  geom_boxplot()  + \n  xlab(\"Home values\") + ylab(\"Crime rates\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\nZależnośc między odsetkiem przestęczpości, a wiekiem zabudowy.\n\nWyróżnione zostały 3 klasy zabudowy:\n\nold - wybudowane przed 1950,\nmedium - wybudowane między 1950 a 2000,\nnew - wybudowane po 2000.\n\n\nsocio_data_attr %&gt;% na.omit() %&gt;%  \n  mutate(BUILT_YEAR_CAT = factor(BUILT_YEAR_CAT,level=c(\"old\", \"medium\", \"new\"))) %&gt;% \nggplot(., aes(x = BUILT_YEAR_CAT, y = CRIME_RATE)) + \n  geom_boxplot()  + \n  xlab(\"Housing age\") + ylab(\"Crime rates\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n4.4.5 Eksploracyjna przestrzenna analiza danych\nEksploracyjna przestrzenna analiza danych obejmuje przede wszystkim wizualizację zmiennych społeczno-ekonomicznych oraz odsetka przestępczości w celu wizualnego określenia, czy istnieją obszary o wysokim/niskim odsetku przestępczości i czy są one powiązane z wyższymi/niższymi wartościami zmiennych społeczno-ekonomicznych.\nWizualizacja danych ma także na celu sprawdzenie poprawności danych, w tym między innymi identyfikacja danych odstających lokalnie.\n\n4.4.5.1 Odsetek przestępczości\n\ntm_shape(socio_data) + \n  tm_polygons(fill = \"CRIME_RATE\", palette = \"YlOrRd\", n = 4, style = 'jenks')\n\n\n\n\n\n\n\n\n\n\n4.4.5.2 Struktura rasowo-etniczna ludności\n\ntm_shape(socio_data) + \n  tm_polygons(fill = \"WHITE\", palette = \"YlOrRd\")\n\n\n\n\n\n\n\n\n\ntm_shape(socio_data) + \n  tm_polygons(fill = \"BLACK\", palette = \"YlOrRd\") \n\n\n\n\n\n\n\n\n\ntm_shape(socio_data) + \n  tm_polygons(fill = \"FOREIGN_BORN\", palette = \"YlOrRd\", n = 3) \n\n\n\n\n\n\n\n\n\n\n4.4.5.3 Poziom wykształcenia\n\ntm_shape(socio_data) + \n  tm_polygons(fill = \"EDU_HIGH\", palette = \"YlOrRd\")\n\n\n\n\n\n\n\n\n\n\n4.4.5.4 Zabudowa niezamieszkała\n\ntm_shape(socio_data) + \n  tm_polygons(fill = \"VACANT\", palette = \"YlOrRd\")\n\n\n\n\n\n\n\n\n\n\n4.4.5.5 Poziom dochodów\n\ntm_shape(socio_data) + \n  tm_polygons(fill = \"INCOME_CAT\", palette = c(low=\"#FFF68F\", medium=\"#EEB422\", high=\"#FF4040\"))\n\n\n\n\n\n\n\n\n\n\n4.4.5.6 Ceny domów\n\ntm_shape(socio_data) + \n  tm_polygons(fill = \"HOME_VALUES_CAT\", palette = c(low=\"#FFF68F\", medium=\"#EEB422\", high=\"#FF4040\"))\n\n\n\n\n\n\n\n\n\n\n4.4.5.7 Wiek zabudowy\n\ntm_shape(socio_data) + \n  tm_polygons(fill = \"BUILT_YEAR_CAT\", palette = c(old=\"#FFF68F\", medium=\"#EEB422\", new=\"#FF4040\"))\n\n\n\n\n\n\n\n\n\n\n\n4.4.6 Podsumowanie\n\nPrzeprowadzona analiza korelacji wskazała na istnienie słabej zależności:\n\nwprostproporcjonalnej przekraczającej 0.5 między odsetkiem przestępstw na 1000 mieszkańców a odsetkiem czarnych (zmienna BLACK) oraz odsetkiem niezamieszkałej zabudowy (zmienna VACANT) oraz odsetkiem osób korzystających z pomocy społecznej\nodwrotnieproporcjonalnej przekraczającej 0.5 między odsetkiem przestępstw na 1000 miekszańców a odsetkiem ludności białej (zmienna WHITE), odsetkiem zamieszkałej zabudowy (zmienna OCCUPIED) oraz odsetkiem osób urodzonych za granicą.\n\nAnaliza wykresów pudełkowych pokazuje, że odsetek przestępczości przyjmuje wyższe wartości dla klasy o niskich dochodach, niskich cenach domów oraz dla obszarów o nowej zabudowie (powstałej po 2000 roku).\nObszary o niskim dochodzie (184 obszary), a także o małej wartości domów (258 obszarów) są zlokalizowane przede wszystkim w południowej części analizowanego obszaru.\nMapa przestępczości pokazuje wyższe wartości w południowej części analizowanego obszaru oraz środkowej części obszaru.",
    "crumbs": [
      "WSTĘP",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Eksploracyjna analiza danych przestrzennych</span>"
    ]
  },
  {
    "objectID": "06_punkty.html",
    "href": "06_punkty.html",
    "title": "DANE PUNKTOWE",
    "section": "",
    "text": "Wprowadzenie\nAnaliza rozkładu przestrzennego danych punktowych ma na celu określenie w jaki sposób obiekty/zdarzenia są rozmieszczone w przestrzeni.",
    "crumbs": [
      "DANE PUNKTOWE"
    ]
  },
  {
    "objectID": "06_punkty.html#wprowadzenie",
    "href": "06_punkty.html#wprowadzenie",
    "title": "DANE PUNKTOWE",
    "section": "",
    "text": "Czy punkty są rozmieszczone losowo? (tzn. na ich rozmieszczenie nie działa żaden czynnik lub zależą od wielu czynników które się wzajemnie znoszą)\nCzy punkty są romieszczone regularnie? (tj. efekt świafomego działania, nie jest to proces naturalny)\nCzy występują skupiska (klastry) punktów? (tzn. na rozmieszczenie punktów wpływa jakiś czynnik)\n\n\n\nMetody: Statystyki centrograficzne\nStatystyki centrograficzne stanowią przestrzenną modyfikację statystyk opisowych w klasycznej statystyce. Są podstawową formą opisu rozkładu przestrzennego danych punktowych. Do statystyk centrograficznych zalicza się:\n\nśrednią centralną - punkt o współrzędnych stanowiących średnią arytmetyczną długości i szerokości geograficznej obiektów rozproszonych w przestrzeni\nodległość standardową - określa średnią odległość puntków od punktu centralnego (centroidu) i stanowi absolutną miarę roproszenia punktów w przestrzeni\nelipsę odchylenia standardowego - pozwala na określenia kierunków rozrzutu obiektów (obserwacji) w przestrzeni\n\n\nŻródło: https://mgimond.github.io/Spatial/chp11_0.html#centrography\n\n\nMetody oparte na analizie intensywności\nMetody oparte na analizie intensywności analizują rozkład przestrzenny punktów pod względem badanego obszaru.\n\nintensywność zdarzenia (event) jest stała dla całego obszaru\n\nśrednia intensywność (\\(\\lambda\\)) obliczana jako liczba zdarzeń \\(n\\) na jednostkę powierzchni \\(a\\) (\\(\\lambda = \\frac{n}{a}\\))\n\nintensywność zmienia się wraz z lokalizacją\n\ntest zliczania w kwadratach (ang. quadrat count test)\nestymacja gęstości jądra (ang. kernel density estimation)\n\n\n\n\nMetody oparte na odległości\nMetody oparte na odległości wykorzystują informację o odległości między każdą lokalizacją pomiaru/obserwacji, a najbliższym innym pomiarem/obserwacją.\n\nstatystyki najbliższego sąsiada - wykorzystywane do określenia rozkładu odległości pomiędzy każdą lokalizacją pomiaru/obserwacji, a najbliższym innym pomiarem/obserwacją;\nfunkcja G - podsumowuje rozkład odległości do najbliższego sąsiada w postaci dystrybuanty, wykorzystywana do porównania odległości teoretycznych wynikających z losowego rozkładu punktów z odległościami empirycznymi obliczonymi na podstawie danych;\nfunkcja K Ripley’a - wykorzystywana do porównania odległości teoretycznych wynikających z losowego rozkładu punktów z odległościami empirycznymi obliczonymi na podstawie danych;\nwskaźnik Clarka-Evansa (1954) - stosunek między rzeczywistą średnią odległością od najbliższego sąsiada, a oczekiwaną dla rozkładu losowego.",
    "crumbs": [
      "DANE PUNKTOWE"
    ]
  },
  {
    "objectID": "06_punkty.html#analiza-danych-punktowych-w-r",
    "href": "06_punkty.html#analiza-danych-punktowych-w-r",
    "title": "DANE PUNKTOWE",
    "section": "Analiza danych punktowych w R",
    "text": "Analiza danych punktowych w R\n\nPakiet sfdep\nPakiet sfdep pozwala na obliczanie różnych statystyk centrograficznych w oparciu o dane punktowe wczytane za pomocą pakietu sf:\n\ncenter_mean() - średnia centralna\nstd_distance() - odległość standardowa\nstd_dev_ellipse() - elipsa odchylenia standardowego\n\n\n\nPakiet spatstat\nPakiet spatstat dostarcza szeregu funkcji do analizy rozkładu danych punktowych w R. Wzór punktowy w pakiecie spatstat jest obiektem klasy ppp (plannar point pattern). Wiele funkcji z pakietu wymaga zdefiniowania także obszaru analizy (tzw. okna, window).\nPakiet spatstat zawiera funkcje pozwalające na zastosowanie:\n\nmetod opartych na analizie intensywności:\n\nas.ppp - konwertuje dane na obiekty klasy ppp. Wymaga podania obiektu oraz określenia obszaru analizy.\nas.owin - tworzy okno (window) na podstawie innego obiektu (np. granic miasta Poznania)\n\nmetod opartych na odległości\n\npairdist() - zwraca macierz z odległościami między wszystkimi parami punktów w zbiorze danych\nnndist() - zwraca wektor odległości od punktu do najbliżego sąsiada; odległości te są uzyskiwane przez sortowanie odległości między parami punktów i wybierana jest minimalna wartości dla każdego punktu\ndistmap() - oblicza odległość od każdej komórki do najbliższego punktu i zwraca mapę rastrową.\nclarkevans() - obliczenie wskaźnika Clarka-Evansa\nGest() - obliczenie funkcji G\nKest() - obliczenie funkcji K Ripley’a\nenvelope() - estowania hipotezy zerowej dla funkcji G, K Ripley’a\nquadratcount() - test zliczania w kwadratach",
    "crumbs": [
      "DANE PUNKTOWE"
    ]
  },
  {
    "objectID": "07_punkty_centrograficzne.html",
    "href": "07_punkty_centrograficzne.html",
    "title": "5  Statystyki centrograficzne",
    "section": "",
    "text": "5.1 Statystyki centrograficzne\nStatystyki centrograficzne stanowią przestrzenną modyfikację statystyk opisowych w klasycznej statystyce. Są podstawową formą opisu rozkładu przestrzennego danych punktowych. Statystyki centrograficzne oblicza się za pomocą funkcji z pakietu sfdep.\nŻródło: https://mgimond.github.io/Spatial/chp11_0.html#centrography",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statystyki centrograficzne</span>"
    ]
  },
  {
    "objectID": "07_punkty_centrograficzne.html#statystyki-centrograficzne",
    "href": "07_punkty_centrograficzne.html#statystyki-centrograficzne",
    "title": "5  Statystyki centrograficzne",
    "section": "",
    "text": "5.1.1 Średnia centralna\nŚrednia centralna wyznaczona jest przez punkt o współrzędnych \\(\\overline{x_c}\\), \\(\\overline{y_c}\\) stanowiących średnią arytmetyczną długości i szerokości geograficznej obiektów rozproszonych w przestrzeni.\n\\[\\overline{x_c} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\]\n\\[\\overline{y_c} = \\frac{\\sum_{i=1}^{n} y_i}{n}\\]\nStatystyki centrograficzne (średnia centralna, odległość standardowa) mogą być obliczone na podstawie ramki danych zawierającej współrzędne x oraz y. Funkcja st_coordinates() z pakietu sf pozwala na wydobycie współrzędnych z obiektu punktowego klasy sf oraz stworzenie ramki danych zawierającej dwie kolumny: x oraz y.\n\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE\n\np2019 = read_sf(\"data/przestepstwa_2019.gpkg\")\n#granica miasta Poznań\npzn = read_sf(\"data/pzn_borders.gpkg\")\n\np2019_coords = st_coordinates(p2019)\nhead(p2019_coords)\n\n            X        Y\n[1,] 361379.5 503546.3\n[2,] 361691.3 503694.6\n[3,] 360347.3 506216.0\n[4,] 353099.3 504204.1\n[5,] 361715.2 503305.9\n[6,] 360260.9 510415.0\n\n\nŚrednią centralną obliczamy poprzez wyliczenie średniej wartości dla kolumny X oraz Y.\n\n# mean center\nmc = apply(p2019_coords, 2, mean)\nmc\n\n       X        Y \n358716.4 506363.5 \n\n\nWizualizacja średniej centralnej wymaga utworzenia obiektu przestrzennego w oparciu o współrzędne punktu. W tym celu wykorzystanie zostanie funkcja st_point() oraz st_sfc() z pakietu sf.\n\n#Przekształcenie obiektu mc na obiekt przestrzenny klasy sfc_POINT\nmc_point = st_sfc(st_point(mc), crs = st_crs(p2019))\n\n\nplot(mc_point)\n\n\n\n5.1.2 Odległość standardowa\nOdległość standardowa określa średnią odległość puntków (\\(x_{i}\\), \\(y_{i}\\)) od punktu centralnego (centroidu, (\\(\\overline{x_c}\\), \\(\\overline{y_c}\\))) i obliczana jest wg wzoru:\n\\[d = \\sqrt{\\frac{\\sum_{i=1}^{n} d_{ic}^2}{n}}\\]\n\\[d_{ic}^2 = (x_i - \\overline{x_c})^2 + (y_i - \\overline{y_c})^2 \\]\nW powyższym wzorze\n\n\\(x_{i}\\) oznacza współrzędną x dla każdego punktu (p2019_coords[,1] ),\n\n\\(y_{i}\\) oznacza współrzędną y dla każdego punktu (p2019_coords[,2]),\n\\(\\overline{x_c}\\) to współrzędna x dla średniej centralnej (mc[1]),\n\\(\\overline{y_c}\\) to współrzędna y dla średniej centralnej (mc[2]).\n\n\n# standard distance\nsd = sqrt(sum((p2019_coords[,1] - mc[1])^2 + (p2019_coords[,2] - mc[2])^2) / nrow(p2019_coords))\nsd\n\n[1] 3886.935\n\n\nOdległość standardowa graficznie przedstawiana jest jako koło o środku w punkcie wyznaczonym przez średnią centralną oraz promieniu równym odległości standardowej. Do stworzenia obiektu przestrzennego przedstawiającego zasięg odległości standardowej wykorzystana zostanie funkcja st_buffer() z pakietu sf.\n\n#wyznaczenie strefy buforowej wokół średniej centralnej o promieniu równym odległości standardowej\nsd_buffer = st_buffer(mc_point, sd)\n\n\n\nWizualizacja średniej centralnej i odległości standardowej\n\nlibrary(tmap)\n\ntm_shape(pzn) + \n    tm_borders() +\n    tm_shape(p2019) +\n    tm_dots() +\n    tm_shape(mc_point) +\n    tm_symbols(fill = \"red\") +\n    tm_shape(sd_buffer) +\n    tm_borders(col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n5.1.3 Elipsa odchylenia standardowego\nElipsa odchylenia standardowego pozwala na określenia kierunków rozrzutu obiektów (obserwacji) w przestrzeni. W R może być obliczona za pomocą funkcji std_dev_ellipse() z pakietu sfdep.",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statystyki centrograficzne</span>"
    ]
  },
  {
    "objectID": "07_punkty_centrograficzne.html#statystyki-centrograficzne-w-r",
    "href": "07_punkty_centrograficzne.html#statystyki-centrograficzne-w-r",
    "title": "5  Statystyki centrograficzne",
    "section": "5.2 Statystyki centrograficzne w R",
    "text": "5.2 Statystyki centrograficzne w R\nStatystyki centrograficzne w R oblicza sie z wykorzystaniem funkcji z pakietu sfdep. Pakiet sfdep pozwala na obliczanie różnych statystyk centrograficznych w oparciu o dane punktowe wczytane za pomocą pakietu sf:\n\ncenter_mean() - średnia centralna\nstd_distance() - odległość standardowa\nstd_dev_ellipse() - elipsa odchylenia standardowego",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statystyki centrograficzne</span>"
    ]
  },
  {
    "objectID": "07_punkty_centrograficzne.html#przykład-1-przestępczość-w-poznaniu",
    "href": "07_punkty_centrograficzne.html#przykład-1-przestępczość-w-poznaniu",
    "title": "5  Statystyki centrograficzne",
    "section": "5.3 Przykład 1: Przestępczość w Poznaniu",
    "text": "5.3 Przykład 1: Przestępczość w Poznaniu\n\n5.3.1 Dane\nW przykładzie 1 wykorzystano dane dotyczące przestępczości w Poznaniu (plik przestepstwa_2019.gpkg) wraz z granicą miasta Poznania (plik pzn_borders.gpkg). Do podsumowania rozkładu przestępczości wykorzystano statystyki centrograficzne. Plik przestepstwa_2019.gpkg zawiera tylko kolumnę geom bez dodatkowych atrybutów.\n\nlibrary(sf)\n# dane punktowe\np2019 = read_sf(\"data/przestepstwa_2019.gpkg\")\n#granica miasta Poznań\npzn = read_sf(\"data/pzn_borders.gpkg\")\n\n\nhead(p2019)\n\nSimple feature collection with 6 features and 0 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 353099.3 ymin: 503305.9 xmax: 361715.2 ymax: 510415\nProjected CRS: ETRF2000-PL / CS92\n# A tibble: 6 × 1\n                 geom\n          &lt;POINT [m]&gt;\n1 (361379.5 503546.3)\n2 (361691.3 503694.6)\n3   (360347.3 506216)\n4 (353099.3 504204.1)\n5 (361715.2 503305.9)\n6   (360260.9 510415)\n\n\n\nlibrary(tmap)\ntm_shape(pzn) + \n    tm_borders() +\n    tm_shape(p2019) +\n    tm_dots()\n\n\n\n\n\n\n\n\n\n\nObliczanie statystyk centrograficznych - pakiet sfdep\nStatystyki centrograficzne w R można wyznaczyć używając funkcji z pakietu sfdep:\n\ncenter_mean() - średnia centralna\n\n\nlibrary(pracma)\nlibrary(sfdep)\n\n#srednia centralna\nmc_point2 = center_mean(p2019)\n\n\nstd_distance() - odległość standardowa\n\n\nlibrary(pracma)\nlibrary(sfdep)\n\n# odległość standardowa \nsd2 = std_distance(p2019) #zwraca wartość z odległością\nsd2_buffer = st_buffer(mc_point2, sd2) #zwraca obiekt przestrzenny\n\n\nstd_dev_ellipse() - elipsa odchylenia standardowego\n\n\nlibrary(pracma)\nlibrary(sfdep)\n\n# funkcja std_dev_ellipse zwraca parametry elipsy, st_ellipse tworzy na ich podstawie obiekt liniowy \np2019_e = std_dev_ellipse(p2019)\np2019_e = st_ellipse(p2019_e, sx = p2019_e$sx, sy = p2019_e$sy)\n\n\n\n5.3.2 Wizualizacja statystyk centrograficznych\n\ntm_shape(pzn) + \n    tm_borders() +\n    tm_shape(p2019) +\n    tm_dots() +\n    tm_shape(mc_point) +\n    tm_dots(fill = \"red\", size = 2) +\n    tm_shape(sd2_buffer) +\n    tm_borders(col = \"red\", lwd = 3) + \n    tm_shape(p2019_e) +\n    tm_lines(col = \"darkgreen\", lwd = 3)\n\n\n\n\nStatystyki centrograficzne - średnia centralna (czerwony), odległość standardowa (czerwona linia), elipsa odchylenia standardowego (zielony)",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statystyki centrograficzne</span>"
    ]
  },
  {
    "objectID": "07_punkty_centrograficzne.html#przykład-2-rozkład-przestrzenny-szkół-w-poznaniu",
    "href": "07_punkty_centrograficzne.html#przykład-2-rozkład-przestrzenny-szkół-w-poznaniu",
    "title": "5  Statystyki centrograficzne",
    "section": "5.4 Przykład 2: Rozkład przestrzenny szkół w Poznaniu",
    "text": "5.4 Przykład 2: Rozkład przestrzenny szkół w Poznaniu\n\n5.4.1 Dane\nW drugim przykładzie wykorzystano dane pozyskane z BDOT10k z warstwy przedstawiającej budynki (OT_BUBD_A) oraz granice administracyjne (OT_ADJA_A) do przeanalizowania rozkładu przestrzennego szkół podstawowych w Poznaniu.\nGranica miasta Pozania została pozyskana z warstwy OT_ADJA_A.\n\ngranica = read_sf(\"data/bdot10k/PL.PZGiK.308.BDOT10k.3064__OT_ADJA_A.gpkg\")\ngranica = granica[granica$RODZAJ == \"powiat\", \"geometria\"]\n\nWarstwa OT_BUBD_A jest warstwą poligonową zawierającą różne typy budynków.\n\nbudynki &lt;- read_sf(\"data/bdot10k/PL.PZGiK.308.BDOT10k.3064__OT_BUBD_A.gpkg\")\n\n\nhead(budynki)\n\nSimple feature collection with 6 features and 19 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 359155.6 ymin: 500467.5 xmax: 362596.6 ymax: 502295.9\nProjected CRS: ETRF2000-PL / CS92\n# A tibble: 6 × 20\n  TERYT LOKALNYID       PRZESTRZENNAZW WERSJA              POCZATEKWERSJIOBIEKTU\n  &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;dttm&gt;              &lt;dttm&gt;               \n1 3064  3c25a636-5303-… PL.PZGiK.308.… 2023-12-31 12:00:00 2023-12-31 12:00:00  \n2 3064  12e66037-0d58-… PL.PZGiK.308.… 2023-12-31 12:00:00 2023-12-31 12:00:00  \n3 3064  021dfd88-65df-… PL.PZGiK.308.… 2023-12-31 12:00:00 2023-12-31 12:00:00  \n4 3064  5b5acb18-3c3a-… PL.PZGiK.308.… 2023-12-31 12:00:00 2023-12-31 12:00:00  \n5 3064  a4b459f2-551c-… PL.PZGiK.308.… 2023-12-31 12:00:00 2023-12-31 12:00:00  \n6 3064  3fddfa4f-6f68-… PL.PZGiK.308.… 2023-12-31 12:00:00 2023-12-31 12:00:00  \n# ℹ 15 more variables: OZNACZENIEZMIANY &lt;chr&gt;,\n#   ZRODLODANYCHGEOMETRYCZNYCH &lt;chr&gt;, KATEGORIAISTNIENIA &lt;chr&gt;, UWAGI &lt;chr&gt;,\n#   INFORMACJADODATKOWA &lt;chr&gt;, KODKARTO10K &lt;chr&gt;, SKROTKARTOGRAFICZNY &lt;chr&gt;,\n#   KODKST &lt;chr&gt;, FUNKCJAOGOLNABUDYNKU &lt;chr&gt;, PRZEWAZAJACAFUNKCJABUDYNKU &lt;chr&gt;,\n#   LICZBAKONDYGNACJI &lt;dbl&gt;, NAZWA &lt;chr&gt;, FSBUD &lt;chr&gt;, IDEGIB &lt;chr&gt;,\n#   geometria &lt;POLYGON [m]&gt;\n\n\nInformacje o typach budynków zapisane są w dwóch kolumnach: FUNKCJAOGOLNABUDYNKU oraz PRZEWAZAJACAFUNKCJABUDYNKU.\n\nunique(budynki$FUNKCJAOGOLNABUDYNKU)\n\n [1] \"budynki szpitali i inne budynki opieki zdrowotnej\"        \n [2] \"budynki transportu i łączności\"                           \n [3] \"budynki mieszkalne\"                                       \n [4] \"budynki produkcyjne, usługowe i gospodarcze dla rolnictwa\"\n [5] \"zbiorniki, silosy i budynki magazynowe\"                   \n [6] \"budynki przemysłowe\"                                      \n [7] \"budynki oświaty, nauki i kultury oraz budynki sportowe\"   \n [8] \"budynki handlowo-usługowe\"                                \n [9] \"pozostałe budynki niemieszkalne\"                          \n[10] \"budynki biurowe\"                                          \n\n\n\nfuncja_szczegolowa_budynku = unique(budynki$PRZEWAZAJACAFUNKCJABUDYNKU)\n\n\n5.4.1.1 Przygotowanie danych do analizy\n\n\nSelekcja budynków w kategorii szkoła podstawowa\nSzkoły podstawowe to kategoria określona jako przeważająca funkcja budynku (kolumna PRZEWAZAJACAFUNKCJABUDYNKU).\n\nszkoly_podstawowe = budynki[budynki$PRZEWAZAJACAFUNKCJABUDYNKU%in%c(\"szkoła podstawowa\"),]\n\n\n\nKonwersja warstwy poligonowej na punktową\nWarstwa BUBD_A jest warstwą poligonową. Aby zamienić budynki (poligony) na punkty należy użyć funkcji st_centroid() z pakietu sf.\n\nszkoly_pkt = st_centroid(szkoly_podstawowe[, \"geometria\"])\n\n\n\nZapisanie warstw\n\nst_write(szkoly_pkt, dsn = \"data/out_poznan_szkoly.gpkg\", layer = \"szkoly\", delete_dsn = TRUE)\n\nDeleting source `data/out_poznan_szkoly.gpkg' using driver `GPKG'\nWriting layer `szkoly' to data source \n  `data/out_poznan_szkoly.gpkg' using driver `GPKG'\nWriting 84 features with 0 fields and geometry type Point.\n\nst_write(granica, dsn = \"data/out_poznan_szkoly.gpkg\", layer = \"granica\")\n\nWriting layer `granica' to data source \n  `data/out_poznan_szkoly.gpkg' using driver `GPKG'\nWriting 1 features with 0 fields and geometry type Polygon.\n\n\n\n\n5.4.1.2 Lokalizacja szkół podstawowych w mieście Poznań.\nDo wizualizacji lokalizacji szkół podstawowych w mieście Poznań wykorzystano pakiet tmap().\n\nlibrary(tmap)\ntm_shape(granica) + \n  tm_borders(col = \"black\") + \n  tm_shape(szkoly_pkt) + \n  tm_dots(col = \"black\", size = 0.7)\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Statystyki centrograficzne\nWykorzystując funkcje z pakietu sfdep zostały obliczone statystyki centrograficzne: średnia centralna (funkcja center_mean()), odległość standardowa (funkcja std_distance()), elipsa odchylenia standardowego (std_dev_ellipse()).\n\nlibrary(pracma)\nlibrary(sfdep)\n\n#srednia centralna\nmc_point2 = center_mean(szkoly_pkt)\n\n# odległość standardowa \nsd2 = std_distance(szkoly_pkt) #zwraca wartość z odległością\nsd2_buffer = st_buffer(mc_point2, sd2) #zwraca obiekt przestrzenny\n\n# funkcja std_dev_ellipse zwraca parametry elipsy, st_ellipse tworzy na ich podstawie obiekt liniowy \nszkoly_pkt_e = std_dev_ellipse(szkoly_pkt)\nszkoly_pkt_e = st_ellipse(szkoly_pkt_e, sx = szkoly_pkt_e$sx, sy = szkoly_pkt_e$sy)\n\n\n\n5.4.3 Wizualizacja statystyk centrograficznych\n\ntm_shape(granica) + \n  tm_borders(col = \"black\") + \n  tm_shape(szkoly_pkt) + \n  tm_dots(fill = \"black\", size = 0.7) + \n  tm_shape(mc_point2) +\n  tm_dots(fill = \"red\", size = 2) +\n  tm_shape(sd2_buffer) +\n  tm_borders(col = \"red\", lwd = 3) + \n  tm_shape(szkoly_pkt_e) +\n  tm_lines(col = \"darkgreen\", lwd = 3)\n\n\n\n\nStatystyki centrograficzne - średnia centralna (czerwony), odległość standardowa (czerwona linia), elipsa odchylenia standardowego (zielony)",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Statystyki centrograficzne</span>"
    ]
  },
  {
    "objectID": "08_punkty_odleglosc.html",
    "href": "08_punkty_odleglosc.html",
    "title": "6  Metody oparte na odległości",
    "section": "",
    "text": "6.1 Statystyki najbliższego sąsiada\nMetody oparte na odległości wykorzystują informację o odległości między każdą lokalizacją pomiaru/obserwacji, a najbliższym innym pomiarem/obserwacją.\nStatystyki najbliższego sąsiada wykorzystywane są do określenia rozkładu odległości pomiędzy każdą lokalizacją pomiaru/obserwacji, a najbliższym innym pomiarem/obserwacją.",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Metody oparte na odległości</span>"
    ]
  },
  {
    "objectID": "08_punkty_odleglosc.html#wskaźnik-clarka-evansa",
    "href": "08_punkty_odleglosc.html#wskaźnik-clarka-evansa",
    "title": "6  Metody oparte na odległości",
    "section": "6.2 Wskaźnik Clarka-Evansa",
    "text": "6.2 Wskaźnik Clarka-Evansa\nWskaźnik Clarka-Evansa (1954) określa stosunek między rzeczywistą średnią odległością od najbliższego sąsiada, a oczekiwaną dla rozkładu losowego.\n\nWartości niższe od 1 wskazują na występowanie skupień punktów\nWartość równa 1 wskazuje na losowy rozkład punktów\nWartości wyższe od 1 wskazują na bardziej regularny ich rozkład (np., dla regularnej siatki heksagonalnej wartość wskaźnika wynosi 2,15)",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Metody oparte na odległości</span>"
    ]
  },
  {
    "objectID": "08_punkty_odleglosc.html#funkcje-podsumowujące-g-k-ripleya",
    "href": "08_punkty_odleglosc.html#funkcje-podsumowujące-g-k-ripleya",
    "title": "6  Metody oparte na odległości",
    "section": "6.3 Funkcje podsumowujące G, K Ripley’a",
    "text": "6.3 Funkcje podsumowujące G, K Ripley’a\n\nfunkcja G - podsumowuje rozkład odległości do najbliższego sąsiada w postaci dystrybuanty, wykorzystywana do porównania odległości teoretycznych wynikających z losowego rozkładu punktów z odległościami empirycznymi obliczonymi na podstawie danych.\nfunkcja K Ripley’a - wykorzystywana do porównania odległości teoretycznych wynikających z losowego rozkładu punktów z odległościami empirycznymi obliczonymi na podstawie danych.",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Metody oparte na odległości</span>"
    ]
  },
  {
    "objectID": "08_punkty_odleglosc.html#metody-oparte-na-odległości-w-r",
    "href": "08_punkty_odleglosc.html#metody-oparte-na-odległości-w-r",
    "title": "6  Metody oparte na odległości",
    "section": "6.4 Metody oparte na odległości w R",
    "text": "6.4 Metody oparte na odległości w R\nPakiet spatstat() dostarcza kilku funkcji pozwalających na wykorzystanie metod opartych na odległości do analizy rozkładu przestrzennego punktów.\n\npairdist() - zwraca macierz z odległościami między wszystkimi parami punktów w zbiorze danych\nnndist() - zwraca wektor odległości od punktu do najbliżego sąsiada; odległości te są uzyskiwane przez sortowanie odległości między parami punktów i wybierana jest minimalna wartości dla każdego punktu\ndistmap() - oblicza odległość od każdej komórki do najbliższego punktu i zwraca mapę rastrową.\nclarkevans() - obliczenie wskaźnika Clarka-Evansa\nGest() - obliczenie funkcji G\nKest() - obliczenie funkcji K\nenvelope() - estowania hipotezy zerowej dla funkcji G, K\n\nWykorzystanie funkcji z pakietu spatstat wymaga najpierw przekształcenia danych do obiektu klasy ppp (plannar point pattern).\n\nfunkcja ppp() jest używana do tworzenia obiektu punktowego klasy ppp na podstawie ramki danych zawierającej współrzędne x oraz y.\nfunkcja as.ppp() pozwala na przekształcenie obiektu przestrzennego do klasy ppp",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Metody oparte na odległości</span>"
    ]
  },
  {
    "objectID": "08_punkty_odleglosc.html#przykład-1-przestępczość-w-poznaniu",
    "href": "08_punkty_odleglosc.html#przykład-1-przestępczość-w-poznaniu",
    "title": "6  Metody oparte na odległości",
    "section": "6.5 Przykład 1: Przestępczość w Poznaniu",
    "text": "6.5 Przykład 1: Przestępczość w Poznaniu\nW przykładzie 1 wykorzystano dane dotyczące przestępczości w Poznaniu (plik przestepstwa_2019.gpkg) wraz z granicą miasta Poznania (plik pzn_borders.gpkg). Do podsumowania rozkładu przestępczości wykorzystano statystyki centrograficzne. Plik przestepstwa_2019.gpkg zawiera tylko kolumnę geom bez dodatkowych atrybutów.\n\nlibrary(sf)\n# dane punktowe\np2019 = read_sf(\"data/przestepstwa_2019.gpkg\")\n#granica miasta Poznań\npzn = read_sf(\"data/pzn_borders.gpkg\")\n\n\nlibrary(tmap)\ntm_shape(pzn) + \n    tm_borders() +\n    tm_shape(p2019) +\n    tm_dots()\n\n\n\n\n\n\n\n\nWykorzystanie funkcji z pakietu spatstat wymaga najpierw przekształcenia danych do obiektu klasy ppp. W tym celu wykorzystuje się funkcję as.ppp(), która pozwala na przekształcenie obiektu przestrzennego do klasy ppp.\n\nlibrary(spatstat)\np2019_ppp = as.ppp(st_geometry(p2019))\n\n\n6.5.1 Statystyki najbliższego sąsiada\n\n6.5.1.1 Obliczenie odległości między punktami\nWyliczenie statystyk najbliższego sąsiada wymaga obliczenia odległości między punktami. Pakiet spatstat dostarcza 3 funkcje do obliczania odległości euklidesowych:\n\npairdist(): zwraca macierz z odległościami między wszystkimi parami punktów w zbiorze danych.\n\n\np2019_pair = pairdist(p2019_ppp)\np2019_pair[1:5, 1:5]\n\n          [,1]      [,2]     [,3]     [,4]      [,5]\n[1,]    0.0000  345.3003 2862.262 8306.265  412.9845\n[2,]  345.3003    0.0000 2857.224 8607.106  389.4981\n[3,] 2862.2616 2857.2245    0.000 7522.023 3215.6132\n[4,] 8306.2651 8607.1061 7522.023    0.000 8662.6305\n[5,]  412.9845  389.4981 3215.613 8662.631    0.0000\n\n\nW macierzy wyznaczonej za pomocą funkcji pairdist() pierwszy wiersz zawiera odległości między punktem id = 1 a wszystkimi innymi. Jeśli posortujemy te wartości otrzymamy ciąg od 0 (odległość między punktem 1 i 1 jest równa 0). Druga wartość w tym posortowanym wektorze to odległość do najbliższego sąsiada.\n\nsort(p2019_pair[1,])[1:5]\n\n[1]   0.00000  42.45441  59.10611  73.24603 137.13290\n\n\n\nnndist(): zwraca wektor odległości od punktu do najbliżego sąsiada; odległości te są uzyskiwane przez sortowanie odległości między parami punktów i wybierana jest minimalna wartości dla każdego punktu\n\n\np2019_nn1 = nndist(p2019_ppp)\np2019_nn1[1:5]\n\n[1]  42.45441  32.89107  69.73521  56.48927 243.91727\n\n\nWektor ten można następnie podsumować za pomocą statystyk opisowych otrzymując wartości statystyk najbliższego sąsiada.\n\ndistmap() - oblicza odległość od każdej komórki do najbliższego punktu i zwraca mapę rastrową.\n\n\nplot(distmap(p2019_ppp))\n\n\n\n\n\n\n\n\n\n\n6.5.1.2 Analiza statystyk najbliższego sąsiada\nFunkcja nndist() dla każdego punktu zwraca odległość do najbliższego punktu - sąsiada. Na tej podstawie możemy obliczyć statystyki opisowe oraz przeanalizować jak rozkładają się te odległości w analizowanym zbiorze danych.\n\np2019_nn1 = nndist(p2019_ppp)\n#przekształcenie na ramkę danych jest potrzebne do wizualizacji wyników z wykorzystaniem pakietu ggplot2\np2019_nn1_df = data.frame(shortest_distance = p2019_nn1)\nhead(p2019_nn1_df)\n\n  shortest_distance\n1          42.45441\n2          32.89107\n3          69.73521\n4          56.48927\n5         243.91727\n6         178.30293\n\n\n\nsummary(p2019_nn1)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   2.669   42.324   78.596  122.883  148.820 3045.719 \n\n\n\nround(quantile(p2019_nn1, probs = seq(0, 1, 0.1)), 2)\n\n     0%     10%     20%     30%     40%     50%     60%     70%     80%     90% \n   2.67   22.39   35.75   47.97   63.84   78.60  100.06  128.50  171.23  250.91 \n   100% \n3045.72 \n\n\nPodsumowanie wyników dotyczących statystyk najbliższego sąsiada\n\n50% punktów znajduje się w odległości mniejszej lub równiej ok. 79 m od najbliższej próbki\n90% punktów w odległości mniejszej lub równiej ok. 251 m od najbliższej próbki\n\nOdległości między najbliższymi punktami:\n\nMinimalna: 3 m\nMaksymalna: 3046 m\nŚrednia: 123 m\n\nRozkład wartości odległości można także przeanalizować za pomocą histogramu.\n\nlibrary(ggplot2)\nggplot(p2019_nn1_df, aes(x = shortest_distance)) + \n  geom_histogram(binwidth = 100) + \n  labs(x = \"Odległość do najbliższego sąsiada [m]\") + \n  theme_bw()\n\n\n\n\n\n\n\n\nOdległości do najbliższego sąsiada można także przedstawić w postaci dystrybuanty (CDF, cumulative distribution function). Na osi x przedstawiona jest odległość, a na osi y procentowy udział punktów w zbiorze danych. Z wykresu można odczytać, jaki procent punktów ma najbliższego sąsiada w odległości mniejszej lub równej podanej wartości.\n\n# cumulative distribution funcion \n## stat_ecdf - oblicza cdf (cumulative distribution funcion)\nggplot(p2019_nn1_df, aes(shortest_distance)) + \n  stat_ecdf(aes(ymin = 0, ymax = after_stat(y)), geom = \"ribbon\", fill = \"grey95\")+\n  stat_ecdf() +\n  labs(x = \"Odległość do najbliższego sąsiada (m)\",\n       y = \"Udział punktów w zbiorze danyh (%)\") +\n  scale_y_continuous(labels = scales::percent) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n6.5.2 Funkcje podsumowujące G oraz K Ripley’a\n\n6.5.2.1 Funkcja G\nFunkcje G podsumowuje rozkład odległości do najbliższego sąsiada w postaci dystrybuanty (CDF, cumulative distribution function). Wykorzystywana jest do porównania odległości teoretycznych (\\(G_t\\)) wynikających z losowego rozkładu punktów z odległościami empirycznymi (\\(G_{emp}\\)) obliczonymi na podstawie danych.\nFunkcja G obliczana jest z wykorzystaniem funkcji Gest() z pakietu spatstat. Wykres pokazuje empiryczną funkcję G (\\(\\hat{G}_{raw}\\)) obliczoną na podstawie danych (czarna linia) oraz teoretyczną wygenerowaną na podstawie rozkładu Poissona (\\(G_{pois}\\), odpowiadająca losowemu rozkładowi punktów, czerwona przerywana linia).\n\n\\(\\hat{G}_{raw} &gt; G_{pois}\\) - punkty zlokalizowane bliżej niż to wynika z rozkładu losowego (może wskazywać na istnienie klastrów)\n\\(\\hat{G}_{raw} &lt; G_{pois}\\) - punkty zlokalizowane dalej niż to wynika z rozkładu losowego (może wskazywać na rozkład regularny lub rozproszony)\n\n\np2019_g = Gest(p2019_ppp, correction = \"none\")\nplot(p2019_g)\n\n\n\n\n\n\n\n\nW przykładzie funkcja empiryczna (\\(\\hat{G}_{raw}\\)) przyjmuje wyższe wartości niż funkcja teoretyczna (czarna linia znajduje się powyżej czerwonej) - wskazuje to, że punkty zlokalizowane są bliżej siebie niż to wynika z rozkładu losowego (może wskazywać na istnienie klastrów w danych).\n\n\n6.5.2.2 Funkcja K Ripley’a\nFunkcja K Ripley’a wykorzystywana jest do porównania odległości teoretycznych wynikających z losowego rozkładu punktów z odległościami empirycznymi obliczonymi na podstawie danych. Funkcja K Ripley’a obliczana jest z wykorzystaniem funkcji Kest() z pakietu spatstat. Wykres pokazuje empiryczną funkcję K Ripley’a (\\(\\hat{K}_{un}\\)) obliczoną na podstawie danych (czarna linia) oraz teoretyczną wygenerowaną na podstawie rozkładu Poissona (\\(K_{pois}\\); odpowiadająca losowemu rozkładowi punktów). \\(\\hat{K}_{un} &lt; K_{pois}\\) wskazuje na rozrzut punktów, punkty są otoczone przez mniejszą liczbę punktów niż można by oczekiwać w porównaniu do rozkładu losowego.\nW poniższym przykładzie funkcja empiryczna (\\(\\hat{K}_{un}\\)) przyjmuje wyższe wartości niż funkcja teoretyczna (czarna linia znajduje się powyżej czerwonej) - wskazuje to, że punkty zlokalizowane są bliżej siebie niż to wynika z rozkładu losowego (może wskazywać na istnienie klastrów w danych).\n\np2019_k = Kest(p2019_ppp, correction = \"none\")\nplot(p2019_k)\n\n\n\n\n\n\n\n\n\n\n6.5.2.3 Testowanie istotności hipotezy dotyczącej rozkładu\nW przypadku przestrzennej analizy rozkładów punktów istotne jest przetestowanie hipotezy dotyczącej rozkładu danych.\n\nHipoteza zerowa zakłada, że zdarzenia są rozłożone losowo - tj. wykazują przestrzenną przypadkowość (spatial randomeness -&gt; absence of pattern).\nOdrzucenie hipotezy zerowej wskazuje na istnienie jakiś struktur/wzroców przestrzennych (np. skupienia punktów).\n\nDla funkcji G oraz K Ripley’a nie ma formalnego schematu testowania. Wykorzystuje się w tym celu obwiednię (ang. envelope). Stosując symulacje konstruuje się obszar “nieodrzucania hipotezy zerowej”, a następnie sprawdza się czy krzywa empiryczna mieści się w tym przedziale (nazywanym obwiednią). Jeśli krzywa empiryczna znajduje się poza obwiednią, oznacza to, że dane nie wykazują rozkładu losowego.\nFunkcja envelope() z pakietu spatstat tworzy obwiednie dla funkcji G oraz K Ripley’a. Funkcja ta wymaga zdefiniowania:\n\nobiektu klasy ppp (w przykładzie p2019_ppp);\nfunkcji dla której ma być wyznaczona obwiednia (np. Gest);\nliczby symulacji nsim (domyślnie 99);\ndodatkowych argumentów funkcji.\n\nW poniższym przykładzie krzywa empiryczna (czarna linia) znajduje się poza zakresem obwiedni (szary kolor), zatem odrzucamy hipotezę o rozkładzie losowym.\n\np2019_env2 = envelope(p2019_ppp, Gest, nsim = 99, \n                      funargs = list(correction = \"none\"))\nplot(p2019_env2)\n\n\n\n\n\n\n\n\n\np2019_env1 = envelope(p2019_ppp, Kest, nsim = 99, \n                      funargs = list(correction = \"none\"))\nplot(p2019_env1)\n\n\n\n\n\n\n\n\n\n\n\n6.5.3 Wskaźnik Clarka-Evansa\nWskaźnik Clarka-Evansa R (1954) określa stosunek między rzeczywistą średnią odległością od najbliższego sąsiada, a oczekiwaną dla rozkładu losowego.\n\nWartości niższe od 1 wskazują na występowanie skupień punktów\nWartość równa 1 wskazuje na losowy rozkład punktów\nWartości wyższe od 1 wskazują na bardziej regularny ich rozkład (np., dla regularnej siatki heksagonalnej wartość wskaźnika wynosi 2,15)\n\nWskaźnik Clarka-Evansa (1954) obliczany jest wykorzystując funkcję clarkevans() z pakietu spatstat(). W wyniku otrzymuje się 3 wartości w zależności czy została zastosowana\n\nclarkevans(p2019_ppp)[[1]]\n\n[1] 0.5152518\n\n\nWartość wskaźnika wskazuje na występowanie skupień punktów.\nFunkcja clarkevans.test() z pakietu spatstat() pozwala na przeprowadzenie testu dotyczącego agregacji danych w oparciu o wskaźnik Clarka-Evansa.\n\nHipoteza zerowa zakłada, że zdarzenia są rozłożone losowo - tj. wykazują przestrzenną przypadkowość (spatial randomeness -&gt; absence of pattern).\nHipoteza alternatywna (określana przez argument alternative) zakłada:\n\nalternative=“less” lub alternative=“clustered”: hipozeta alternatywna zakłada, że wartość wskaźnika Clarka-Evansa jest mniejsza od 1 (R&lt;1) co wskazuje na występowanie skupień punktów\nalternative=“greater” lub alternative=“regular”: hipozeta alternatywna zakłada, że wartość wskaźnika Clarka-Evansa jest większa od 1 (R&gt;1) co wskazuje na regularny rozkład punktów.\nalternative=“two.sided”: hipoteza alternatywna zakłada, że wartość wskaźnika Clarka-Evansa jest różna od 1 co wskazuje, że rozkład nie jest losowy (może być skupiony lub regularny).\n\n\nW poniższym przykładzie założono hipotezę altenatywną (alternative = “two.sided”), zakładającą, że rozkład punktów nie jest losowy. Wynik testu Clarka-Evansa wskazuje na wartość wskaźnika (R) równą 0.51. Wartość p-value (p-value &lt; 2.2e-16) wskazuje na możliwość odrzucenia hipotezy zerowej (rozkład losowy) - punkty nie mają rozkładu losowego.\n\nclarkevans.test(p2019_ppp, alternative = \"two.sided\")\n\n\n    Clark-Evans test\n    Donnelly correction\n    Z-test\n\ndata:  p2019_ppp\nR = 0.51, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nW poniższym przykładzie założono hipotezę altenatywną (alternative = “clustered”), zakładającą, że rozkład punktów jest skupiony. W tym przypadku wynik testu Clarka-Evansa wskazuje na wartość wskaźnika (R) równą 0.51. Wartość p-value (p-value &lt; 2.2e-16) wskazuje na możliwość odrzucenia hipotezy zerowej (rozkład losowy), na rzecz hipotezy alternatywnej - skupiony rozkład punktów.\n\nclarkevans.test(p2019_ppp, alternative = \"clustered\")\n\n\n    Clark-Evans test\n    Donnelly correction\n    Z-test\n\ndata:  p2019_ppp\nR = 0.51, p-value &lt; 2.2e-16\nalternative hypothesis: clustered (R &lt; 1)\n\n\nW poniższym przykładzie założono hipotezę altenatywną (alternative = “clustered”), zakładającą, że rozkład punktów jest regularny. W tym przypadku wynik testu Clarka-Evansa wskazuje na wartość wskaźnika (R) równą 0.51. Wartość p-value (p-value = 1) wskazuje na brak podstaw do odrzucena hipotezy zerowej. Innymi słowy, nie możemy przyjąć hipotezy alternatywnej wskazującej na regularny rozkładu punktów.\n\nclarkevans.test(p2019_ppp, alternative = \"regular\")\n\n\n    Clark-Evans test\n    Donnelly correction\n    Z-test\n\ndata:  p2019_ppp\nR = 0.51, p-value = 1\nalternative hypothesis: regular (R &gt; 1)",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Metody oparte na odległości</span>"
    ]
  },
  {
    "objectID": "08_punkty_odleglosc.html#przykład-2-rozkład-przestrzenny-szkół-w-poznaniu",
    "href": "08_punkty_odleglosc.html#przykład-2-rozkład-przestrzenny-szkół-w-poznaniu",
    "title": "6  Metody oparte na odległości",
    "section": "6.6 Przykład 2: Rozkład przestrzenny szkół w Poznaniu",
    "text": "6.6 Przykład 2: Rozkład przestrzenny szkół w Poznaniu\nW poniższym przykładzie przeanalizowano przestrzenny rozkład szkół podstawowych w Poznaniu wykorzystując metody oparte na odległości:\n\nstatystyki najbliższego sąsiada\nfunkcje podsumowujące G, K Ripley’a\nwskaźnik Clarka-Evansa\n\n\nlibrary(sf)\nlibrary(spatstat)\n#wczytanie danych przestrzennych. Granica jest wykorzystywana wyłącznie do wizualizacji danych. \nszkoly = read_sf(\"data/out_poznan_szkoly.gpkg\", layer = \"szkoly\")\ngranica = read_sf(\"data/out_poznan_szkoly.gpkg\", layer = \"granica\")\n\n#konwersja do obiektu klasy ppp \nszkoly_ppp = as.ppp(szkoly)\n\n\n6.6.1 Statystyki najbliższego sąsiada\nWykorzystując funkcję nndist tworzony jest wektor wartości zawierający odległość do najbliższego sąsiada. Wektor ten następnie podsumowywany jest za pomocą statystyk opisowych oraz wykresów (histogramu, dystrybuanty).\n\n# obliczenie odległości do najbliższego sąsiada \nszkoly_nn = nndist(szkoly_ppp)\nszkoly_nn_df = data.frame(shortest_distance = szkoly_nn)\nsummary(szkoly_nn)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  11.56   25.36   54.13  372.06  497.66 2902.80 \n\n\n\nlibrary(ggplot2)\nggplot(szkoly_nn_df, aes(x = shortest_distance)) + \n  geom_histogram() + \n  labs(x = \"Odległość do najbliższego sąsiada [km]\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n# cumulative distribution funcion \n## stat_ecdf - oblicza cdf (cumulative distribution funcion)\nggplot(szkoly_nn_df, aes(shortest_distance)) + \n  stat_ecdf(aes(ymin = 0, ymax = after_stat(y)), geom = \"ribbon\", fill = \"grey95\")+\n  stat_ecdf() +\n  labs(x = \"Odległość do najbliższego sąsiada (km)\",\n       y = \"Udział punktów w zbiorze danyh (%)\") +\n  scale_y_continuous(labels = scales::percent) + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n6.6.2 Funkcje podsumowujące G, K Ripley’a\nWykorzystując funkcje Gest(), Kest() oraz envelope() obliczane są funkcje podsumowujące G oraz K Ripley’a oraz testowana jest dla nich hipoteza dotycząca rozkładu przestrzennego punktów.\n\nszkoly_env_g = envelope(szkoly_ppp, Gest, nsim = 99, \n                      funargs = list(correction = \"none\"))\nplot(szkoly_env_g)\n\n\n\n\n\n\n\n\n\nszkoly_env_k = envelope(szkoly_ppp, Kest, nsim = 99, \n                      funargs = list(correction = \"none\"))\nplot(szkoly_env_k)\n\n\n\n\n\n\n\n\n\n\n6.6.3 Wskaźnik Clarka-Evansa\n\nclarkevans(szkoly_ppp)[[1]]\n\n[1] 0.5404279\n\n\n\nJaki jest rozkład szkół w Poznaniu? Zintepretuj powyższe wyniki.",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Metody oparte na odległości</span>"
    ]
  },
  {
    "objectID": "09_punkty_intensywnosc.html",
    "href": "09_punkty_intensywnosc.html",
    "title": "7  Metody oparte na analizie intensywności",
    "section": "",
    "text": "7.1 Metody oparte na analizie intensywności w R\nW analizie intensywności rozkład przestrzenny punktów jest charakteryzowany względem określonego obszaru badań (np. granica miasta Poznań). Rozkład charakteryzowany jest za pomocą gęstości \\(\\lambda\\) - liczby zdarzeń (\\(n\\)) na jednostkę powierzchni (\\(a\\)) obliczanej na podstawie danych.\nObliczenie gęstości wymaga przekształcenia obiektów na typ ppp (plannar point pattern) oraz zdefiniowanie obszaru analizy (tzw. okna, ang. window).\nW tym celu zostaną wykorzystane dwie funkcje z pakietu spatstat:\nPakiet spatstat dostarcza także funkcji:\nPonadto funkcja summary() zastosowana do obiektu klasy ppp dostarcza informacji o średniej intensywności, liczbie puntków oraz powierzchni analizowanego obszaru.",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metody oparte na analizie intensywności</span>"
    ]
  },
  {
    "objectID": "09_punkty_intensywnosc.html#metody-oparte-na-analizie-intensywności-w-r",
    "href": "09_punkty_intensywnosc.html#metody-oparte-na-analizie-intensywności-w-r",
    "title": "7  Metody oparte na analizie intensywności",
    "section": "",
    "text": "as.ppp - konwertuje dane na obiekty klasy ppp. Wymaga podania obiektu oraz określenia obszaru analizy.\nas.owin - tworzy okno (window) na podstawie innego obiektu (np. granic miasta Poznania)\n\n\n\nquadratcount() - zliczanie punktów w kwadratach\nquadrat.test() - test zliczania w kwadratach\ndensity() - analiza gęstości jądra",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metody oparte na analizie intensywności</span>"
    ]
  },
  {
    "objectID": "09_punkty_intensywnosc.html#przykład-1-przestępczość-w-poznaniu",
    "href": "09_punkty_intensywnosc.html#przykład-1-przestępczość-w-poznaniu",
    "title": "7  Metody oparte na analizie intensywności",
    "section": "7.2 Przykład 1: Przestępczość w Poznaniu",
    "text": "7.2 Przykład 1: Przestępczość w Poznaniu\nW przykładzie 1 wykorzystano dane dotyczące przestępczości w Poznaniu (plik przestepstwa_2019.gpkg) wraz z granicą miasta Poznania (plik pzn_borders.gpkg). Do podsumowania rozkładu przestępczości wykorzystano statystyki centrograficzne. Plik przestepstwa_2019.gpkg zawiera tylko kolumnę geom bez dodatkowych atrybutów.\n\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE\n\n# dane punktowe\np2019 = read_sf(\"data/przestepstwa_2019.gpkg\")\n#granica miasta Poznań\npzn = read_sf(\"data/pzn_borders.gpkg\")\n\n\n7.2.1 Analiza intensywności\nW analizie intensywności rozkład przestrzenny punktów jest charakteryzowany względem określonego obszaru badań (np. granica miasta Poznań). Rozkład charakteryzowany jest za pomocą gęstości - liczby zdarzeń (\\(n\\)) na jednostkę powierzchni (\\(a\\)) obliczanej na podstawie danych.\n\\[\\hat{\\lambda} = \\frac{n}{a}\\]\nObliczenie gęstości wymaga przekształcenia obiektów na typ ppp (plannar point pattern) oraz zdefiniowanie obszaru analizy (tzw. okna, ang. window). Funkcja as.ppp() z pakiety spatstat służy do przekonwertowania danych na obiekty klasy ppp. Wymaga podania obiektu oraz określenia obszaru analizy. Funkcja as.owin() z pakietu spatstat tworzy okno (window) na podstawie innego obiektu (np. granicy miasta Poznania).\n\nlibrary(spatstat)\n#przekształcenie obiektu sf na obiekt owin definiujący okno (obszar analizy) dla obiektów ppp\npzn_owin = as.owin(pzn)\n#przekształcenie daych punktowych na obiekt ppp. \np2019_ppp = as.ppp(st_geometry(p2019), W = pzn_owin)\n\nObiekt p2019_ppp klasy ppp zawiera informację o liczbie punktów położonych w analizowanym obszarze oraz o współrzędnych oganiczających obszar analizy.\n\np2019_ppp\n\nPlanar point pattern: 1661 points\nwindow: polygonal boundary\nenclosing rectangle: [345943.2, 368927.8] x [493660, 517878.9] units\n\n\nFunkcja summary() zastosowana dla obiektu klasy ppp dostarcza dodatkowych informacji, w tym o średniej intensywności (avarage intensity) - tj. liczbie punktów na jednostkę powierzchni oraz powierzchni analizowanego obszaru (window area) w jednostkach układu współrzędnych.\n\nsummary(p2019_ppp)\n\nPlanar point pattern:  1661 points\nAverage intensity 6.347623e-06 points per square unit\n\nCoordinates are given to 10 decimal places\n\nWindow: polygonal boundary\nsingle connected closed polygon with 3139 vertices\nenclosing rectangle: [345943.2, 368927.8] x [493660, 517878.9] units\n                     (22980 x 24220 units)\nWindow area = 261673000 square units\nFraction of frame area: 0.47\n\n\nDane dla miasta Poznania są w PUWG1992, w którym jednostki są podane w metrach. Obiekt klasy ppp można dowolnie przeszkalować. Poniżej obiekt zostanie przeskalowany z metrów na km.\n\np2019_ppp2 = rescale(p2019_ppp, 1000)\n\n\nsummary(p2019_ppp2)\n\nPlanar point pattern:  1661 points\nAverage intensity 6.347623 points per square unit\n\nCoordinates are given to 13 decimal places\n\nWindow: polygonal boundary\nsingle connected closed polygon with 3139 vertices\nenclosing rectangle: [345.9432, 368.9278] x [493.66, 517.8789] units\n                     (22.98 x 24.22 units)\nWindow area = 261.673 square units\nFraction of frame area: 0.47\n\n\nAby uzyskać tylko informację o gęstości punktów można użyć polecenia:\n\nsummary(p2019_ppp2)$intensity\n\n[1] 6.347623\n\n\nAnaliza intensywności zależna jest od przyjętego obszaru analizy. Poniżej jako obszar analizy przyjmiemy obwiednię obszaru miasta Poznania (bounding box).\n\npzn_owin_bb = as.owin(st_bbox(pzn))\np2019_ppp_bb = as.ppp(st_geometry(p2019), W = pzn_owin_bb)\np2019_ppp_bb2 = rescale(p2019_ppp_bb, 1000)\nsummary(p2019_ppp_bb2)\n\nPlanar point pattern:  1661 points\nAverage intensity 2.983862 points per square unit\n\nCoordinates are given to 13 decimal places\n\nWindow: rectangle = [345.9432, 368.9278] x [493.66, 517.8789] units\n                    (22.98 x 24.22 units)\nWindow area = 556.661 square units\n\n\nPrzyjmując jako obszar analizy zasięg warstwy (bounding box) zamiast granicy miasta Poznania średnia intensywność zmienia się z 6,34 (granica Poznania) do 2,98. Poniższa wizualizacja pokazuje zakres granicy Poznania (niebieska linia), zasięgu warstwy (szary poligon) oraz położenia punktów.\n\nlibrary(tmap)\n\nbb = st_as_sfc(st_bbox(pzn))\ntm_shape(bb) + \n  tm_borders(fill = \"grey\") + \ntm_shape(pzn) + \n  tm_borders(col = \"blue\") + \ntm_shape(p2019) + \n  tm_dots()\n\n\n\n\n\n\n\n\n\n\n7.2.2 Zliczenie punktów w siatce\nIntensywność punktów może nie być stała dla całego obszaru, a zmieniać się wraz z lokalizacją. Do zliczenia punktów w siatce można użyć funkcji quadratcount() z pakietu spatstat. Wynik można przedstawić numerycznie lub graficznie. Funkcja quadratcount() wymaga podania nazwy obiektu klasy ppp (p2019_pp) oraz liczby komórek w osi x oraz y (parametr nx oraz ny).\n\np2019_count = quadratcount(p2019_ppp, nx = 4, ny = 4)\nplot(quadratcount(p2019_ppp, nx = 4, ny = 4), main = \"Liczba przestępstw\")\n\n\n\n\n\n\n\n\n\np2019_count\n\ntile\nTile row 1, col 1 Tile row 1, col 2 Tile row 1, col 3 Tile row 2, col 1 \n                2                14                61                 8 \nTile row 2, col 2 Tile row 2, col 3 Tile row 2, col 4 Tile row 3, col 1 \n              200               636                26                11 \nTile row 3, col 2 Tile row 3, col 3 Tile row 3, col 4 Tile row 4, col 2 \n              230               408                48                 0 \nTile row 4, col 3 Tile row 4, col 4 \n               15                 2 \n\n\nAby otrzymać bardziej zawansowaną wizualizację należy utworzyć siatkę jako obiekt przestrzenny pakietu sf\n\nTworzenie siatki - pakiet sf\n\n\n#utworzenie poligonu o zasiegu warstwy pzn z granicami miasta Poznania\npzn_bb = st_sf(geom = st_as_sfc(st_bbox(pzn)))\n#utworzenie siatki, parametr n oznacza liczbę oczek siatki. \npzn_grid = st_make_grid(pzn_bb, n = 4)\n#obiekt przestrzenny zawierający siatkę \npzn_grid_sf = st_sf(geometry = pzn_grid)\n\n\nplot(pzn_grid_sf)\n\n\n\n\n\n\n\n\n\nZliczenie liczby punktów w każdym oczku siatki\n\nLiczba punktów w każdym oczku siatki zostanie obliczona poprzez przecięcie dwóch obiektów przestrzennych: pzn_grid zawierającego siatkę oraz warstwy punktowej p2019. W poniższym poleceniu funkcja st_intersects() przecina dwa obiekty oraz zwraca listę. Każdy element listy to jedno oczko siatki, wartości to numery punktów znajdujące się w danym oczku. Funkcja length() zwraca długość każdego elementu listy.\n\npzn_grid_sf$dane = lengths(st_intersects(pzn_grid, p2019))\n\n\nhead(pzn_grid_sf$dane, 16)\n\n [1]   0   0  15   2  11 230 408  48   8 200 636  26   2  14  61   0\n\n\n\nlibrary(tmap)\nlibrary(viridis)\ntm_shape(pzn_grid_sf) + \n    tm_polygons(col = \"dane\",  palette = \"YlGn\", n = 4, title = \"Liczba punktów\") + \n    tm_shape(p2019) + \n    tm_dots() + \n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n7.2.3 Test zliczania w kwadratach\nTest zliczania w kwadratach służy do testowania hipotezy zerowej zakładającej, że dane są rozmieszczone losowo. Test ten dzieli obszar na równe kwadraty, a następnie zlicza liczbę punktów w każdym kwadracie oraz porównuje z wartościami oczekiwanymi (ta sama liczba punktów w każdym kwadracie). Test zliczenia w kwadratach można wykonać wykorzystując funkcję quadrat.test() z pakietu spatstat. Funkcja wymaga podania nazwy obiektu klasy ppp oraz liczby oczek siatki w osi x oraz y (nx, ny).\n\nquadrat.test(p2019_ppp, nx = 4, ny = 4)\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  p2019_ppp\nX2 = 1538, df = 13, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\nQuadrats: 14 tiles (irregular windows)\n\n\n\nJak zinterpretujemy wynik testu?\n\n\nqtest = quadrat.test(p2019_ppp, nx = 4, ny = 4)\n\nWarning: Some expected counts are small; chi^2 approximation may be inaccurate\n\nplot(qtest, nx = 4, ny = 4)\n\n\n\n\n\n\n\n\n\n\n7.2.4 Analiza gęstości jądra\nTest zliczenia w kwadratach oblicza lokalną intensywność poprzez podzielenie obszaru na równej wielkości kwadraty. Analiza gęstości jądra (kernel density estimation) wykorzystuje ruchome okno do obliczenia lokalnej intensywności dla podzbiorów badanego obszaru. Ruchome okno definiuje się poprzez ustawienie parametrów jądra (kernel) - kształtu oraz rozmiaru (bandwith). W wyniku otrzymujemy siatkę rastrową, gdzie każde oczko siatki ma przypisaną wartość intensywności obliczoną dla obszaru o wielkości ruchomego okna wyśrodkowanego dla komórki do której przypisana jest wartość. Rozdzielczość wynikowej komórki będzie zawsze mniejsza niż rozmiar ruchomego okna. Jako jądro (kernel) często wykorzystuje się funkcję gaussowską.\nW R analizę gęstości jądra można wykonać używając funkcji density(). W poniższym przykładzie parametr sigma oznacza rozmiar (bandwith). Sigma = 500 oznacza ruchome okno o wielkości 500m.\n\nlibrary(terra)\np2019_density1 = rast(density(p2019_ppp, sigma = 500))\ncrs(p2019_density1) = crs(pzn)\n\ntm_shape(p2019_density1) + \n    tm_raster(palette = \"-viridis\", style = \"cont\") +\n    tm_shape(p2019) +\n    tm_dots() + \n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\np2019_density2 = rast(density(p2019_ppp, sigma = 1500))\ncrs(p2019_density2) = crs(pzn)\n\ntm_shape(p2019_density2) + \n    tm_raster(palette = \"-viridis\", style = \"cont\") +\n    tm_shape(p2019) +\n    tm_dots() + \n  tm_layout(legend.outside = TRUE)",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metody oparte na analizie intensywności</span>"
    ]
  },
  {
    "objectID": "09_punkty_intensywnosc.html#przykład-2-rozkład-przestrzenny-szkół-w-poznaniu",
    "href": "09_punkty_intensywnosc.html#przykład-2-rozkład-przestrzenny-szkół-w-poznaniu",
    "title": "7  Metody oparte na analizie intensywności",
    "section": "7.3 Przykład 2: Rozkład przestrzenny szkół w Poznaniu",
    "text": "7.3 Przykład 2: Rozkład przestrzenny szkół w Poznaniu\nW poniższym przykładzie przeanalizowano przestrzenny rozkład szkół podstawowych w Poznaniu wykorzystując metody oparte na intensywnści:\n\nAnalizę średniej intensywności\nTest zliczania w kwadratach\nAnalizę gęstości jąda\n\n\nlibrary(sf)\nlibrary(spatstat)\n#wczytanie danych przestrzennych. Granica jest wykorzystywana wyłącznie do wizualizacji danych. \nszkoly = read_sf(\"data/out_poznan_szkoly.gpkg\", layer = \"szkoly\")\npzn = read_sf(\"data/out_poznan_szkoly.gpkg\", layer = \"granica\")\n\n#konwersja do obiektu klasy ppp \npzn_owin = as.owin(pzn)\nszkoly_ppp = as.ppp(szkoly, w = pzn_owin)\n\n\n7.3.1 Analiza intensywności\n\nsummary(szkoly_ppp)\n\nPlanar point pattern:  84 points\nAverage intensity 5.274679e-07 points per square unit\n\nCoordinates are given to 10 decimal places\n\nWindow: rectangle = [351938.9, 365782.3] x [500644.3, 512148] units\n                    (13840 x 11500 units)\nWindow area = 159251000 square units\n\n\n\n\n7.3.2 Zliczenie punktów w siatce\n\nqc = quadratcount(szkoly_ppp, nx = 4, ny = 4)\nplot(qc)\n\n\n\n\n\n\n\n\n\n\n7.3.3 Test zliczania w kwadratach\n\nquadrat.test(szkoly_ppp, nx = 4, ny = 4)\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  szkoly_ppp\nX2 = 64.571, df = 15, p-value = 8.123e-08\nalternative hypothesis: two.sided\n\nQuadrats: 4 by 4 grid of tiles\n\n\n\nJak zinterpetujesz wynik testu?\n\n\nplot(quadrat.test(szkoly_ppp, nx = 4, ny = 4))\n\n\n\n\n\n\n\n\n\nquadrat.test(szkoly_ppp, nx = 2, ny = 2)\n\n\n    Chi-squared test of CSR using quadrat counts\n\ndata:  szkoly_ppp\nX2 = 6.381, df = 3, p-value = 0.189\nalternative hypothesis: two.sided\n\nQuadrats: 2 by 2 grid of tiles\n\n\n\nJak zinterpetujesz wynik testu dla obszaru podzielonego na 2 kwadraty?\n\n\nplot(quadrat.test(szkoly_ppp, nx = 2, ny = 2))\n\n\n\n\n\n\n\n\n\n\n7.3.4 Analiza gęstości jądra\n\nkde12&lt;- density(szkoly_ppp, sigma = 1200)\nkde12 &lt;- rast(kde12)\ntm_kde12 = tm_shape(kde12) + \n  tm_raster(palette = \"-viridis\", style = \"cont\", title = \"Bandwidth = 1.2km\")\n\nkde05 &lt;- density(szkoly_ppp, sigma = 500)\nkde05 &lt;- rast(kde05)\ntm_kde05 = tm_shape(kde05) + \n  tm_raster(palette = \"-viridis\", style = \"cont\", title = \"Bandwidth = 0.5km\")\n\ntmap_arrange(tm_kde12, tm_kde05)\n\n\n\n\n\n\n\n\n\nZinterpretuj wyniki analizy z wykorzystaniem metod opartych o intensywności. Jaki jest rozkład szkół podstawowych w Poznaniu?",
    "crumbs": [
      "DANE PUNKTOWE",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Metody oparte na analizie intensywności</span>"
    ]
  },
  {
    "objectID": "10_poligony.html",
    "href": "10_poligony.html",
    "title": "DANE OBSZAROWE",
    "section": "",
    "text": "Dane obszarowe (ang. areal data) powstają poprzez podzielenie obszaru badań na jednostki przestrzenne, w których agregowane są wyniki, np. podział na jednostki administracyjne (gminy, powiaty). Dane obszarowe charakteryzują się skokową zmiennością wartości. Przykładami danych obszarowych są np. dane społeczno-ekonomiczne udostępniane w ramach spisów ludności (np. poziom bezrobocia wg powiatów).\nEksploracja danych obszarowych ma na celu ocenę autokorelacji przestrzennej i sprawdzenia, czy obszary bliskie mają podobne, czy różne wartości. W tym celu stosuje się rożne miary autokorelacji przestrzennej.",
    "crumbs": [
      "DANE OBSZAROWE"
    ]
  },
  {
    "objectID": "11_poligony_sasiedztwo.html",
    "href": "11_poligony_sasiedztwo.html",
    "title": "8  Określanie sąsiedztwa danych obszarowych",
    "section": "",
    "text": "8.1 Kryteria wyznaczania sąsiedztwa\nDane obszarowe (ang. areal data) powstają poprzez podzielenie obszaru badań na jednostki przestrzenne, w których agregowane są wyniki, np. podział na jednostki administracyjne (gminy, powiaty). Dane obszarowe charakteryzują się skokową zmiennością wartości. Przykładami danych obszarowych są np. dane społeczno-ekonomiczne udostępniane w ramach spisów ludności (np. poziom bezrobocia wg powiatów).\nKoncepcja sąsiedztwa przestrzennego jest przydatna do eksploracji danych obszarowych w celu oceny autokorelacji przestrzennej i sprawdzenia, czy obszary bliskie mają podobne, czy różne wartości.\nSąsiadów przestrzennych można definiować na kilka sposobów.",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Określanie sąsiedztwa danych obszarowych</span>"
    ]
  },
  {
    "objectID": "11_poligony_sasiedztwo.html#kryteria-wyznaczania-sąsiedztwa",
    "href": "11_poligony_sasiedztwo.html#kryteria-wyznaczania-sąsiedztwa",
    "title": "8  Określanie sąsiedztwa danych obszarowych",
    "section": "",
    "text": "8.1.1 Kryterium wspólnej granicy obszarów (adjacency)\nKryterium wspólnej granicy obszarów (adjacency) zakłada, że sąsiedzi to obszary, które mają wspólną granincę. Wspólną granicę obszarów można wyznaczyć posługując się regułą Rook lub Queen\n\n\n\n\n\n\n\n\n\n\n\n8.1.2 Kryterium odległości\nKryterium odległości (odległość mierzona między centroidami obszarów) zakłada, że sąsiedzi to obszary znajdujące się w pewnej odległości od siebie, ale niekoniecznie sąsiadujące. Wyróżnia się:\n\nmacierz sąsiadów w promieniu d* km* - sąsiadem będzie obiekt, którego środek jest oddalony w linii prostej o nie więcej niż d km\nk najbliższych sąsiadów (knn) - najbliższy sąsiad to obszar którego środek leży najbliżej środka danego obszaru stosując odległość euklidesową.\n\ntworzona dla danych punktowych\nużywając danych obszarowych należy najpierw wyznaczyć ich centroidy\nna liczbę najbliższych sąsiadów ma zatem wpływ wielkość oraz kształt regionu i regionów sąsiednich\n\nodległość odwrotna (\\(\\frac{1}{d}\\))",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Określanie sąsiedztwa danych obszarowych</span>"
    ]
  },
  {
    "objectID": "11_poligony_sasiedztwo.html#macierz-wag-przestrzennych",
    "href": "11_poligony_sasiedztwo.html#macierz-wag-przestrzennych",
    "title": "8  Określanie sąsiedztwa danych obszarowych",
    "section": "8.2 Macierz wag przestrzennych",
    "text": "8.2 Macierz wag przestrzennych\nMacierz wag przestrzennych przedstawiana jest w formie tabeli, w której przechowywane są informacje o występowaniu oraz sile zależności przestrzennych między obiektami (np. jednostkami administracyjnymi). Każdy element macierzy reprezentuje powiązanie między parą obszarów. Macierz wag przestrzennych stanowi podstawę wielu technik analizy przestrzennej, w tym autokorelacji przestrzennej, grupowania przestrzennego i regresji przestrzennej.\nMacierz wag przestrzennych tworzona jest w dwóch krokach:\n\nutworzenie macierzy sąsiedztwa\nstandaryzacja macierzy sąsiedztwa w celu otrzymania wag.\n\n\n\n8.2.1 Macierz sąsiedztwa\nMając definicję sąsiedztwa przestrzennego, możemy skonstruować macierz sąsiedztwa przestrzennego - tabelę, w której przechowywane są informacje o występowaniu (lub nie) relacji między obszarami (czy obszary ze sobą sąsiadują?). Najprostszą formą macierzy sąsiedztwa jest macierz binarna.\n\\[\n\\begin{cases}\n1 & \\text{obiekt i jest sąsiadem obiektu j (mają wspólną granicę)} \\\\\n0 & \\text{obiekt i nie jest sąsiadem obiektu j (nie mają wspólnej granicy)} \\\\\n0 & \\text{elementy diagonalne macierzy} \\\\\n\\end{cases}\n\\]\n\n\n8.2.2 Standaryzacja macierzy sąsiedztwa\nW kolejnym kroku standaryzuje się macierz sąsiedztwa w celu otrzymania wag przestrzennych. Najczęściej stosowaną standaryzacją jest standaryzacja wierszami do jedynki tak aby suma wag w każdym wierszu była równa 1:\n\\[\nw_{ij} = { m_{ij} \\over w_i}\n\\]\n\\(m_{ij}\\) - wartość macierzy sąsiedztwa dla obiektu i oraz j, \\(w_i\\) jest sumą wiersza.",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Określanie sąsiedztwa danych obszarowych</span>"
    ]
  },
  {
    "objectID": "11_poligony_sasiedztwo.html#macierze-wag-przestrzennych-w-r",
    "href": "11_poligony_sasiedztwo.html#macierze-wag-przestrzennych-w-r",
    "title": "8  Określanie sąsiedztwa danych obszarowych",
    "section": "8.3 Macierze wag przestrzennych w R",
    "text": "8.3 Macierze wag przestrzennych w R\nDo utworzenia macierzy wag przestrzennych wykorzystuje się w R funkcje z pakietu spdep.\nDwie główne klasy z pakietu spdep:\n\nklasa \"nb\" - lista sąsiadów\nklasa \"listw\" - lista sąsiadów z wagami przestrzennymi dla wybranego schematu.\n\nMożliwe jest przekształcenie tych klas w inne klasy i z powrotem, np:\n\nnb2mat() - funkcja generuje macierz wag przestrzennych dla listy sąsiadów dla dowolnego schematu kodowania wag.\nnb2listw() - funkcja uzupełnia listę sąsiadów o wartości wag wygenerowane dla dowolnego schematu kodowania.\nlistw2mat() - funkcja przekształca listę sąsiadów z przypisanymi wagami w macierz.\n\nMacierz sąsiedztwa, w zależności od wybranego kryterium tworzona jest z wykorzystaniem następujących funkcji:\n\nNa podstawie sąsiedztwa wyznaczanego przez wspólną granicę (funkcja poly2nb(), nb2listw())\nNa podstawie odległości (funkcja dnearneigh())\nMetoda k-najbliższych sąsiadów (funkcja knearneigh())\nMacierze sąsiedztwa dostarczone przez użytkownika (np. przy użyciu funkcji mat2listw())\nOdwrotność odległości",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Określanie sąsiedztwa danych obszarowych</span>"
    ]
  },
  {
    "objectID": "11_poligony_sasiedztwo.html#przykład-1-kryterium-sąsiedztwa---wspólna-granica",
    "href": "11_poligony_sasiedztwo.html#przykład-1-kryterium-sąsiedztwa---wspólna-granica",
    "title": "8  Określanie sąsiedztwa danych obszarowych",
    "section": "8.4 Przykład 1: Kryterium sąsiedztwa - wspólna granica",
    "text": "8.4 Przykład 1: Kryterium sąsiedztwa - wspólna granica\nTworzenie macierzy wag przestrzennych na podstawie kryterium wspólnej granicy wymaga wykonania w R 3 kroków:\n\nutworzenia listy sąsiadów (poly2nb())\nutworzenia listy zawierającej wagi (nb2listw)\nprzekształcenia listy w macierz wag przestrzennych (listw2mat())\n\n\n8.4.1 Dane\n\n#wczytanie danych\nlibrary(sf)\nex = read_sf(\"data/area_ex.gpkg\")\nrownames(ex) &lt;- ex$id\nex = ex[order(rownames(ex)),]\nst_crs(ex) &lt;- 2180\n\n\n#wyswietlenie danych\ntm_shape(ex) +\n  tm_polygons(col = \"grey\", legend.show = FALSE) + \n  tm_text(\"id\", size = 2)\n\n\n\n\n\n\n\n\n\n\n8.4.2 Utworzenie macierzy wag przestrzennych\n\nTworzenie listy sąsiadów\n\nLista sąsiadów tworzona jest z wykorzystaniem funkcji poly2nb(). Funkcja ta tworzy listę sąsiadów na podstawie obiektu poligonowego wczytanego lub utworzonego za pomocą biblioteki sf. Pozwala ona na zdefiniowanie czy lista sąsiadów ma być tworzona z wykorzystaniem reguły Rook (opcja queen = FALSE) czy reguły Queen (queen = TRUE)\n\nex_nb = poly2nb(ex, queen = FALSE)\nex_nb\n\nNeighbour list object:\nNumber of regions: 6 \nNumber of nonzero links: 12 \nPercentage nonzero weights: 33.33333 \nAverage number of links: 2 \n\n\nW wyniku otrzymamy listę sąsiadów dla każdego z obszarów. Poniższa lista składa się z 6 elementów (mamy 6 obszarów w zbiorze danych). Każdy element oznacza sąsiadów dla danego obszaru (np. obszar 1 wg reguły Rook ma dwóch sąsiadów: obszar nr 2 oraz obszar nr 4).\n\nstr(ex_nb)\n\nList of 6\n $ : int [1:2] 2 4\n $ : int [1:3] 1 3 5\n $ : int [1:2] 2 6\n $ : int [1:2] 1 5\n $ : int [1:2] 2 4\n $ : int 3\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:6] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = ex, queen = FALSE)\n - attr(*, \"type\")= chr \"rook\"\n - attr(*, \"snap\")= num 0.01\n - attr(*, \"sym\")= logi TRUE\n - attr(*, \"ncomp\")=List of 2\n  ..$ nc     : int 1\n  ..$ comp.id: int [1:6] 1 1 1 1 1 1\n\n\n\nTworzenie macierzy sąsiedztwa\n\nFunkcja nb2listw() uzupełnia listę sąsiadów o wagi przestrzenne dla wybranego schematu kodowania (patrz opcje dla argumentu style) style = \"B\" oznacza, że chcemy utworzyć macierz binarną.\n\nex_lw = nb2listw(ex_nb, style = \"B\")\nex_lw\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 6 \nNumber of nonzero links: 12 \nPercentage nonzero weights: 33.33333 \nAverage number of links: 2 \n\nWeights style: B \nWeights constants summary:\n  n nn S0 S1  S2\nB 6 36 12 24 104\n\n\nFunkcja listsw2mat() przekształca listę na macierz. W wyniku otrzymamy macierz binarną, gdzie 1 oznacza, że obiekty ze sobą sąsiadują, a 0 oznacza brak sąsiedztwa między obszarami\n\nex_mat = listw2mat(ex_lw)\nex_mat\n\n  1 2 3 4 5 6\n1 0 1 0 1 0 0\n2 1 0 1 0 1 0\n3 0 1 0 0 0 1\n4 1 0 0 0 1 0\n5 0 1 0 1 0 0\n6 0 0 1 0 0 0\n\n\n\nTworzenie macierzy wag przestrzennych\n\nArgument style = \"W\" tworzy macierz wag przestrzennych, gdzie wagi są standaryzowane wierszami (wagi w każdym wierszu sumują się do 1).\n\nex_lwp = nb2listw(ex_nb, style = \"W\")\nex_mat_wp = listw2mat(ex_lwp)\nex_mat_wp\n\n          1   2         3   4         5   6\n1 0.0000000 0.5 0.0000000 0.5 0.0000000 0.0\n2 0.3333333 0.0 0.3333333 0.0 0.3333333 0.0\n3 0.0000000 0.5 0.0000000 0.0 0.0000000 0.5\n4 0.5000000 0.0 0.0000000 0.0 0.5000000 0.0\n5 0.0000000 0.5 0.0000000 0.5 0.0000000 0.0\n6 0.0000000 0.0 1.0000000 0.0 0.0000000 0.0\n\n\n\nUtwórz macierz sąsiedztwa oraz macierz wag przestrzennych dla sąsiedztwa poligonów wyznaczonego regułą Queen.",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Określanie sąsiedztwa danych obszarowych</span>"
    ]
  },
  {
    "objectID": "11_poligony_sasiedztwo.html#przykład-2-sąsiedztwo-obszarów---powiaty-województwa-wielkopolskiego",
    "href": "11_poligony_sasiedztwo.html#przykład-2-sąsiedztwo-obszarów---powiaty-województwa-wielkopolskiego",
    "title": "8  Określanie sąsiedztwa danych obszarowych",
    "section": "8.5 Przykład 2: Sąsiedztwo obszarów - powiaty województwa wielkopolskiego",
    "text": "8.5 Przykład 2: Sąsiedztwo obszarów - powiaty województwa wielkopolskiego\n\n8.5.1 Dane\n\npowiaty = read_sf(\"data/wlkp_powiaty.gpkg\")\ntm_shape(powiaty) + tm_polygons(fill = \"grey\", col = \"black\")\n\n\n\n\n\n\n\n\n\n\n8.5.2 Sąsiedztwo: krytetium wspólnej granicy\nW poniższym przykładzie dla każdego z powiatów w województwie wielkopolskim wyznaczono sąsiadów posługując się kryterium wspólnej granicy (określonej poprzez regułę Queen)\n\n#lista sąsiadów\npowiaty_nb_q = poly2nb(powiaty, queen = TRUE, row.names = as.character(powiaty$Nazwa))\n#macierz binarna, aby otrzymać macierz standaryzowaną wierszami należy podac style = \"W\"\npowiaty_lw = nb2listw(powiaty_nb_q, style = \"B\")\npowiaty_mat = listw2mat(powiaty_lw)\ncolnames(powiaty_mat) &lt;- as.character(powiaty$Nazwa)\n\nMacierz sąsiedztwa powiatów w województwie wielkopolskim\n\npowiaty_mat[1:5, 1:5]\n\n                        chodzieski czarnkowsko-trzcianecki gnieźnieński\nchodzieski                       0                       1            0\nczarnkowsko-trzcianecki          1                       0            0\ngnieźnieński                     0                       0            0\ngostyński                        0                       0            0\ngrodziski                        0                       0            0\n                        gostyński grodziski\nchodzieski                      0         0\nczarnkowsko-trzcianecki         0         0\ngnieźnieński                    0         0\ngostyński                       0         0\ngrodziski                       0         0\n\n\nPoniższy kod pozwala na wypisanie sąsiadów dla danego powiatu\n\npowiat_nazwy = as.character(powiaty$Nazwa)\npowiat_nazwy[powiaty_nb_q[[which(powiat_nazwy==\"poznański\")]]]\n\n [1] \"gnieźnieński\" \"grodziski\"    \"kościański\"   \"nowotomyski\"  \"obornicki\"   \n [6] \"Poznań\"       \"szamotulski\"  \"średzki\"      \"śremski\"      \"wągrowiecki\" \n[11] \"wrzesiński\"  \n\n\n\nZ jakimi powiatami graniczy powiat koniński?\n\n\nZ jakimi powiatami graniczy powiat pilski?\n\n\nWizualizacja macierzy sąsiedztwa\n\n#wyznaczenie centroidów dla powiatów \npowiaty_cen = st_centroid(st_geometry(powiaty), of_largest_polygon = FALSE)\n#konwersja listy sąsiadów na obiekt liniowy (linie łączące sąsiadujące powiaty)\npowiaty_nb_net = st_as_sf(nb2lines(powiaty_nb_q, coords = st_coordinates(powiaty_cen)))\n\nWarning in CRS(proj4string): CRS: projargs should not be NULL; set to NA\n\nst_crs(powiaty_nb_net)&lt;- 2180\n#wizualizacja\ntm_shape(powiaty) + tm_polygons() + \n  tm_shape(powiaty_nb_net) + tm_lines() + tm_shape(powiaty_cen) + tm_dots(size =0.6 , col = \"black\")\n\n\n\n\n\n\n\n\n\n\n\n8.5.3 Sąsiedztwo: krytetium odległości\n\nodległość między obszarami jest mierzona między centroidami obszarów.\nsąsiadem będzie obiekt, którego środek jest oddalony w linii prostej o nie więcej niż d km.\n\n\nAnaliza odległości centroidami sąsiadujących powiatów\n\n#centroidy powiatów\npowiaty_cen = st_centroid(st_geometry(powiaty), of_largest_polygon = FALSE)\n#lista sąsiadów wg kryterium sąsiedztwa \npowiaty_nb_q = poly2nb(powiaty, queen = TRUE, row.names = as.character(powiaty$Nazwa))\n#obliczenie odległosci między sąsiadami, obiekt powiaty_dists zwraca odległosc euklidesową między centroidami sąsiadujących powiatów\npowiaty_dists = nbdists(powiaty_nb_q, powiaty_cen)\nhead(powiaty_dists, 2)\n\n[[1]]\n[1] 42356.78 30099.31 22127.30 25311.28\n\n[[2]]\n[1] 42356.78 41310.84 39130.37 48353.71 34813.16\n\n\n\npowiaty_dists_vec = unlist(powiaty_dists)\n\nPoniższy histogram pokazuje rozkład odległości między centroidami sąsiadujących powiatów.\n\nhist(powiaty_dists_vec)\n\n\n\n\n\n\n\n\nPoniżej obliczone zostały statystyki opisowe dla odległości euklidesowej między centroidami sąsiadujących powiatów.\n\nsummary(powiaty_dists_vec)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2063   24892   31809   31915   37644   54911 \n\n\n\n\nMacierz sąsiedztwa na podstawie kryterium odległości\nFunkcja dnearneigh() identyfikuje sąsiadów na podstawie zadanej odległości. W poniższym przykładzie za sąsiadów zostaną uznane powiaty, których centroidy w linii prostej (odległość euklidesowa) są od siebie oddalone o 0 do 30000 m.\n\npowiaty_nb_d = dnearneigh(powiaty_cen, d1 = 0, d2 = 30000, longlat = FALSE, row.names = as.character(powiaty$Nazwa))\n\nWarning in dnearneigh(powiaty_cen, d1 = 0, d2 = 30000, longlat = FALSE, :\ndnearneigh: longlat argument overrides object\n\n\nWarning in dnearneigh(powiaty_cen, d1 = 0, d2 = 30000, longlat = FALSE, :\nneighbour object has 8 sub-graphs\n\npowiaty_nb_d\n\nNeighbour list object:\nNumber of regions: 35 \nNumber of nonzero links: 64 \nPercentage nonzero weights: 5.22449 \nAverage number of links: 1.828571 \n5 regions with no links:\nczarnkowsko-trzcianecki, gnieźnieński, kolski, obornicki, turecki\n8 disjoint connected subgraphs\n\n\n\npowiaty_mat2 = nb2mat(powiaty_nb_d, zero.policy = TRUE)\ncolnames(powiaty_mat2) &lt;- as.character(powiaty$Nazwa)\npowiaty_mat2[1:5, 1:5]\n\n                        chodzieski czarnkowsko-trzcianecki gnieźnieński\nchodzieski                       0                       0            0\nczarnkowsko-trzcianecki          0                       0            0\ngnieźnieński                     0                       0            0\ngostyński                        0                       0            0\ngrodziski                        0                       0            0\n                        gostyński grodziski\nchodzieski                      0         0\nczarnkowsko-trzcianecki         0         0\ngnieźnieński                    0         0\ngostyński                       0         0\ngrodziski                       0         0\n\n\n\n\nWizualizacja sąsiedztwa\n\nWizualizacja sąsiedztwa używając funkcji plot()\n\n\nplot(st_geometry(powiaty), border = \"grey\", lwd = 1.5, cex = 3)\nplot(powiaty_nb_d, coords = st_coordinates(powiaty_cen), add = TRUE, lwd = 1.5)\n\n\n\n\n\n\n\n\n\nWizualizacja sąsiedztwa używając pakietu tmap()\n\n\n#przekształcenie obiektu klasy nb na obiekt klasy sf\npowiaty_nb_d_net = st_as_sf(nb2lines(powiaty_nb_d, coords = st_coordinates(powiaty_cen)))\nst_crs(powiaty_nb_d_net)&lt;- 2180\n#wizualizacja\ntm_shape(powiaty) + tm_polygons() + \n  tm_shape(powiaty_nb_d_net) + tm_lines() + tm_shape(powiaty_cen) + tm_dots(size =0.6 , col = \"black\")\n\n\n\n\n\n\n\n\nPoniższy kod pozwala na wypisanie sąsiadów dla danego powiatu.\n\nJakie powiaty sąsiadują z powiatem Poznań?\n\n\npowiat_nazwy = as.character(powiaty$Nazwa)\npowiat_nazwy[powiaty_nb_d[[which(powiat_nazwy==\"Poznań\")]]]\n\n[1] \"poznański\"\n\n\n\nJakie powiaty sąsiadują z powiatem Kalisz?\n\n\npowiat_nazwy[powiaty_nb_d[[which(powiat_nazwy==\"Kalisz\")]]]\n\n[1] \"kaliski\"    \"ostrowski\"  \"pleszewski\"\n\n\n\nJakie powiaty sąsiadują z powiatem konińskim biorąc pod uwagę kryterium odległości (30km)?\n\n\n\n\n8.5.4 Sąsiedztwo: k-najbliższych sąsiadów\n\ntworzona dla danych punktowych\nużywając danych obszarowych należy najpierw wyznaczyć ich centroidy\nnajbliższy sąsiad to obszar którego środek leży najbliżej środka danego obszaru stosując odległość euklidesową.\n\nna liczbę najbliższych sąsiadów ma zatem wpływ wielkość oraz kształt regionu i regionów sąsiednich\n\n\nFunkcja knearneigh() wyznacza k najbliższych sąsiadów na podstawie odległości między centroidami powiatów.\n\npowiaty_cen = st_centroid(st_geometry(powiaty), of_largest_polygon = FALSE)\npowiaty_knn = knearneigh(powiaty_cen, k = 5)\npowiaty_nb_knn = knn2nb(powiaty_knn)\npowiaty_nb_knn[1:3]\n\n[[1]]\n[1]  2 19 22 32 35\n\n[[2]]\n[1]  1 17 19 22 28\n\n[[3]]\n[1] 25 27 29 32 34\n\n\nKażdy element listy odnosi się do powiatu, np. [[1]] oznacza powiat chodzieski, a poszczególne elementy wskazują id powiatów, z które stanowią 5 najbliższych sąsiadów. Dla powiatu chodzieskiego będą to powiaty:\n\npowiaty$Nazwa[rownames(powiaty)%in%c(2, 19 ,22, 32, 35)]\n\n[1] \"czarnkowsko-trzcianecki\" \"obornicki\"              \n[3] \"pilski\"                  \"wągrowiecki\"            \n[5] \"złotowski\"              \n\n\n\nplot(st_geometry(powiaty), border = \"darkgrey\", lwd = 2.5, cex = 3)\nplot(powiaty_nb_knn, coords = st_coordinates(powiaty_cen), add = TRUE, lwd = 2.5)",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Określanie sąsiedztwa danych obszarowych</span>"
    ]
  },
  {
    "objectID": "12_poligony_autokorelacja.html",
    "href": "12_poligony_autokorelacja.html",
    "title": "9  Miary autokorelacji danych obszarowych",
    "section": "",
    "text": "9.1 Autokorelacja przestrzenna danych obszarowych\nAutokorelacja przestrzenna (spatial autocorrelation) określa pewną zależność, która może być obserowana jako zgrupowania (klastry) podobnych wartości lub systematyczny wzorzec przestrzenny (spatial pattern).",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miary autokorelacji danych obszarowych</span>"
    ]
  },
  {
    "objectID": "12_poligony_autokorelacja.html#autokorelacja-przestrzenna-danych-obszarowych",
    "href": "12_poligony_autokorelacja.html#autokorelacja-przestrzenna-danych-obszarowych",
    "title": "9  Miary autokorelacji danych obszarowych",
    "section": "",
    "text": "Losowość przestrzenna :=================================================================================================: wartości obserwowane w danym obszarze nie zależą od wartości obserwowanych w sąsiednich obszarach\nDodatnia autokorelacja przestrzenna data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAASCAYAAABWzo5XAAAAbElEQVR4Xs2RQQrAMAgEfZgf7W9LAguybljJpR3wEse5JOL3ZObDb4x1loDhHbBOFU6i2Ddnw2KNiXcdAXygJlwE8OFVBHDgKrLgSInN4WMe9iXiqIVsTMjH7z/GhNTEibOxQswcYIWYOR/zAjBJfiXh3jZ6AAAAAElFTkSuQmCC | Ujemna autokorelacja przestrzenna :=================================================================================================================:+:===============================================================================:+ obserwuje się podobne wartości w sąsiadujących obszarach (wysokie wartości otoczone wysokimi, lub niskie niskimi) | wartości sąsiednie różnią się od siebie (np. niskie wartości otoczone wysokimi) |",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miary autokorelacji danych obszarowych</span>"
    ]
  },
  {
    "objectID": "12_poligony_autokorelacja.html#miary-autokorelacji-przestrzennej",
    "href": "12_poligony_autokorelacja.html#miary-autokorelacji-przestrzennej",
    "title": "9  Miary autokorelacji danych obszarowych",
    "section": "9.2 Miary autokorelacji przestrzennej",
    "text": "9.2 Miary autokorelacji przestrzennej\nCelem analiz jest wyszukanie obszarów podobnych do siebie lub obszarów różniących się od sąsiednich. Losowość przestrzenna oznacza brak istnienia wzorca w przestrzeni. Przy testowaniu istnienia autokorelacji przestrzennej losowość przestrzenna jest przyjmowana jako hipoteza zerowa. W ramach analiz chcemy odrzucić hipotezę o występowaniu losowości przestrzennej (Odrzucenie hipotezy zerowej oznacza, że w danych istnieje jakiś wzorzec). Do testowania istnienia autokorelacji przestrzennej stosuje się różne miary autokorelacji przestrzennej.\n\n9.2.1 Miary globalne\n\njednoliczbowy wskaźnik autokorelacji przestrzennej lub ogólnego podobieństwa regionów\ninformuje o tym, że wartości nie są w przestrzeni rozmieszczone losowo -&gt; tj. istnieją przesłanki, że podobne wartości grupują się w przestrzeni\nmiary globalne nie informują nas gdzie zlokalizowane są zgrupowania podobnych wartości\nstatystyki globalne: I Morana, C Geary’ego, G Getisa & Orda\n\n\n\n9.2.2 Miary lokalne\n\nobliczane dla każdego regionu;\npozwalaja odpowiedzieć na pytanie: czy dany region jest otoczony regionami o wysokich lub niskich wartościach? czy dany region jest podobny/różny wzgledem regionow sąsiednich?\nstatystyki lokalne: Lokalne I Morana, Lokalne C Geary’ego, Lokalne G Getisa & Orda\n\n\n\n\n\n\n\n\n\nStatystyka przestrzenna\nInterpretacja\nKomenda w R\n\n\n\n\n\nMIARY GLOBALNE\n\n\n\nMoran \\(I\\)\n\\(I &gt; 0\\) - dodatnia autokorelacja przestrzenna\n\\(I &lt; 0\\) - ujemna autokorelacja przestrzenna\n\\(I = 0\\) - brak autokorelacji\nmoran()\nmoran.test()\nmoran.mc()\nmoran.plot()\n\n\nGeary \\(C\\)\n\\(0-1\\) - dodatnia autokorelacja przestrzenna\n\\(1-2\\) - ujemna autokorelacja przestrzenna\n\\(1\\) - brak autokorelacji\ngeary()\ngeary.test()\ngeary.mc()\n\n\nGetis & Ord \\(G\\)\ntestuje \\(H_0\\) o braku autokorelacji przestrzennej przeciwko \\(H_A\\) o dodatniej autokorelacji przestrzennej. Nie mierzy ujemnej autokorelacji przestrzennej.\nglobalG.test()\n\n\n\nMIARY LOKALNE\n\n\n\nLocal Moran \\(I_{i}\\)\n\\(I_{i} &gt; 0\\) - grupowanie podobnych wartości (wyższych lub niższych od średniej)\n\\(I_{i} &lt; 0\\) - połączenie różnych wartości (np. wysokie wartości otoczone niskimi wartościami lub niskie wartości otoczone wysokimi wartościami)\nlocal.moran()\n\n\nLocal Geary \\(C_{i}\\)\n\\(C_i &lt; 0\\) - niskie wartości wskazują na dodatnią autokorelację przestrzenną (podobieństwo regionu \\(i\\) z sąsiadami)\n\\(C_i &gt; 0\\) - wysokie wartości wskazują na ujemną autokorelację przestrzenną, tj. region nie jest podobny do sąsiadów\n\\(C_i = 0\\) - brak autokorelacji przestrzennej\nlocalC(), localC_perm()\n\n\nLocal \\(G_{i}\\)\n\\(G_{i} &gt; 0\\) - zgrupowanie regionów o relatywnie wysokich wartościach, klaster wysokich wartości (hot-spots)\n\\(G_{i} &lt; 0\\) - zgrupowanie regionów o relatywnie niskich wartościach, klaster niskich wartości (cold-spots)\nlocalG(), localG_perm()\n\n\n\nŹródło: Kopaczewska K. 2023. Przestrzenne metody ilościowe w R. (rozdział 4, str. 264-265), uzupełnione",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miary autokorelacji danych obszarowych</span>"
    ]
  },
  {
    "objectID": "12_poligony_autokorelacja.html#miary-autokorelacji-przestrzennej-w-r",
    "href": "12_poligony_autokorelacja.html#miary-autokorelacji-przestrzennej-w-r",
    "title": "9  Miary autokorelacji danych obszarowych",
    "section": "9.3 Miary autokorelacji przestrzennej w R",
    "text": "9.3 Miary autokorelacji przestrzennej w R\nObliczenie miar autokorelacji przestrzennej w R wymaga wykonania dwóch kroków:\n\nUtworzenie macierzy wag przestrzennych na podstawie wybranego sąsiedztwa i standaryzacji.\nObliczenie globalnych i lokalnych statystyk autokorelacji przestrzennych.\n\n\n9.3.1 Pakiet spdep\nPakiet spdep dostarcza funkcji pozwalających na:\n\ntworzenie macierzy przestrzennych (poly2nb, nb2listw)\nobliczanie miar autokorelacji przestrzennych (patrz tabela powyżej)",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miary autokorelacji danych obszarowych</span>"
    ]
  },
  {
    "objectID": "12_poligony_autokorelacja.html#przykład-stopa-bezrobocia-w-polsce-w-2023-roku",
    "href": "12_poligony_autokorelacja.html#przykład-stopa-bezrobocia-w-polsce-w-2023-roku",
    "title": "9  Miary autokorelacji danych obszarowych",
    "section": "9.4 Przykład: Stopa bezrobocia w Polsce w 2023 roku",
    "text": "9.4 Przykład: Stopa bezrobocia w Polsce w 2023 roku\n\nlibrary(sf)\nlibrary(tmap)\nlibrary(spdep)\nlibrary(dplyr)\n\n\n9.4.1 Dane\nW przykładzie zostaną wykorzystane dane dotyczące stopy bezrobocia w powiatach w Polsce w 2023 roku. Dane zostały pobrane z Banku Danych Lokalnych.\n\nPlik bezrobocie_pl.csv zawiera identyfikator powiatu, nazwę powiatu oraz dane dotyczące stopy bezrobocia dla lat 2004, 2010, 2021, 2023 (odpowiednio SB2004, SB2010, SB2021, SB2023).\nPlik powiaty_pl.gpkg zawiera granice powiatów w Polsce.\nPlik wojewodztwa.gpkg zawiera granice województw, jest używany wyłącznie przy wizualizacji wyników.\n\n\nbezrobocie = read.csv(\"data/bezrobocie_pl.csv\", sep = \";\", dec = \".\")\npowiaty_granica = read_sf(\"data/powiaty_pl.gpkg\")\nwoj_granica = read_sf(\"data/wojewodztwa.gpkg\")\n\nAby połączyć dane przestrzenne z informacją o stopie bezrobocia wykorzystane zostaną pola TERYT oraz Kod. Kolumna TERYT w pliku powiaty_pl.gpkg zawiera identyfikator powiatu w postaci 4 cyfr. Do pola TERYT trzeba dodać “000”, aby identyfikator powiatu w obu obiektach był taki sam. Do połączenia danych przestrzennych i nieprzestrzennych zostanie wykorzystana funkcja merge().\n\npowiaty_granica$TERYT = as.integer(paste(powiaty_granica$TERYT, \"000\", sep=\"\"))\npowiaty = merge(powiaty_granica[, \"TERYT\"], bezrobocie, by.x = \"TERYT\", by.y = \"Kod\")\n\nWizualizacja została wykonana z wykorzystaniem pakietu tmap.\n\ntm_shape(powiaty) + \n  tm_polygons(\"SB2023\", palette = \"YlOrBr\") + \n  tm_shape(woj_granica) + \n  tm_borders(col = \"black\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\n9.4.2 Cel analizy\n\nanaliza podobieństwa i różnic między obszarami/regionami\nwyszukanie obszarów podobnych do siebie\nznalezienie obszarów znacząco różnych od sąsiednich\n\n\n\n9.4.3 Określanie sąsiedztwa danych poligonowych\nObliczenie miar autokorelacji przestrzennej wymaga określenia, które powiaty ze sobą sąsiadują oraz zapisania tej informacji w postaci macierzy wag przestrzennych.\n\n9.4.3.1 Utworzenie macierzy wag przestrzennych\nMacierz sąsiedztwa została utworzona z wykorzystaniem kryterium wspólnej granicy stosując regułę Queen.\n\nFunkcja poly2nb() z pakieru spdep tworzy listę sąsiadów na podstawie regionów mających wspólne granice. Funkcja poly2nb() wymaga podania obiektu przestrzennego z granicami poligonów (w przykładzie obiekt powiaty) oraz określenia, czy stosowana będzie reguła queen (queen = TRUE), czy rook do określenia sąsiedztwa.\nFunkcja nb2listw() z pakietu spdep tworzy macierz wag przestrzennych na podstawie macierzy sąsiedztwa. Funkcja nb2listw() wymaga podania obiektu zawierającego listę sąsiadów (obiekt będący wynikiem działania funkcji poly2nb() oraz typu macierzy wag (w przykładzie style = “W” oznacza, że zostanie utworzona macierz standaryzowana wierszami).\n\n\n#macierz sasiedztwa\nnb_q = poly2nb(powiaty, queen = TRUE)\n#macierz wag przestrzennych. \nlw = nb2listw(nb_q, style = \"W\")\nlw\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 380 \nNumber of nonzero links: 2006 \nPercentage nonzero weights: 1.389197 \nAverage number of links: 5.278947 \n\nWeights style: W \nWeights constants summary:\n    n     nn  S0       S1      S2\nW 380 144400 380 169.7589 1632.39\n\n\n\nNa czym polega reguła QUEEN stosowana do określenia sąsiadów?\n\n\n\n\n9.4.4 Globalne miary autokorelacji przestrzennej\n\n9.4.4.1 Globalny test I Morana\nStatystyka globalna I Morana jest najstarszą miarą przestrzenną wprowadzoną przez Morana w 1950 roku. Wykorzystywana jest do testowania globalnej autokorelacji przestrzennej:\n\n\\(H_0\\) : brak autokorelacji przestrzennej\n\\(H_1\\) : autokorelacja przestrzenna\n\nStatystyka I Morana przyjmuje wartości:\n\n\\(I = 0\\) - wartości obserwacji w odległości \\(d\\) są rozłożone losowo\n\\(I &gt; 0\\) - autokorelacja dodatnia (pozytywna); sąsiadujące wartości są podobne; wartości podobne stykają się częściej niż losowo\n\\(I &lt; 0\\) - autokorelacja ujemna (negatywna); sąsiadujące wartości są różne\n\nUwaga!: nawet wysokie wartości statystyki nie świadczą, że jest ona istotna statystycznie, dlatego poza policzeniem wartości statytyki należy wykonać test określający jej istotność statystyczną. Testowania istnienia autokorelacji przestrzennej może być wykonane wykorzystując dwa rodzaje testów:\n\ntradycyjne podejście (moran.test())\nsymulację z wykorzystaniem Monte Carlo (moran.mc())\n\nW tradycyjnym podejściu do testowania należy podać nazwę zmiennej (w przykładzie powiaty$SB2023) oraz macierz wag przestrzennych (argument listw). Do obliczenia statystyki I Morana zaleca się stosowanie macierzy wag przestrzennych standaryzowanej wierszami.\n\nmoran.test(powiaty$SB2023, listw = lw)\n\n\n    Moran I test under randomisation\n\ndata:  powiaty$SB2023  \nweights: lw    \n\nMoran I statistic standard deviate = 12.997, p-value &lt; 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.439401477      -0.002638522       0.001156697 \n\n\nW wyniku otrzymamy wartość statystyki Morana (Moran I statistics) oraz istotność statystyczną wyniku (p-value).\nStosowanie tradycyjnego podeścia do testowania istotności autokorelacji wymaga spełnienia wielu założeń. Dlatego częściej stosuje się podejście oparte o symulacji. W każdej symulacji wartości danych przypisano losowo do poligonów, wyliczana jest wartość statystyki, a następnie sprawdza się, czy obliczona wartość statystyki Morana na podstawie danych jest wśród wartości, których spodziewalibysmy się z losowego rozkładu.\nDo wykonania testu z wykorzystaniem symulacji stosuje się funkcję moran.mc() z pakietu spdep. Funkcja ta wymaga podania nazwy zmiennej (w przykładzie powiaty$SB2023), macierzy wag przestrzennych (argument listw) oraz liczby symulacji (argument nsim).\n\nmoran_test = moran.mc(powiaty$SB2023, listw = lw, nsim = 99)\nmoran_test\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  powiaty$SB2023 \nweights: lw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.4394, observed rank = 100, p-value = 0.01\nalternative hypothesis: greater\n\n\nW wyniku otrzymamy wartość statystyki (statistic) oraz psedo p-value (p-value). Należy pamiętać, że w tym wypadku wartość pseudo p-value jest zależne od liczby symulacji. (gdybyśmy wykonali 999 symulacji, zamiast 99 pseudo p-value wyniesie 0.001). Niska wartość p-value nadal wskazuje na istotność statystyczną wyniku.\nW przypadku stopy bezrobocia w powiatach wartość statystyki globalnej I Morana wyniosła 0,4394 co wskazuje na autokorelacja dodatnia (pozytywna), tj. istnieje przesłanka, że w danych istnieją zgrupowania powiatów o podobnych wartościach stopy bezrobocia.\n\n\n9.4.4.2 Globalny test C Geary’ego\nMiara C Geary’ego jest kolejną miarą globalną wykorzystywaną do testowania globalnej autokorelacji przestrzennej:\n\n\\(H_0\\) : brak autokorelacji przestrzennej\n\\(H_1\\) : autokorelacja przestrzenna\n\nStatystyka C Geary’ego przyjmuje wartości w przedziale od 0 do 2.\n\n0-1: autokorelacja dodatnia (pozytywna); sąsiadujące wartości są podobne\n1: brak autokorelacji przestrzennej; sąsiadujące wartości są rozłożone losowo\n1-2: autokorelacja ujemna (negatywna); sąsiadujące wartości nie są podobne\n\nTak jak w przypadku statystyki I Morana do testowania istotności można zastsować tradycyjne podejście (funkcja geary.test() z pakietu spdep) lub podejście oparte o symulacje (funkcja geary.mc() z pakietu spdep).\n\ngeary.test(powiaty$SB2023, listw = lw)\n\n\n    Geary C test under randomisation\n\ndata:  powiaty$SB2023 \nweights: lw   \n\nGeary C statistic standard deviate = 11.024, p-value &lt; 2.2e-16\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n      0.541515148       1.000000000       0.001729799 \n\n\n\ngeary.mc(powiaty$SB2023, listw = lw, nsim = 99)\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  powiaty$SB2023 \nweights: lw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.54152, observed rank = 1, p-value = 0.01\nalternative hypothesis: greater\n\n\nWartość statystyki (0,54) wskazuje na istnienie autokorelacji dodatniej.\n\n\n9.4.4.3 Globalny test G Getisa & Orda\nTestuje hipotezę zerową o braku autokorelacji przestrzennej przeciwko hipotezie alternatywnej o dodatniej autokorelacji przestrzennej. Miara ta nie mierzy autokorelacji ujemnej.\n\n\\(H_0\\) : brak autokorelacji przestrzennej\n\\(H_1\\) : dodatnia autokorelacja przestrzenna\n\n\n#obliczenie wag binarnych\nlb = nb2listw(nb_q, style = \"B\")\n#testowanie\nglobalG.test(powiaty$SB2023, listw = lb)\n\n\n    Getis-Ord global G statistic\n\ndata:  powiaty$SB2023 \nweights: lb   \n\nstandard deviate = 5.679, p-value = 6.774e-09\nalternative hypothesis: greater\nsample estimates:\nGlobal G statistic        Expectation           Variance \n      1.582796e-02       1.392862e-02       1.118554e-07 \n\n\n\nJak zinterpretujesz wynik?\n\n\n\n\n9.4.5 Lokalne miary autokorelacji przestrzennej\n\n9.4.5.1 Zależność między regionem a sąsiadami\n\n\n\n\n\n\n\n\n\nWartości niskie w regionie \\(i\\) (L)\nWartości wysokie w regionie \\(i\\) (H)\n\n\n\n\nWartości wysokie w regionach sąsiednich (H)\nKwadrat LH (Q2)\nUjemna autokorelacja przestrzenna\nKwadrat HH (Q1)\nDodatnia autokorelacja przestrzenna\n\n\nWartości niskie w regionach sąsiednich (L)\nKwadrat LL (Q3)\nDodatnia autokorelacja przestrzenna\nKwadrat HL (Q4)\nUjemna autokorelacja przetrzenna\n\n\n\n\n\n9.4.5.2 Wykres rozrzutu Morana\nWykres rozrzutu Morana pokazuje zależność między wartościami zmiennej (oś x) oraz wartościami zmiennej późnionej przestrzennie (oś y, spatial lag, tj. średnia wartość wyliczona na podstawie sąsiadów graniczących z regionem). Pionowa przerywana linia wyznacza wartość średnią dla analizowanej zmiennej, pozioma przerywana linia oznacza średnią wartość dla zmiennej opóźnionej przestrzennie. Linie te dzielą wykres na 4 części.\n\nmoran.plot(powiaty$SB2023, listw = lw, labels = powiaty$Nazwa)\ntext(3, 15, \"Q2: LH\", cex = 2, col = \"steelblue1\")\ntext(22, 15, \"Q1: HH\", cex = 2, col = \"red\")\ntext(22, 2, \"Q4: HL\", cex = 2, col = \"lightcoral\")\ntext(3, 2, \"Q3: LL\", cex = 2, col = \"blue\")\n\n\n\n\n\n\n\n\nKwarta 1 (Q1) wskazuje, że wartości relatywnie wysokie (wyższe od średniej) otoczone są przez relatywnie wysokie wartości sąsiadów, co wskazuje na istnienie autokorelacji dodatniej. Kwarta 3 (Q3) pokazuje, że wartości w regionie są niższe od średnich oraz są otoczone relatywnie niższymi wartościami sąsiadów (to też świadczy o autokorelacji dodatniej). Kwarta 2 (Q2) oraz kwarta 4 (Q4) przedstawiają ujemną autokorelacją przestrzenną. W przypadku kwarty 2 (Q2) relatywnie niskie wartości zmiennej (poniżej średniej) otoczone są przez relatywnie wysokie wartości dla sąsiadów. W kwarcie 4 relatywnie wysokie wartości w regionie (powyżej średniej) otoczone są przez relatywnie niskie wartości dla sąsiadów.\nWykres rozrzutu statystyki Morana można także wykonać dla wartości standaryzowanych. Wykres punktowy statystyki Morana wykonany w oparciu o standaryzowane wartości zmiennej oraz zmiennej opóźnionej przestrzennie (spatial lag) stanowi graficzną reprezentację globalnej Statystyki Morana. Nachylenie linii regresji jest tożsame z globalną statystyką Morana. Linie przerywane na wykresie wyznaczają średnią wartość obserwowanej zmiennej oraz średnią wartość zmiennej opóźnionej przestrzennie. Po standaryzacji średnie będą równe 0.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n#standaryzacja warości zmiennej\npowiaty$SB_scaled = as.vector(scale(powiaty$SB2023))\n#obliczenie wartości standaryzowanych dla zmiennej opóźnionej przestrzennie\npowiaty$SB_scaled_lag = lag.listw(lw, powiaty$SB_scaled)\n\n# typy zależnosci \npowiaty = powiaty |&gt;\n  mutate(mp_q = case_when(\n    powiaty$SB_scaled &gt;= 0 & powiaty$SB_scaled_lag &gt;= 0 ~ \"Q1: HH\",\n    powiaty$SB_scaled &lt;= 0 & powiaty$SB_scaled_lag &lt;= 0 ~ \"Q3: LL\",\n    powiaty$SB_scaled &gt;= 0 & powiaty$SB_scaled_lag &lt;= 0  ~ \"Q4: HL\",\n    powiaty$SB_scaled&lt;= 0 & powiaty$SB_scaled_lag &gt;= 0 ~ \"Q2: LH\"\n  )) # classify the cluster types\n\n#wykres rozrzutu\nggplot(powiaty, aes(x=SB_scaled, y=SB_scaled_lag)) + geom_point(aes(col = mp_q), shape=20, show.legend = F, size = 3) + \n    scale_color_manual(values = c(\"Q1: HH\"= \"red\", \"Q2: LH\" = \"steelblue1\", \"Q3: LL\" = \"blue\", \"Q4: HL\" = \"lightcoral\")) + \n    geom_hline(yintercept=mean(powiaty$SB_scaled_lag), lty=2) + \n    geom_vline(xintercept=mean(powiaty$SB_scaled), lty=2) + theme_minimal(base_size = 18) + \n    geom_smooth(formula=y ~ x, method=\"lm\") + \n    annotate(geom=\"text\", x=-1, y=-1.5, label=\"Q3: LL\", color=\"blue\", size = 7) + \n    annotate(geom=\"text\", x=-1, y=2, label=\"Q2: LH\", color=\"steelblue1\", size = 7) + \n    annotate(geom=\"text\", x=3, y=-1.5, label=\"Q4: HL\", color=\"lightcoral\", size = 7) + \n    annotate(geom=\"text\", x=3, y=2, label=\"Q1: HH\", color=\"red\", size = 7)\n\n\n\n\n\n\n\n\n\n\n9.4.5.3 Lokalny test Morana \\(I_{i}\\)\nStatytyka lokalna Morana \\(I\\) obliczana jest dla każdego regionu. Statystyka ta mierzy czy region jest otoczony przez regiony sąsiedzkie o podobnych czy różnych wartościach w stosusnku do losowego rozmieszczenia tych wartości w przestrzeni:\n\n\\(I_{i} &gt; 0\\) - grupowanie podobnych wartości (wyższych lub niższych od średniej)\n\\(I_{i} &lt; 0\\) - połączenie różnych wartości (np. wysokie wartości otoczone niskimi wartościami lub niskie wartości otoczone wysokimi wartościami)\n\nStatytyka lokalna Morana \\(I\\)umożliwia wykrycie znaczących grup identycznych wartości wokół określonej lokalizacji (klastrów).\nW R do obliczenia lokalnej statystyki Morana wykorzystuje się funkcję localmoran() z pakietu spdep. Funkcja ta wymaga zdefiniowania wartości zmiennej oraz macierzy wag przestrzennych (argument listw).\n\nlocm = localmoran(powiaty$SB2023, listw = lw)\nhead(locm)\n\n           Ii          E.Ii      Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.02618885 -3.278138e-03 0.174557285 -0.05483648      0.9562687\n2 -0.14316746 -4.621114e-04 0.028866576 -0.83992908      0.4009482\n3  0.01752931 -6.811164e-05 0.004256387  0.26972955      0.7873683\n4 -0.66667478 -8.537739e-03 0.452226259 -0.97867488      0.3277406\n5  0.19350355 -1.352469e-03 0.072156657  0.72539678      0.4682086\n6  0.11137371 -3.325013e-04 0.024994379  0.70657154      0.4798328\n\n\nW wyniku otrzymamy wartość statystyki Morana dla każdego regionu (Ii) oraz wartość poziomu istotności statystyki wyliczonej dla każdego regionu (kolumna Pr(z != E(Ii))). Wartości poziomu istotności oraz statystyki możemy przypisać do obiektu powiaty co pozwoli nam na zwizualizowanie wyników.\n\npowiaty$I = locm[, 1]\npowiaty$I_pvalue = locm[, 5]\n\nPoniższa mapa przedstawia rozkład wartości statystyki lokalnej I Morana, bez uwzględniania informacji o istotności statystycznej wyniku. Kolorem niebieskim zaznaczono powiaty dla których odnotowano wartości wskazujące na autokorelację ujemną, a kolorem czerwonym na autokorelację dodatnią.\n\ntm_shape(powiaty) + \n  tm_polygons(fill = \"I\",  palette = \"-RdBu\", midpoint = 0) \n\n\n\n\n\n\n\n\nNa poniższej mapie kolorem zilustrowano wartości poziomu isotności wyliczonego dla każdego powiatu. Powiaty oznaczone kolorem szarym na mapie nie mają istotnych statystycznie wartości statystyki lokalnej I Morana.\n\ntm_shape(powiaty) + tm_polygons(fill = \"I_pvalue\", breaks = c(0, 0.001, 0.01, 0.05, 1), palette = c(\"red\", \"orange\", \"yellow\", \"darkgrey\"), colorNA = \"white\")\n\n\n\n\n\n\n\n\nStatystyka lokalna I Morana umożliwia identyfikację klastrów wysokich lub niskich wartości. Do identyfikacji klastrów wykorzystuje się wyłącznie istotne statystycznie wartości statystyki I Morana. Powiaty zostaną sklasyfikowane na 4 grupy: - wysokie wartości otoczone wysokimi wartościami - niskie wartości otoczone niskimi wartościami - wysokie wartości otoczone niskimi wartościami - niskie wartości otoczone wysokimi wartościami.\nDo podziału na 4 grupy wykorzystuje się wykres rozrzutu Morana.\n\n#standaryzacja wartości zmiennej SB2023\npowiaty$SB_scaled = as.vector(scale(powiaty$SB2023))\n#obliczenie wartości standaryzowanych dla zmiennej opóźnionej przestrzennie\npowiaty$SB_scaled_lag = lag.listw(lw, powiaty$SB_scaled)\n\n#klasyfikacja na 4 grupy biorąc pod uwagę kwartę wykresu rozrzutu oraz informację czy statystyka jest istotna statystycznie\npowiaty = powiaty |&gt;\n  mutate(mcluster_type = case_when(\n    powiaty$SB_scaled &gt;= 0 & powiaty$SB_scaled_lag &gt;= 0 & locm[, 5] &lt;= 0.05 ~ \"High-High\",\n    powiaty$SB_scaled &lt;= 0 & powiaty$SB_scaled_lag &lt;= 0 & locm[, 5] &lt;= 0.05 ~ \"Low-Low\",\n    powiaty$SB_scaled &gt;= 0 & powiaty$SB_scaled_lag &lt;= 0 & locm[, 5] &lt;= 0.05 ~ \"High-Low\",\n    powiaty$SB_scaled&lt;= 0 & powiaty$SB_scaled_lag &gt;= 0 & locm[, 5] &lt;= 0.05 ~ \"Low-High\",\n    locm[, 5] &gt; 0.05 ~ \"Not Significant\"\n  )) # classify the cluster types\n\n\ncluster_colors &lt;- c(\"High-High\"= \"red\", \"High-Low\" = \"lightcoral\", \"Low-High\" = \"steelblue1\", \"Low-Low\" = \"blue\", \"Not Significant\" = \"lightgrey\" )\nMyPalette = cluster_colors[names(cluster_colors) %in% unique(powiaty$mcluster_type)]\n\ntm_shape(powiaty) + \n  tm_polygons(\"mcluster_type\", palette = MyPalette, colorNA = \"white\") + \n  tm_shape(woj_granica) + \n  tm_borders(col = \"black\", lwd = 3)\n\n\n\n\n\n\n\n\nPowyższą klasyfikację można także uzyskać stosując funkcję hotspot() z pakietu spdep.\n\nlocalmoran(powiaty$SB2023, listw = lw) |&gt;  hotspot(Prname=\"Pr(z != E(Ii))\", cutoff=0.05, p.adjust=\"none\") -&gt; powiaty$localI\n\n\ntable(powiaty$localI)\n\n\n  Low-Low  High-Low  Low-High High-High \n       33         1         6        43 \n\n\n\ncluster_colors &lt;- c(\"High-High\"= \"red\", \"High-Low\" = \"lightcoral\", \"Low-High\" = \"steelblue1\", \"Low-Low\" = \"blue\")\nMyPalette = cluster_colors[names(cluster_colors) %in% unique(powiaty$localI)]\n\ntm_shape(powiaty) + \n  tm_polygons(\"localI\", palette = MyPalette, colorNA = \"white\") + \n  tm_shape(woj_granica) + \n  tm_borders(col = \"black\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\n9.4.5.4 Lokalna statystyka C Geary’ego\nLokalna statystyka C Geary’ego pokazuje średnie różnice między obiektem a sąsiadami co pomaga znależć wartośc odstające. Niskie wartości lokalnego Geary’ego wskazują na dodatnią autokorelację przestrzenną, a duże na ujemną autokorelację przestrzenną.\n\n0: brak autokorelacji\n\\(C_{i} &lt; 0\\) - niskie wartości wskazują na dodatnią autokorelację przestrzenną, tj. podobieństwo regionu \\(i\\) z sąsiednimi\n\\(C_{i} &gt; 0\\) - wysokie wartości wskazują na ujemną autokorelację przestrzenną, tj. region nie ma podobnych wartości do sąsiadów\n\nW R lokalną statystykę C Geary’ego oblicza się używając funkcji localC_perm() z pakietu spdep, który wymaga zdefiniowania zmiennej uwzględnianej w analizie oraz macierzy wag przestrzennych (argument listw). Funkcja hotspot() z pakietu spdep zwraca wektor z przypisanymi kategoriami klastrów dla określonego poziomu isotności. Kategorie te można przypisac do obiektu przestrzennego powiaty oraz zwizualizować.\n\nlocalC_perm(powiaty$SB2023, listw = lw, nsim = 99) |&gt;  hotspot(Prname=\"Pr(z != E(Ci)) Sim\", cutoff=0.05, p.adjust=\"none\") -&gt; powiaty$localC\n\n\ntable(powiaty$localC)\n\n\nHigh-High   Low-Low  Negative \n       28        49         1 \n\n\n\ncluster_colors &lt;- c(\"High-High\"= \"red\", \"Low-Low\" = \"blue\",  \"Other Positive\" = \"lightcoral\", \"Negative\" = \"steelblue1\", \"Insignificant\" = \"lightgrey\" )\n\nMyPalette = cluster_colors[names(cluster_colors) %in% unique(powiaty$localC)]\n\ntm_shape(powiaty) + \n  tm_polygons(\"localC\", palette = MyPalette, colorNA = \"white\") + \n  tm_shape(woj_granica) + \n  tm_borders(col = \"black\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\n9.4.5.5 Lokalna statystyka G Getisa & Orda\nWskaźnik do identyfikacji lokalnych zależności przestrzennych (koncentracji przestrzennych), które nie pojawiają się w analizie globalnej:\n\n\\(G_i &gt; 0\\) - zgrupowanie regionów o wysokich wartościach (region \\(i\\) oraz regiony sąsiednie mają wysokie wartości) - tzw. klastr wysokich wartości (hot-spots)\n\\(G_i &lt; 0\\) - zgrupowanie regionow o niskich wartościach (region \\(i\\) jest otoczony przez podobne mu regiony o niskich wartościach) - tzw. cold-spots\n\nJako wysokie wartości uznaje się wartości wyższe od średniej, a jako niskie wartości uznaje się wartości niższe od średniej.\nLokalna statystyka G Getisa & Orda nie mierzy autokorelacji ujemnej. \n\nW R lokalną statystykę G Getisa & Orda oblicza się używając funkcji localG_perm() z pakietu spdep, który wymaga zdefiniowania zmiennej uwzględnianej w analizie oraz macierzy wag przestrzennych (argument listw). Funkcja hotspot() z pakietu spdep zwraca wektor z przypisanymi kategoriami klastrów dla określonego poziomu isotności. Kategorie te można przypisac do obiektu przestrzennego powiaty oraz zwizualizować.\n\nlocalG = localG_perm(powiaty$SB2023, listw = lw, nsim = 99)\nlocalG |&gt;  hotspot(Prname=\"Pr(z != E(Gi)) Sim\", cutoff=0.05, p.adjust=\"none\") -&gt; powiaty$localG\n\n\ntable(powiaty$localG)\n\n\n Low High \n  41   31 \n\n\n\ncluster_colors &lt;- c(\"Low\" = \"blue\", \"High\"= \"red\")\nMyPalette = cluster_colors[names(cluster_colors) %in% unique(powiaty$localG)]\n\ntm_shape(powiaty) + \n  tm_polygons(\"localG\", palette = MyPalette, colorNA = \"white\") + \n  tm_shape(woj_granica) + \n  tm_borders(col = \"black\", lwd = 3)",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miary autokorelacji danych obszarowych</span>"
    ]
  },
  {
    "objectID": "12_poligony_autokorelacja.html#przykład-2-stopa-bezrobocia-w-polsce-w-2004-roku",
    "href": "12_poligony_autokorelacja.html#przykład-2-stopa-bezrobocia-w-polsce-w-2004-roku",
    "title": "9  Miary autokorelacji danych obszarowych",
    "section": "9.5 Przykład 2: Stopa bezrobocia w Polsce w 2004 roku",
    "text": "9.5 Przykład 2: Stopa bezrobocia w Polsce w 2004 roku\n\n9.5.1 Dane\n\nlibrary(sf)\nbezrobocie = read.csv(\"data/bezrobocie_pl.csv\", sep = \";\", dec = \".\")\npowiaty_granica = read_sf(\"data/powiaty_pl.gpkg\")\nwoj_granica = read_sf(\"data/wojewodztwa.gpkg\")\n\npowiaty_granica$TERYT = as.integer(paste(powiaty_granica$TERYT, \"000\", sep=\"\"))\npowiaty = merge(powiaty_granica[, \"TERYT\"], bezrobocie, by.x = \"TERYT\", by.y = \"Kod\")\n\npowiaty$SB2004[is.na(powiaty$SB2004)]&lt;-0\n\n\ntm_shape(powiaty) + \n  tm_polygons(\"SB2004\", palette = \"YlOrBr\") + \n  tm_shape(woj_granica) + \n  tm_borders(col = \"black\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\n9.5.2 Miary autokorelacji przestrzennej\n\nlibrary(spdep)\n#macierz sasiedztwa\nnb_q = poly2nb(powiaty, queen = TRUE)\n#macierz wag przestrzennych. \nlw = nb2listw(nb_q, style = \"W\")\n\n#globalna statystyka Morana \nmoran_test = moran.mc(powiaty$SB2004, listw = lw, nsim = 99)\n\n# globalna statystyka C Geary'ego \ngeary_test = geary.mc(powiaty$SB2004, listw = lw, nsim = 99)\n\n#globalna statystyka Getisa i Orda \ng_test = globalG.test(powiaty$SB2004, listw = lw)\n\n#lokalna statystyka Morana - klastry wartości \nlocalmoran(powiaty$SB2004, listw = lw) |&gt;  hotspot(Prname=\"Pr(z != E(Ii))\", cutoff=0.05, p.adjust=\"none\") -&gt; powiaty$localI\n\n# lokalna statystyka Geary'ego \nlocalC_perm(powiaty$SB2004, listw = lw, nsim = 99) |&gt;  hotspot(Prname=\"Pr(z != E(Ci)) Sim\", cutoff=0.05, p.adjust=\"none\") -&gt; powiaty$localC\n\n# lokalna statstyka Getisa i Orda\nlocalG_perm(powiaty$SB2004, listw = lw, nsim = 99)|&gt;  hotspot(Prname=\"Pr(z != E(Gi)) Sim\", cutoff=0.05, p.adjust=\"none\") -&gt; powiaty$localG\n\n\n\n9.5.3 Wizualizacja wyników\n\nlibrary(tmap)\n#wizualizacja wyników\n\n#local I\ncluster_colors &lt;- c(\"High-High\"= \"red\", \"High-Low\" = \"lightcoral\", \"Low-High\" = \"steelblue1\", \"Low-Low\" = \"blue\")\nMyPalette = cluster_colors[names(cluster_colors) %in% unique(powiaty$localI)]\n\ntm_i = tm_shape(powiaty) + \n  tm_polygons(\"localI\", palette = MyPalette, colorNA = \"white\") + \n  tm_shape(woj_granica) + \n  tm_borders(col = \"black\", lwd = 3)\n\n#local C\ncluster_colors &lt;- c(\"High-High\"= \"red\", \"Low-Low\" = \"blue\",  \"Other Positive\" = \"lightcoral\", \"Negative\" = \"steelblue1\")\nMyPalette = cluster_colors[names(cluster_colors) %in% unique(powiaty$localC)]\n\ntm_c = tm_shape(powiaty) + \n  tm_polygons(\"localC\", palette = MyPalette, colorNA = \"white\") + \n  tm_shape(woj_granica) + \n  tm_borders(col = \"black\", lwd = 3)\n\n#local G\ncluster_colors &lt;- c(\"Low\" = \"blue\", \"High\"= \"red\")\nMyPalette = cluster_colors[names(cluster_colors) %in% unique(powiaty$localG)]\n\ntm_g  = tm_shape(powiaty) + \n  tm_polygons(\"localG\", palette = MyPalette, colorNA = \"white\") + \n  tm_shape(woj_granica) + \n  tm_borders(col = \"black\", lwd = 3)\n\ntmap_arrange(tm_i, tm_c, tm_g)",
    "crumbs": [
      "DANE OBSZAROWE",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miary autokorelacji danych obszarowych</span>"
    ]
  },
  {
    "objectID": "13_pola.html",
    "href": "13_pola.html",
    "title": "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
    "section": "",
    "text": "Analiza danych ciągłych przestrzennie w R\nDane powierzchniowe ciągłe przestrzennie (surface data, spatially continous data) zwane też geostatystycznymi, są gromadzone w sposób ciągły w przestrzeni dwuwymiarowej (np. temperatura powietrza, opady atmosferyczne, występowanie złóż surowców naturalnych). W przypadku danych powierzchniowych ciągłych przestrzennie pomiaru atrybutów dokonuje się w wybranym zbiorze lokalizacji. Następnie wykorzystuje się metody geostatystyczne, które pozwalają na analizę zmienności przestrzennej lub szacowanie wartości zmierzonej zmiennej w nieznanych lokalizacjach.",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE"
    ]
  },
  {
    "objectID": "13_pola.html#analiza-danych-ciągłych-przestrzennie-w-r",
    "href": "13_pola.html#analiza-danych-ciągłych-przestrzennie-w-r",
    "title": "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
    "section": "",
    "text": "Pakiet terra\nPakiet terra dostarcza funkcji do pracy z danymi wektorowymi oraz rastrowymi, m.in w zakresie tworzenia siatek interpolacyjnych (funkcja rast()) oraz interpolacji przestrzennej (interpolate(), voronoi())\n\n\nPakiet gstat\nPakiet gstat dostarcza funkcji do geostatystycznej analizy danych:\n\nfunkcja variogram() - tworzenie semiwariogramów\nfunkcje vgm() oraz fit.variogram() - modelowanie semiwariogramów\nfunkcja gstat() - tworzenie interpolacji przestrzennej jedną z metod krigingu lub metodą IDW.",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE"
    ]
  },
  {
    "objectID": "14_pola_interpolacja.html",
    "href": "14_pola_interpolacja.html",
    "title": "10  Metody interpolacji",
    "section": "",
    "text": "10.1 Interpolacja przestrzenna\nOdtworzenie obliczeń z tego rozdziału wymaga załączenia poniższych pakietów\nInterpolacje przestrzenne są jedną z ważniejszych procedur obliczeniowych w systemach geoinformacyjnych. Interpolacja przestrzenna opiera się zbiorze punktów pomiarowych i zasadzie podobieństwa przestrzennego, malejącego wraz z odległością. W procesie interpolacji, na podstawie danych punktowych oblicza się wartość zmiennej w nieznanej lokalizacji, dla której nie mamy danych pomiarowych. Większość metod interpolacji wymaga stworzenia siatki interpolacyjnej (pustego rastra), dla którego wykonuje się interpolację.\nInterpolacja ma sens, jeśli interpolowana cecha charakteryzuje się ciągłością zmienności przestrzennej, rozumianej jako tendencja wzrostu zróżnicowania wartości wraz ze wzrostem odległości pomiędzy punktami.\nPrzykłady zmiennych, dla których można wykonać interpolację:\nPrzykłady zastosowania interpolacji:",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metody interpolacji</span>"
    ]
  },
  {
    "objectID": "14_pola_interpolacja.html#interpolacja-przestrzenna",
    "href": "14_pola_interpolacja.html#interpolacja-przestrzenna",
    "title": "10  Metody interpolacji",
    "section": "",
    "text": "Dane dotyczące wysokości terenu\nTemperatura powietrza\nOpady\nGrubość pokrywy śnieżnej\nGęstość zaludnienia\nZanieczyszczenia powietrza\n\n\n\nOpracowanie map pola opadu, temperatury i innych elementów meteorologicznych.\nOszacowanie wysokości terenu w danym punkcie na podstawie cyfrowego modelu wysokości (DEM).\nPrzestrzenny rozkład zanieczyszczenia powietrza.\nZmiana rozdzielczości przestrzennej rastra przy jego przekształceniu do innego układu współrzędnych.",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metody interpolacji</span>"
    ]
  },
  {
    "objectID": "14_pola_interpolacja.html#klasyfikacja-metod-interpolacji-przestrzennej",
    "href": "14_pola_interpolacja.html#klasyfikacja-metod-interpolacji-przestrzennej",
    "title": "10  Metody interpolacji",
    "section": "10.2 Klasyfikacja metod interpolacji przestrzennej",
    "text": "10.2 Klasyfikacja metod interpolacji przestrzennej\nMożna wyróżnić dwie główne grupy metod interpolacji:\n\nmetody deterministyczne\nmetody statystyczne\n\n\n10.2.1 Metody deterministyczne\nMetody deterministyczne charakteryzują się tym, że ich parametry są zazwyczaj ustalane w oparciu o funkcję odległości lub powierzchni. Nie dostarczają one szacunków na temat oceny błędu modelu. Zaletą tych modeli jest ich prostota oraz krótki czas obliczeń.\nDo metod deterministycznych należą, między innymi:\n\nMetoda diagramów Woronoja (ang. Voronoi diagram)\nMetoda średniej ważonej odległością (ang. Inverse Distance Weighted - IDW)\nFunkcje wielomianowe (ang. Polynomials)\nFunkcje sklejane (ang. Splines)\n\n\n\n10.2.2 Metody statystyczne\nMetody statystyczne charakteryzują się tym, że ich parametry określane są w oparciu o teorię prawdopodobieństwa, dodatkowo wynik estymacji zawiera także oszacowanie błędu. Metody te zazwyczaj wymagają większych zasobów sprzętowych.\nDo metod statystycznych należą, między innymi:\n\nKriging\nModele regresyjne\nModele bayesowskie\n\nW tym rozdziale omówione zostaną przykłady metod deterministycznych, a w kolejnym rozdziale wprowadzone zostaną metody estymacji geostatystycznych.",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metody interpolacji</span>"
    ]
  },
  {
    "objectID": "14_pola_interpolacja.html#metody-interpolacji-w-r",
    "href": "14_pola_interpolacja.html#metody-interpolacji-w-r",
    "title": "10  Metody interpolacji",
    "section": "10.3 Metody interpolacji w R",
    "text": "10.3 Metody interpolacji w R\n\n10.3.1 Diagramy Woronoja\nMetoda diagramów Woronoja polega na stworzeniu nieregularnych poligonów na podstawie analizowanych punktów, a następnie wpisaniu w każdy poligon wartości odpowiadającego mu punktu.\n\nDane\n\n#granica obszaru wczytana jako obiekt klasy SpatVector z pakietu terra \ngranica = vect(\"data/granica.gpkg\")\n\n\nlibrary(terra)\npunkty = read.csv(\"data/punkty.csv\")\npunkty = punkty[!is.na(punkty$temp),]\npunkty_vect = vect(punkty, c(\"x\", \"y\"), crs = granica)\n\n\n\nInterpolacja\nW poniższym przykładzie metoda poligonów Woronoja została zastosowana z użyciem funkcji voronoi() z pakietu terra.\n\n#interpolacja\nvoronoi_interp = voronoi(punkty_vect)\n\nFunkcja crop() z pakietu terra służy do przycięcia warstwy wektorowej przez inny wektor.\n\nvoronoi_interp_granica = crop(voronoi_interp, granica)\n\n\n\nWizualizacja\n\ntm_shape(voronoi_interp_granica) +\n  tm_polygons(fill = \"temp\",\n              fill.scale = tm_scale_continuous(n = 10, values = \"-spectral\"), \n              fill.legend = tm_legend(title = \"Diagramy Woronoja:\\ntemperatura\")) +\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n10.3.2 Metoda średniej ważonej odległością (IDW)\nMetoda średniej ważonej odległością (IDW) jest jedną z najczęściej stosowanych metod interpolacji. Metoda IDW wylicza wartość dla każdej komórki na podstawie wartości punktów obokległych ważonych odwrotnością ich odległości. W efekcie, czym bardziej jest punkt oddalony, tym mniejszy jest jego wpływ na interpolowaną wartość.\n\n10.3.2.1 Wykładnik potęgowy\nWagę punktów ustala się z użyciem argumentu wykładnika potęgowego (idp, ang. inverse distance weighting power).\n\nDomyślnie stosuje się wartość wykładnik idp=2. Wartość 0 oznacza, że wszystkie punkty będą miały taką samą wagę. Wyższa wartość wykładnika oznacza, że największy wpływ będą miały najbliższe punkty pomiarowe. W efekcie otrzymana powierzchnia będzie bardziej szczegółówa (mniej wygładzona).\nNiższa wartość wykładnika oznacza większy wpływ punktów bardziej odległych, co skutkuje nardziej wygładzoną powierzchnią powstałą w wyniku interpolacji.\n\n\n10.3.2.2 Przykład\nWartość w nieznanej lokalizacji jest obliczana na podstawie wzoru:\n\\[z = \\frac{\\sum_{i=1}^{n} w_iz_i}{\\sum_{i=1}^{n} w_i}\\] Wartość wagi \\(w_i\\) dla danego punktu jest obliczana jako:\n\\[w_i = \\frac{1}{d_{i}^{p}}\\]\nZatem:\n\\[z = \\frac{\\sum_{i=1}^{n} \\frac{z_i}{d_{i}^{p}}}{\\sum_{i=1}^{n} \\frac{1}{d_{i}^{p}}}\\]\nOszacujemy wartość zmiennej (w czerwonym punkcie) na podstawie wartości oraz odległości przedstawionych na poniższym rysunku.\n\nObliczenie wartości \\(z\\) dla wykładnika potęgi równego 1:\n\\[z = \\frac{10/100 + 15/150 + 20/600}{1/100 + 1/150 + 1/600} =\n\\frac{\\frac{60 + 60 + 20}{600}}{\\frac{6 + 4 + 1}{600}} =\n\\frac{\\frac{140}{600}}{\\frac{11}{600}} = \\frac{140}{11} = 12.72727\\]\nObliczenie wartości \\(z\\) dla wykładnika potęgi równego 2:\n\\[z = \\frac{10/100^2 + 15/150^2 + 20/600^2}{1/100^2 + 1/150^2 + 1/600^2} = 11.69811\\]\nPoniższa funkcja obliczy wartość w punkcie na podstawie wektora wartości obokległych punktów (argument wartosc), oraz odległości między punktami o znanych wartościach a punktem o nieznanej wartości (argument d). Argument power oznacza wykładnik potęgi.\n\nidw &lt;- function(wartosc, d, power = 2) {\n\n  # Obliczanie wag\n  wagi &lt;- 1 / (d^power)\n  # Obliczanie wartości z\n  estymowana_wartosc &lt;- sum(wagi * wartosc) / sum(wagi)\n\n  return(estymowana_wartosc)\n}\n\nObliczenie wartości \\(z\\) dla wykładnika potęgi równego 1:\n\nwartosc = c(10, 15, 20)\nd = c(100, 150, 600)\nidw(wartosc, d, power = 1)\n\n[1] 12.72727\n\n\n\nWykorzystując powyższą funckję oblicz wartość z wykorzystując wykładnik potęgi 2.\n\n\n\n10.3.2.3 IDW w R\n\n\nDane\n\n#granica obszaru wczytana jako obiekt klasy SpatVector z pakietu terra \ngranica = vect(\"data/granica.gpkg\")\n\n\npunkty = read.csv(\"data/punkty.csv\")\npunkty = punkty[!is.na(punkty$temp),]\n\n\n\nTworzenie siatki\nMetoda IDW wymaga utworzenia siatki interpolacyjnej (pustego rastra). Siatka interpolacyjna zostanie utworzona jako obiekt SpatRaster wykorzystując funkcję rast() z pakietu terra.\n\nsiatka = rast(ext = granica, res = 50, crs = crs(granica))\nsiatka\n\nclass       : SpatRaster \nsize        : 172, 229, 1  (nrow, ncol, nlyr)\nresolution  : 50, 50  (x, y)\nextent      : 745541.7, 756991.7, 712651.6, 721251.6  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \n\n\n\n\nInterpolacja\nWykonanie interpolacji w siatce wymaga stworzenia obiektu klasy gstat z parametrami. Obiekt ten tworzony jest wykorzystując funkcję gstat() z pakietu gstat.\nObiekt klasy gstat składa się z kilku list zawierających:\n\nformula - formuła wykorzystywana do interpolacji (w przykładzie temp~1),\ndata - ramka danych z danymi wejściowymi zawierającymi wartości estymowanej zmiennej (w przykładzie obiekt punkty),\nlocations - formuła określająca współrzędne punktów dla których ma być wykonana interpolacja (x, y to nazwy kolumn w obiekcie punkty, które zawierają współrzędne)\ninne parametry (np. set=list(idp = 2) pozwala na zdefiniowanie wykładnika potęgowego używanego w metodzie IDW).\n\n\nlibrary(gstat)\nidw_param = gstat(id = \"temp\",\n                  formula = temp ~ 1, \n                  locations = ~x+y,\n                  data = punkty, \n                  set=list(idp = 2))\n\nFunkcja interpolate() z pakietu terra wykonuje interpolację dla obiektu siatki (siatka) oraz zdefiniowanych parametrów (idw_param).\n\nidw_interp = interpolate(siatka, idw_param)\n\n[inverse distance weighted interpolation]\n[inverse distance weighted interpolation]\n\n\nFunkcja mask() jest wykorzystywana do przycięcia obiektu rastrowego do granic obiektu wektorowego.\n\nidw_interp_granica = mask(idw_interp, granica)\n\n\n\nWizualizacja\n\ntm_shape(idw_interp_granica) +\n  tm_raster(col = c(\"temp.pred\"), \n            col.scale = tm_scale_continuous(n = 10, values = \"-spectral\"), \n            col.legend = tm_legend(title = \"IDW\")) +\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n10.3.3 Funkcje wielomianowe\nInterpolacja z wykorzystaniem funkcji wielomianowych dopasowuje gładką powierzchnię zdefiniowaną przez funkcję matematyczną (wielomian) do punktowych danych. Metoda jest czuła na występowanie odstających wartości.\n\nDane\nPakiet gstat wykorzystuje dane w formacie pakietu sp. Obiekt punkty trzeba przekonwertować z formatu sf do formatu SpatialPoints z pakietu sp.\n\nlibrary(terra)\nlibrary(sf)\n#punkty przekształcone do warstwy geoprzestrzennej typu sf\npunkty = read.csv(\"data/punkty.csv\")\npunkty = st_as_sf(punkty, coords = c(\"x\", \"y\"), crs = \"EPSG:2180\", remove = FALSE)\npunkty = punkty[!is.na(punkty$temp), ]\n\n#granica obszaru wczytana jako obiekt klasy SpatVector z pakietu terra \ngranica = vect(\"data/granica.gpkg\")\n\n\nsp_punkty &lt;- as(punkty, \"Spatial\")\n\n\n\nTworzenie siatki\nFunkcje wielomianowe wymagają utworzenia siatki interpolacyjnej (pustego rastra). Siatka interpolacyjna zostanie utworzona jako obiekt SpatRaster wykorzystując funkcję rast() z pakietu terra, a następnie przekształcona do obiektu SpatialGrid z pakietu sp wykorzystywanego przez pakiet gstat.\n\nlibrary(terra)\nsiatka = rast(ext = granica, res = 50, crs = crs(granica))\nsiatka\n\nclass       : SpatRaster \nsize        : 172, 229, 1  (nrow, ncol, nlyr)\nresolution  : 50, 50  (x, y)\nextent      : 745541.7, 756991.7, 712651.6, 721251.6  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \n\n\nFunkcja xyFromCell() pobiera współrzędne środka komórki rastra. Funkcja coordinates() z pakietu sp na podstawie współrzędnych tworzy obiekt typu Spatial Points. Funkcja proj4string ustawia układ odniesienia.\n\nlibrary(sp)\nsiatka_xy &lt;- data.frame(xyFromCell(siatka, 1:ncell(siatka)))\ncoordinates(siatka_xy) &lt;- ~x + y\nproj4string(siatka_xy) &lt;- CRS(\"EPSG:2180\")\n\n\n\nInterpolacja\nStosowanie funkcji wielomianowych w R może odbyć się z wykorzystaniem funkcji gstat() z pakietu gstat. Wymaga ona utworzenia obiektu klasy gstat i zdefiniowania trzech argumentów:\n\nformula określającego naszą analizowaną cechę (temp~1 mówi, że chcemy interpolować wartość temperatury zależnej od samej siebie),\ndata określający analizowany zbiór danych,\ndegree określający stopień wielomianu.\n\n\nlibrary(gstat)\nwielomian_1 &lt;- gstat(formula = temp ~ 1, data = sp_punkty, degree = 1)\n\nNastępnie funkcja predict() przenosi nowe wartości na wcześniej stworzoną siatkę.\n\nwielomian_1_pred = predict(wielomian_1, newdata = siatka_xy)\n\n[ordinary or weighted least squares prediction]\n\n\nOtrzymamy wynik w formacie SpatialPointsDataFrame, który musimy przekształcić do formatu rastrowego (SpatRaster).\n\nwielomian_1_pred_rst &lt;- setValues(siatka, wielomian_1_pred$var1.pred)\n\nWynik interpolacji można przyciąć do granicy.\n\nwielomian_1_pred_rst_granica = mask(wielomian_1_pred_rst, granica)\n\n\n\nWizualizacja\n\ntm_shape(wielomian_1_pred_rst_granica) +\n  tm_raster(col = c(\"lyr.1\"), \n            col.scale = tm_scale_continuous(values = \"-spectral\", n = 10), \n            col.legend = tm_legend(title = \"Wielomian pierwszego stopnia\")) +\n  tm_layout(legend.outside = TRUE)\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\nWielomian II stopnia\n\nwielomian_2 &lt;- gstat(formula = temp ~ 1, data = sp_punkty, degree = 2)\nwielomian_2_pred = predict(wielomian_2, newdata = siatka_xy)\n\n[ordinary or weighted least squares prediction]\n\nwielomian_2_pred_rst &lt;- setValues(siatka, wielomian_2_pred$var1.pred)\nwielomian_2_pred_rst_granica = mask(wielomian_2_pred_rst, granica)\n\n\ntm_shape(wielomian_2_pred_rst_granica) +\n  tm_raster(col = c(\"lyr.1\"), \n            col.scale = tm_scale_continuous(values = \"-spectral\", n = 10), \n            col.legend = tm_legend(title = \"Wielomian drugiego stopnia\")) +\n  tm_layout(legend.outside = TRUE)\n\n[plot mode] fit legend/component: Some legend items or map compoments do not\nfit well, and are therefore rescaled.\nℹ Set the tmap option `component.autoscale = FALSE` to disable rescaling.\n\n\n\n\n\n\n\n\n\n\n\nWielomian III stopnia\n\nwielomian_3 &lt;- gstat(formula = temp ~ 1, data = sp_punkty, degree = 3)\nwielomian_3_pred = predict(wielomian_3, newdata = siatka_xy)\n\n[ordinary or weighted least squares prediction]\n\nwielomian_3_pred_rst &lt;- setValues(siatka, wielomian_3_pred$var1.pred)\nwielomian_3_pred_rst_granica = mask(wielomian_3_pred_rst, granica)\n\n\ntm_shape(wielomian_3_pred_rst_granica) +\n  tm_raster(col = c(\"lyr.1\"), \n            col.scale = tm_scale_continuous(values = \"-spectral\", n = 10), \n            col.legend = tm_legend(title = \"Wielomian trzeciego stopnia\")) +\n  tm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n10.3.4 Funkcje sklejane\nInterpolacja z wykorzystaniem funkcji sklejanych dopasowuje krzywą powierzchnię do wartości analizowanych punktów. W wyniku interpolacji otrzymuje się gładką powierzchnię przechodzącą dokładnie przez punkty wejściowe. Metoda ta jest stosowana np. do generowanie delikatnie zmieniających się powierzchni, takich jak wysokość, wysokość zwierciadła wody lub stężenie zanieczyszczeń.\nInterpolacja z użyciem funkcji sklejanych wykonuje się wykorzystując funkcję Tps() z pakietu fields.\n\nDane\n\nlibrary(terra)\nlibrary(sf)\n#punkty przekształcone do warstwy geoprzestrzennej typu sf\npunkty = read.csv(\"data/punkty.csv\")\nsf_punkty = st_as_sf(punkty, coords = c(\"x\", \"y\"), crs = \"EPSG:2180\", remove = FALSE)\nsf_punkty = sf_punkty[!is.na(sf_punkty$temp), ]\n\n#granica obszaru wczytana jako obiekt klasy SpatVector z pakietu terra \ngranica = vect(\"data/granica.gpkg\")\n\n\n\nTworzenie siatki\n\nlibrary(terra)\nsiatka = rast(ext = granica, res = 50, crs = crs(granica))\nsiatka\n\nclass       : SpatRaster \nsize        : 172, 229, 1  (nrow, ncol, nlyr)\nresolution  : 50, 50  (x, y)\nextent      : 745541.7, 756991.7, 712651.6, 721251.6  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \n\n\n\n\nInterpolacja\n\nlibrary(fields) \ntps &lt;- Tps(st_coordinates(sf_punkty), sf_punkty$temp)\n\n# use model to predict values at all locations\ntps_interp &lt;- interpolate(siatka, tps)\n\ntps_interp_granica &lt;- mask(tps_interp, granica)\n\n\ntm_shape(tps_interp_granica) +\n  tm_raster(col = c(\"lyr.1\"), \n            col.scale = tm_scale_continuous(values = \"-spectral\", n = 10), \n            col.legend = tm_legend(title = \"Funkcje sklejane\")) +\n  tm_layout(legend.outside = TRUE)",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metody interpolacji</span>"
    ]
  },
  {
    "objectID": "14_pola_interpolacja.html#porównanie-modeli-deterministycznych",
    "href": "14_pola_interpolacja.html#porównanie-modeli-deterministycznych",
    "title": "10  Metody interpolacji",
    "section": "10.4 Porównanie modeli deterministycznych",
    "text": "10.4 Porównanie modeli deterministycznych",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metody interpolacji</span>"
    ]
  },
  {
    "objectID": "15_pola_modelowanie.html",
    "href": "15_pola_modelowanie.html",
    "title": "11  Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej",
    "section": "",
    "text": "11.1 Geostatystyczna analiza danych\nW poniższych przykładach wykorzystamy dane zapisane w pliku punkty.csv oraz przestrzenny zasięg granicy obszaru (granica.gpkg).\nGeostatystyka to gałąź statystyki skupiająca się na przestrzennych lub czasoprzestrzennych zbiorach danych.\nCelem geostatystyki może być:\nGeostatystyczna analiza danych może przyjmować różną postać w zależności od postawionego celu analizy. Poniższy schemat przedstawia uproszczoną ścieżkę postępowania geostatystycznego.\nUproszczona ścieżka postępowania geostatystycznego.",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej</span>"
    ]
  },
  {
    "objectID": "15_pola_modelowanie.html#geostatystyczna-analiza-danych",
    "href": "15_pola_modelowanie.html#geostatystyczna-analiza-danych",
    "title": "11  Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej",
    "section": "",
    "text": "zrozumienie zmienności przestrzennej lub czasowej zjawiska,\ninterpolacja (estymacja przestrzenna),\nsymulowanie wartości,\noptymalizacja sieci pomiarowej.\n\n\n\n\nEtap 1: Pozyskanie i wstępna weryfikacja danych\nPunktem wyjścia analizy geostatystycznej jest posiadanie danych przestrzennych opisujących badane zjawisko, np. w postaci punktowej. Przykładem takiego zjawiska są punktowo wykonane pomiary temperatury.\n\n\n\n\n\nRozkład wartości temperatury powietrza w analizowanym obszarze.\n\n\n\n\n\n\nEtap 2: Nieprzestrzenna i przestrzenna eksploracja danych\nDane należy poddać eksploracji w celu ich lepszego poznania, wyszukania relacji między zmiennymi, czy znalezienia potencjalnych błędów.\nEksploracyjna analiza danych obejmuje:\n\nCharakterystykę statystyczną danych.\nSprawdzenie poprawności współrzędnych.\nSprawdzenie poprawności danych, w tym między innymi identyfikacja danych odstających lokalnie.\nWgląd w tym próbkowania.\nOgólny pogląd na zmienność przestrzenną, wykorzystanie prostej automatycznej procedury interpolacji.\n\n\n\nEtap 3: Analiza i interpretacja struktury przestrzennej\nJednym z celów geostatystycznej analizy danych jest identyfikacja i zrozumienie przestrzennej zmienności analizowanej cechy. Zmienność przestrzenna analizowanej cechy może być określona za pomocą semiwariancji, a następnie zwizualizowana za pomocą semiwariogramu.\nSemiwariogram empiryczny (oparty o danych) jest wykresem pokazującym relację pomiędzy odległością a semiwariancją (jedną z miar autokorelacji przestrzennej). Za jego pomocą możemy stwierdzić jak to zjawisko zmienia się w przestrzeni.\n\n\n\n\n\nWykres, nazywany semiwariogramem, reprezentujący niepodobieństwo wartości wraz z odległością dla zmiennej temperatura.\n\n\n\n\n\n\nEtap 4: Modelowanie matematyczne struktury przestrzennej (autokorelacji przestrzennej)\nModelowanie struktury przestrzennej polega na dopasowanie modelu (funkcji) do semiwariogramu empirycznego (wyliczonego z danych). Pozwala on zarówno na lepszy opis zmienności zjawiska, jak również służy do tworzenia estymacji czy też symulacji.\n\n\n\n\n\n\n\n\n\n\n\n11.1.1 Etap 5: Estymacja lub symulacja\nEstymacja to rodzaj interpolacji przestrzennej, która oblicza najbardziej potencjalnie możliwą wartość dla wybranej lokalizacji.\n\n\n\n\n\nEstymacja (oszacowanie) wartości badanej zmiennej dla całego obszaru.\n\n\n\n\nRolą symulacji jest natomiast generowanie równie prawdopodobne możliwości rozkładu badanej cechy.\n\n\n\n\n\nPrzykłady symulowanych wartości badanej zmiennej dla całego obszaru.",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej</span>"
    ]
  },
  {
    "objectID": "15_pola_modelowanie.html#wykres-rozrzutu-z-przesunięciem",
    "href": "15_pola_modelowanie.html#wykres-rozrzutu-z-przesunięciem",
    "title": "11  Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej",
    "section": "11.2 Wykres rozrzutu z przesunięciem",
    "text": "11.2 Wykres rozrzutu z przesunięciem\nW klasycznej statystyce relację między dwoma zmiennymi określa się za pomocą korelacji (np. współczynnik korelacji liniowej Pearsona). W geostatystyce badamy jedną zmienną, ale pomiędzy dwoma punktami odległymi od siebie o pewien dystans (określany jako lag, h). Wykorzystuje się w tym celu wykres rozrzutu z przesunięciem. Wykres rozrzutu z przesunięciem pokazuje korelację pomiędzy wartościami analizowanej cechy w pewnych grupach odległości. Taki wykres można stworzyć używając funkcji hscat() z pakietu gstat\n\nhscat(temp~1, data = punkty, breaks = seq(0, 4000, by = 500))\n\n\n\n\n\n\n\n\nPrzykładowo, na powyższym wykresie widać wartość cechy temp z kolejnymi przesunięciami - od 0 do 500 metrów, od 500 metrów do 1000 metrów, itd. W pierwszym przedziale wartość cechy temp z przesunięciem wykazuje korelację na poziomie 0,876, a następnie z każdym kolejnym przedziałem (odległością) ta wartość maleje. W przedziale 3500 do 4000 metrów osiąga jedynie 0,128. Pozwala to na stwierdzenie, że cecha temp wykazuje zmienność przestrzenną - podobieństwo jej wartości zmniejsza się wraz z odległością.",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej</span>"
    ]
  },
  {
    "objectID": "15_pola_modelowanie.html#semiwariancja",
    "href": "15_pola_modelowanie.html#semiwariancja",
    "title": "11  Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej",
    "section": "11.3 Semiwariancja",
    "text": "11.3 Semiwariancja\nZmienność przestrzenna analizowanej cechy może być określona m.in za pomocą semiwariancji \\(\\gamma(h)\\).\nSemiwariancja to połowa średniej kwadratu różnicy pomiędzy wartościami badanej zmiennej (\\(z\\)) w dwóch lokalizacjach odległych o wektor \\(h\\)\n\\[\\gamma(h) = \\frac{1}{2}(z(u_{\\alpha}) - z(u_{\\alpha} + h))^2\\]\n\n11.3.1 Obliczenie semiwariancji między dwoma punktami\nPrzykładowo, aby wyliczyć wartość semiwariancji (gamma) pomiędzy dwoma punktami musimy znać:\n\nwartość pierwszego z nich (\\(z(u_{\\alpha})\\)),\n\nw przykładzie \\(z(u_{\\alpha}) = 13.85\\) stopni Celcjusza\n\nwartość drugiego z nich (\\(z(u_{\\alpha} + h)\\),\n\nw przykładzie \\(z(u_{\\alpha} + h)=15.48\\) stopni Celsjusza.\n\n\nMożemy zatem obliczyć wartość semiwariancji:\n\\[\\gamma(h) = \\frac{1}{2}(13,85 - 15,48)^2 =\n\\frac{1}{2}(-1,63)^2 = \\\\\n\\frac{1}{2} \\times 2,6569 = 1,32845 \\approx 1,33 \\]\nKorzystając z wzoru na semiwariację otrzymujemy wartość równą ok. 1,33. Znamy również odległość między punktami (ok. 3240,89 metra), więc możemy w uproszczeniu stwierdzić, że dla tej pary punktów odległych o 3240 metrów wartość semiwariancji wynosi około 1,33.\nW R obliczenia wyglądałyby następująco:\n\n#wykorzystanie funkcji st_distance do obliczenia odległości między dwoma punktami\nodl = st_distance(punkty[c(1, 2), ])[2]\n#obliczenie wartości semiwariancji między dwoma puntami\ngamma = 0.5 * (punkty$temp[1] - punkty$temp[2]) ^ 2\ngamma\n\n[1] 1.331691\n\n\n\n\n11.3.2 Chmura semiwariogramu\nJeżeli w badanej próbie mamy \\(n\\) obserwacji oznacza to, że możemy zaobserwować \\(\\frac{1}{2}n(n-1)\\) par obserwacji. Dla każdej z par obserwacji obliczana jest semiwariancja. Tę semiwariancję można zaprezentować na wykresie zwanym chmurą semiwariogramu. Lokalizacja każdego punktu na chmurze semiwariogramu określona jest przez 2 współrzędne:\n\n\\(x\\): odległość między dwoma punktami\n\\(y\\): wartość semiwariancji.\n\nChmurę semiwariogramu w R używając funkcji variogram() z argumentem cloud = TRUE.\n\nvario_cloud = variogram(temp ~ 1, locations = punkty, \n                         cloud = TRUE)\nplot(vario_cloud)\n\n\n\n\nChmura semiwariogramu zmiennej temp.\n\n\n\n\nChmura semiwariogramu pozwala także na zauważenie par punktów, których wartości znacząco odstają.",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej</span>"
    ]
  },
  {
    "objectID": "15_pola_modelowanie.html#semiwariogram-charakterystyka-struktury-przestrzennej",
    "href": "15_pola_modelowanie.html#semiwariogram-charakterystyka-struktury-przestrzennej",
    "title": "11  Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej",
    "section": "11.4 Semiwariogram: charakterystyka struktury przestrzennej",
    "text": "11.4 Semiwariogram: charakterystyka struktury przestrzennej\nSemiwariogram jest wykresem pokazującym relację pomiędzy odległością a semiwariancją. Inaczej mówiąc, dla kolejnych odstępów (lagów) wartość semiwariancji jest uśredniana i przestawiania w odniesieniu do odległości.\n\\[\\hat{\\gamma}(h) = \\frac{1}{2N(h)}\\sum_{\\alpha=1}^{N(h)}(z(u_{\\alpha}) - z(u_{\\alpha}+h))^2\\]\n, gdzie \\(N(h)\\) oznacza liczbę par punktów w odstępie \\(h\\).\nW oparciu o semiwariogram empiryczny (czyli oparty na danych) możemy następnie dopasować do niego model.\n\n11.4.1 Tworzenie semiwariogramu\nStworzenie podstawowego semiwariogramu w pakiecie gstat odbywa się z użyciem funkcji variogram(). Należy w niej zdefiniować analizowaną zmienną (w tym przykładzie temp ~ 1) oraz zbiór punktowy (punkty).\n\nvario_par = variogram(temp ~ 1, locations = punkty)\nvario_par\n\n     np      dist     gamma dir.hor dir.ver   id\n1   113  214.0615  1.205854       0       0 var1\n2   296  492.6029  2.664303       0       0 var1\n3   450  794.4754  4.168964       0       0 var1\n4   659 1119.4215  5.258232       0       0 var1\n5   825 1429.3537  6.330027       0       0 var1\n6   930 1747.4842  7.078197       0       0 var1\n7   960 2059.9222  7.634849       0       0 var1\n8  1126 2380.7339  8.485727       0       0 var1\n9  1244 2693.4708  9.900739       0       0 var1\n10 1179 3012.0195  9.558005       0       0 var1\n11 1294 3321.8058 11.621229       0       0 var1\n12 1336 3645.0145 11.352249       0       0 var1\n13 1349 3959.8583 11.863130       0       0 var1\n14 1400 4277.9695 14.207606       0       0 var1\n15 1458 4595.7696 15.371019       0       0 var1\n\n\nDo wyświetlenia semiwariogramu służy funkcja plot(). Można również dodać informację o liczbie par punktów, jaka posłużyła do wyliczenia semiwariancji dla kolejnych odstępów poprzez argument plot.numbers = TRUE\n\nplot(vario_par, plot.numbers = TRUE)\n\n\n\n\nSemiwariogram empiryczny zmiennej temp wraz z zaznaczoną liczbą par dla każdego odstępu.\n\n\n\n\n\n\n11.4.2 Parametry semiwariogramu\nPrzy ustalaniu parametrów semiwariogramu powinno się stosować do kilku utartych zasad (tzw. rules of thumb):\n\nW każdym odstępie powinno się znaleźć co najmniej 30 par punktów.\nMaksymalny zasięg semiwariogramu (ang. cutoff distance) to 1/2 pierwiastka z badanej powierzchni (inne źródła mówią o połowie z przekątnej badanego obszaru/jednej trzeciej).\nLiczba odstępów powinna nie być mniejsza niż 10.\nOptymalnie maksymalny zasięg semiwariogramu powinien być dłuższy o 10-15% od zasięgu zjawiska.\nOptymalnie odstępy powinny być jak najbliżej siebie i jednocześnie nie być chaotyczne.\nWarto metodą prób i błędów określić optymalne parametry semiwariogramu.\n\n\n\n11.4.3 Obliczenia pomocnicze\n\nLiczba par obserwacji\nW zrozumieniu danych oraz przy określaniu parametrów semiwariogramu może pomóc szereg obliczeń pomocniczych. Przykładowo, aby wyliczyć liczbę par obserwacji można użyć poniższego kodu.\n\n0.5 * nrow(punkty) * (nrow(punkty) - 1)\n\n[1] 30381\n\n\n\n\nŚrednia odległość między punktami\nPowierzchnia zajmowana przez jedną próbkę jest osiągana poprzez podzielenie całej badanej powierzchni poprzez liczbę punktów.\n\npow_pr = st_area(granica) / nrow(punkty)\npow_pr\n\n256585.6 [m^2]\n\n\nŚrednia odległość między punktami to wartość pierwiastka z powierzchni zajmowanej przez jedną próbkę.\n\nsqrt(pow_pr)\n\n506.5428 [m]\n\n\n\n\nMaksymalny zasięg semiwariogramu\nOstatnim obliczeniem pomocniczym jest określenie połowy pierwiastka powierzchni. Może być ono następnie użyte do ustalenia maksymalnego zasięgu semiwariogramu.\n\npow = st_area(granica)\n0.5 * sqrt(pow)\n\n3980.472 [m]\n\n\n\n\n\n11.4.4 Modyfikacja semiwariogramu\nMaksymalny zasięg semiwariogramu (ang. cutoff distance) jest domyślnie wyliczany w pakiecie gstat jako 1/3 z najdłuższej przekątnej badanego obszaru. Można jednak tę wartość zmodyfikować używając argumentu cutoff .\n\nvario_par = variogram(temp ~ 1, locations = punkty,\n                      cutoff = 4000)\nplot(vario_par)\n\n\n\n\nSemiwariogram empiryczny zmiennej temp używając ręcznie ustalonej wartości maksymalnego zasięgu semiwariogramu.\n\n\n\n\nDodatkowo, domyślnie w pakiecie gstat odległość między przedziałami (ang. interval width) jest wyliczana jako maksymalny zasięg semiwariogramu podzielony przez 15. Można to oczywiście zmienić używając zarówno argumentu cutoff, jak i argumentu width mówiącego o szerokości odstępów\n\nvario_par = variogram(temp ~ 1, locations = punkty,\n                      cutoff = 4000, width = 100)\nplot(vario_par)\n\n\n\n\nSemiwariogram empiryczny zmiennej temp używając ręcznie ustalonej wartości szerokości odstępów.",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej</span>"
    ]
  },
  {
    "objectID": "15_pola_modelowanie.html#modelowanie-autokorelacji-przestrzennej",
    "href": "15_pola_modelowanie.html#modelowanie-autokorelacji-przestrzennej",
    "title": "11  Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej",
    "section": "11.5 Modelowanie autokorelacji przestrzennej",
    "text": "11.5 Modelowanie autokorelacji przestrzennej\nModelowanie autokorelacji przestrzennej polega na dopasowanie modelu (funkcji) do semiwariogramu empirycznego (wyliczonego z danych). Pozwala on zarówno na lepszy opis zmienności zjawiska, jak również służy do tworzenia estymacji czy też symulacji.\n\n11.5.1 Model semiwariogramu\nModel semiwariogramu składa się zazwyczaj z trzech podstawowych elementów:\n\nNugget - efekt nuggetowy - pozwala na określenie błędu w danych wejściowych oraz zmienności na dystansie krótszym niż pierwszy odstęp.\nSill - semiwariancja progowa - oznacza wariancję badanej zmiennej.\nRange - zasięg - to odległość do której istnieje przestrzenna korelacja.\n\n\n\n\n\n\nPodstawowe elementy modelu semiwariogramu.\n\n\n\n\n\n11.5.1.1 Modele podstawowe\nPakiet gstat zawiera 20 podstawowych modeli geostatystycznych, w tym najczęściej używane takie jak:\n\nNuggetowy (ang. Nugget effect model)\n\nokreśla sytuację, w której analizowana zmienna nie wykazuje autokorelacji (niepodobieństwo jej wartości nie wzrasta wraz z odległością)\nnie powinien być używany samodzielnie - w większości zastosowań jest on elementem modelu złożonego. Służy on do określania, między innymi, błędu pomiarowego czy zmienności na krótkich odstępach.\n\nSferyczny (ang. Spherical model)\n\njeden z najczęściej stosowanych modeli\n\nWykładniczy (ang. Exponential model)\n\njeden z najczęściej stosowanych modeli;\nnie ma skończonego zasięgu,\nzamiast zasięgu podaje się tzw. zasięg praktyczny, tj. odległość na jakiej model osiąga 95% wartości wariancji progowej\n\nGaussowski (ang. Gaussian model)\n\nposiada zasięg praktyczny definiowany jako 95% wartości wariancji progowej\njego cechą charakterystyczną jest paraboliczny kształt na początkowym odcinku.\n\nPotęgowy (ang. Power model)\n\nprzykład tzw. modelu nieograniczonego\nwartość modelu rośnie w nieskończoność, dlatego niemożliwe jest określenie jego zasięgu.\nw przypadku modelu potęgowego, parametr range oznacza wykładnik potęgowy.\n\n\nDo wyświetlenia listy nazw modeli i ich skrótów służy funkcja vgm().\n\nvgm()\n\n   short                                      long\n1    Nug                              Nug (nugget)\n2    Exp                         Exp (exponential)\n3    Sph                           Sph (spherical)\n4    Gau                            Gau (gaussian)\n5    Exc        Exclass (Exponential class/stable)\n6    Mat                              Mat (Matern)\n7    Ste Mat (Matern, M. Stein's parameterization)\n8    Cir                            Cir (circular)\n9    Lin                              Lin (linear)\n10   Bes                              Bes (bessel)\n11   Pen                      Pen (pentaspherical)\n12   Per                            Per (periodic)\n13   Wav                                Wav (wave)\n14   Hol                                Hol (hole)\n15   Log                         Log (logarithmic)\n16   Pow                               Pow (power)\n17   Spl                              Spl (spline)\n18   Leg                            Leg (Legendre)\n19   Err                   Err (Measurement error)\n20   Int                           Int (Intercept)\n\n\nMożna się również im przyjrzeć używając funkcji show.vgms()\n\nshow.vgms()\n\n\n\n\n\n\n\n\nIstnieje możliwość wyświetlenia tylko wybranych modeli podstawowych poprzez argument models\n\nshow.vgms(models = c(\"Sph\", \"Gau\", \"Pow\", \"Exp\"), \n          range = 1.4, max = 2.5)\n\n\n\n\n\n\n\n\nDodatkowo, można je porównać na jednym wykresie poprzez argument as.groups = TRUE\n\nshow.vgms(models = c(\"Sph\", \"Gau\", \"Pow\", \"Exp\"), \n          range = 1.4, max = 2.5, as.groups = TRUE)\n\n\n\n\n\n\n\n\n\n\n11.5.1.2 Dopasowanie modelu w R\nDo zbudowania modelu semiwariogramu należy wykonać szereg kroków:\n\nStworzyć i wyświetlić semiwariogram empiryczny analizowanej zmiennej z użyciem funkcji variogram() oraz plot().\nZdefiniować wejściowe parametry semiwariogramu.\n\nW najprostszej sytuacji wystarczy zdefiniować używany model/e poprzez skróconą nazwę używanej funkcji (model).\nMożliwe, ale nie wymagane jest także określenie wejściowej semiwariancji cząstkowej (psill) oraz zasięgu modelu (range) w funkcji vgm().\nUzyskany model można przedstawić w funkcji plot() podając nazwę obiektu zawierającego semiwariogram empiryczny oraz obiektu zawierającego model.\n\nDopasować parametry modelu używając funkcji fit.variogram().\n\nTo dopasowanie można również zwizualizować używając funkcji plot().\n\n\n\n\n\n11.5.2 Przykład: Analiza i modelowanie struktury przestrzennej zmiennej temperatura\n\nDane\n\npunkty = read.csv(\"data/punkty.csv\")\npunkty = st_as_sf(punkty, coords = c(\"x\", \"y\"), crs = \"EPSG:2180\")\npunkty = punkty[!is.na(punkty$temp), ]\n\n\n\nEtap 1: Stworzenie i wyświetlenie semiwariogramu empirycznego\nSemiwariogram empiryczny tworzy się używając funkcji variogram() z pakietu gstat(). Należy w niej zdefiniować analizowaną zmienną (w tym przykładzie temp ~ 1) oraz zbiór punktowy (punkty).\n\nvario = variogram(temp ~ 1, locations = punkty)\nplot(vario)\n\n\n\n\n\n\n\n\n\n\nEtap 2: Zdefiniowanie podstawowych parametrów semiwariogramu\nW tym kroku należy podjąć decyzję o typie modelu (sferyczny, wykładniczy, itp.) oraz zdefiniować podstawowe parametry semiwarigramu:\n\nnugget\n\nzazwyczaj semiwariogram nie zaczyna się w punkcie (0,0).\nwartość nuggetu odczytujemy jako przesunięcie wartości na osi y\n\nsemiwariancję progową (psill)\n\noznacza wariancję badanej zmiennej;\nwartość semiwariancji progowej odczytujemy z osi y: wartość semiwaiancji po ustabilizowaniu się jej wartości (wartość ta już nie rośnie wraz z odległością)\n\nzasięg (range)\n\nto odległość do której istnieje przestrzenna autokorelacja;\njest to wartość odczytana z osi x (odległość) do której następuje wzrost wartości semiwariancji wraz z odległością.\n\n\nW R do określenia typu modelu oraz jego podstawowych parametrów używa się funkcji vgm() z pakietu gstat\n\nmodel_sph = vgm(psill = 10, model = \"Sph\", range = 3000)\nmodel_sph\n\n  model psill range\n1   Sph    10  3000\n\n\nFunkcja plot() pozwala na wizualizację dopasowania modelu do danych. Najważniejsze jest dobre dopasowanie modelu do semiwariogramu empirycznego (punkty na wykresie poniżej) w początkowym odcinku semiwariogramu (tam gdzie obserwujemy wzrost semiwariancji wraz z odleglością).\n\nplot(vario, model = model_sph)\n\n\n\n\n\n\n\n\nDo powyżej zdefiniowanych parametrów dodamy jeszcze wartość efektu nuggetowego:\n\nmodel_sph2 = vgm(psill = 10, model = \"Sph\", range = 3000, nugget =0.5)\nmodel_sph2\n\n  model psill range\n1   Nug   0.5     0\n2   Sph  10.0  3000\n\n\n\nplot(vario, model = model_sph)\n\n\n\n\n\n\n\n\nMożemy zaobserwować, że dodanie wartości nuggetu “nieco podniosło” linię modelu powodując, że jest ona lepiej dopasowana w początkowym odcinku semiwariogramu.\n\n\nEtap 3: Dopasowanie parametrów modelu\nFunkcja fit.variogram() pozwala na automatyczne dopasowanie modelu w oparciu o wstępnie podane parametry.\n\nfitted_sph = fit.variogram(vario, model_sph2)\nfitted_sph\n\n  model      psill  range\n1   Nug  0.6755637    0.0\n2   Sph 13.6466875 5479.9\n\n\n\nplot(vario, model = fitted_sph)",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej</span>"
    ]
  },
  {
    "objectID": "16_pola_estymacja.html",
    "href": "16_pola_estymacja.html",
    "title": "12  Geostatystyczne metody estymacji",
    "section": "",
    "text": "12.1 Estymacje geostatystyczne\nOdtworzenie obliczeń z tego rozdziału wymaga załączenia poniższych pakietów oraz wczytania poniższych danych:\nJednym z celów geostatystycznej analizy danych jest estymacja - tj. szacowanie wartości zmiennej w nieznanych punktach na podstawie danych z próbkowania. W geostatystyce wykorzystuje się grupę metod estymacji (interpolacji) określanych jako kriging. Kriging zakłada, że wartość zmiennej w danej lokalizacji jest zależna od obokległych obserwacji, którym nadawane są odpowiednie wagi na podstawie stopnia przestrzennej korelacji.\nIstnieje szereg metod krigingu Do trzech podstawowych metod zalicza się:\nKriging można zastosować do oszacowania wartość dla wybranych punktów lub dla siatki o określonej rozdzielczości.",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geostatystyczne metody estymacji</span>"
    ]
  },
  {
    "objectID": "16_pola_estymacja.html#estymacje-geostatystyczne",
    "href": "16_pola_estymacja.html#estymacje-geostatystyczne",
    "title": "12  Geostatystyczne metody estymacji",
    "section": "",
    "text": "Kriging prosty (ang. Simple kriging) - zakłada, że średnia jest znana i stała na całym obszarze.\nKriging zwykły (ang. Ordinary kriging) - wykorzystuje do estymacji tylko najbliższe obserwacje, które można określić poprzez podanie liczby punktów, które mają być uwzględnione lub maksymalnej odległości z której mają być uwzględniane punkty.\nKriging z trendem (ang. Kriging with a trend) - wykorzystuje do estymacji (oprócz zmienności wartości wraz z odległością) położenie analizowanych punktów (tj. ich współrzędne x,y).",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geostatystyczne metody estymacji</span>"
    ]
  },
  {
    "objectID": "16_pola_estymacja.html#kriging-w-r",
    "href": "16_pola_estymacja.html#kriging-w-r",
    "title": "12  Geostatystyczne metody estymacji",
    "section": "12.2 Kriging w R",
    "text": "12.2 Kriging w R\n\n12.2.1 Pakiet gstat\nPakiet gstat dostarcza funkcji do geostatystycznej analizy danych, w tym tworzenia semiwariogramów (funkcja variogram()), modelowania semiwariogramów (funkcje vgm() oraz fit.variogram()) oraz przeprowadzenia krigingu oraz symulacji.\nDo wykonania interpolacji jedną z metod krigingu zostanie wykorzystana funkcja gstat() z pakietu gstat. Funkcja gstat() tworzy obiekt klasy gstat, który zawiera wszystkie informacje potrzebne do wykonania krigingu. Funkcja ta wymaga zdefiniowania następujących argumentów:\n\nformula - zmienna dla której będzie wykonywana estymacja (w przykładzie jest to temp~1)\nlocations - dane wejściowe zawierające zmierzone wartości zmiennej (w przykładzie punkty)\nmodel - parametrów modelu dopasowanego do semiwariogramu empirycznego (wynik działania funkcji vgm() lub fit.variogram())\n\nDodatkowo w zależności od wybranej metody należy zdefniować argument:\n\nbeta - średnia wartość stosowana przy krigingu prostym\nnmax - liczba najbliższych punktów uwzględnianych w krigingu zwykłym\nmaxdist - maksymalna odległość, z której mają być uwzględniane punkty w krigingu zwykłym.\n\n\n\n12.2.2 Estymacja dla wybranych lokalizacji\nEstymacja dla wybranych lokalizacji punktowych wymaga:\n\nstworzenia obiektu klasy gstat wykorzystując funkcje gstat() z parametrami krigingu;\nwykonania predykcji dla lokalizacji zapisanych w zbiorze danych punktowych wykorzystując funkcję predict().\n\n\n\n12.2.3 Estymacja w siatce\nEstymacja w siatce wymaga:\n\nstworzenia siatki wykorzystując funkcję rast() z pakietu terra (utworzony zostanie obiekt klasy SpatRaster);\nstworzenia obiektu klasy gstat wykorzystując funkcję gstat() z parametrami krigingu;\nwykonanie interpolacji w siatce wykorzystując funkcję interpolate() z pakietu terra.\n\nw funkcji interpolate() trzeba podać 3 argumenty: siatkę, obiekt klasy gstat z parametrami krigingu oraz funkcję pozwalająca na wykonanie interpolacji w oparciu obiekt gstat (funkcja interpolate_gstat()). Należy pamiętać, aby każdorazowo wykonując kriging wczytać poniższą funkcję.\n\n\n\ninterpolate_gstat = function(model, x, ...) {\n  v = st_as_sf(x, coords = c(\"x\", \"y\"), crs = st_crs(model$data[[1]]$data))\n  p = predict(model, v, ...)\n  st_drop_geometry(p)\n}",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geostatystyczne metody estymacji</span>"
    ]
  },
  {
    "objectID": "16_pola_estymacja.html#przykład-1-estymacja-dla-wybranych-lokalizacji",
    "href": "16_pola_estymacja.html#przykład-1-estymacja-dla-wybranych-lokalizacji",
    "title": "12  Geostatystyczne metody estymacji",
    "section": "12.3 Przykład 1: Estymacja dla wybranych lokalizacji",
    "text": "12.3 Przykład 1: Estymacja dla wybranych lokalizacji\nW poniższym przykładzie zastosujemy kriging prosty (ang. simple kriging) do oszacowania wartości w wybranych lokalizacjach.\n\n12.3.1 Dane\n\nlibrary(sf)\ngranica = read_sf(\"data/granica.gpkg\")\npunkty = read.csv(\"data/punkty.csv\")\npunkty = st_as_sf(punkty, coords = c(\"x\", \"y\"), crs = \"EPSG:2180\")\npunkty = punkty[!is.na(punkty$temp), ]\n\nDo wygenerowania nowych lokalizacji użyjemy funkcji st_sample() z pakietu sf. Funkcja ta wymaga podania zasięgu obszaru, w którym mają być wygenerowane punkty, liczby punktów oraz typu próbkowania. W poniższym przykładzie wygenerowano lokalizacje 100 punktów losowo rozmieszczonych w granicy analizowanego obszaru.\n\n#utworzenie zbioru punktowego z nowymi lokalizacjami, w których ma być wykonana estymacja. \nset.seed(12)\nnowe_punkty = st_sample(granica, 100, type = \"random\")\n\n\nlibrary(tmap)\ntm_shape(granica) + \n  tm_polygons(col = \"grey\") + \ntm_shape(punkty) + \n  tm_dots(fill = \"darkgreen\", size = 0.7) + \ntm_shape(nowe_punkty) + \n  tm_dots(fill = \"black\", size = 0.7)\n\n\n\n\nLokalizacja punktów pomiarowych (zielony) oraz punktów o nieznanych wartościach zmiennej, dla których będzie wykonana estymacja (czarny)\n\n\n\n\n\n\n12.3.2 Analiza i modelowanie semiwariogramu\nEstymacja z wykorzystaniem jednej z metod krigingu musi być poprzedzona analizą oraz modelowaniem semiwariogramu. Estymacja wymaga wykonania najpierw kilku kroków:\n1. Stworzenie i wyświetlenie semiwariogram empiryczny analizowanej zmiennej z użyciem funkcji variogram() oraz plot().\n\nlibrary(gstat)\nv = variogram(temp~1, punkty, cutoff = 5000)\nplot(v)\n\n\n\n\n\n\n\n\n2. Zdefiniowanie wejściowych parametrów semiwariogramu używając funkcji vgm() (typ modelu, nugget, wariancja progowa, zasięg).\nW poniższym przykładzie po stworzeniu semiwariogramu empirycznego, dopasowano model semiwariogramu składający się z funkcji sferycznej o zasięgu 4000 metrów i wartości nuggetu równej 0,5.\n\nmodel_sph = vgm(psill = 10, model = \"Sph\", range = 4000, nugget =0.5)\nplot(v, model_sph)\n\n\n\n\n\n\n\n\n3. Dopasowanie parametrów modelu używając funkcji fit.variogram().\n\nmodel = fit.variogram(v, model_sph)\nplot(v, model = model)\n\n\n\n\n\n\n\n\n\n\n12.3.3 Estymacja\nW przykładzie wykorzystamy kriging prosty (simple kriging), który zakłada, że średnia jest znana i stała na całym obszarze. Jako średnią podamy średnią wartość zmiennej temp.\n\nmean(punkty$temp)\n\n[1] 15.27321\n\n\nW pierwszym etapie zostanie utworzony obiekt klasy gstat zawierający parametry krigingu. Obiekt klasy gstat składa się z kilku list zawierających:\n\nformułę (w przykładzie temp~1),\nramkę danych z danymi wejściowymi zawierającymi wartości estymowanej zmiennej (w przykładzie obiekt punkty),\nlistę zawierającą parametry modelu (obiekt model będący wynikiem działania funkcji fit.variogram()),\ninne parametry krigingu (np. parametr beta określający średnią wartość zmiennej w analizowanym obszarze).\n\n\nsk_param = gstat(formula = temp ~ 1, \n            locations = punkty,\n            model = model,\n            beta = 15)\n\nPo wyświetleniu obiektu sk_param otrzymamy następujące informacje:\n\nformuła\nwielkość zbioru danych wejściowych (liczba kolumn i wierszy w zbiorze danych punkty)\nparametry modelu\n\n\nsk_param\n\ndata:\nvar1 : formula = temp`~`1 ; data dim = 247 x 5 beta = 15\nvariograms:\n        model      psill    range\nvar1[1]   Nug  0.6557973    0.000\nvar1[2]   Sph 15.1551913 6188.152\n\n\nKolejnym krokiem jest wykonanie predykcji wykorzystując funkcję predict(). Funkcja predict() wymaga zdefiniowania dwóch argumentów: obiektu klasy gstat zawierającego parametry krigingu (w przykładzie obiekt sk_param) oraz nazwy zbioru danych zawierającego nowe lokalizacje, dla których ma być wykonana estymacja (w przykładzie obiekt nowe punkty).\n\nsk_punkty = predict(sk_param, nowe_punkty)\n\n[using simple kriging]\n\n\nWynik krigingu prostego (oraz innych metod krigingu dostępnych w pakiecie gstat), można podejrzeć wpisując nazwę wynikowego obiektu. Szczególnie ważne są dwie, nowe zmienne:\n\nvar1.pred - to wartość estymowana dla każdego punktu (lub oczka siatki)\nvar1.var - informuje o wariancji estymacji.\n\n\nsk_punkty\n\nSimple feature collection with 100 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 745637 ymin: 712676.2 xmax: 755590.9 ymax: 720205.4\nProjected CRS: ETRF2000-PL / CS92\nFirst 10 features:\n   var1.pred var1.var                  geometry\n1   14.33853 2.016268 POINT (754903.3 718651.9)\n2   13.02704 3.059513 POINT (748625.5 716346.2)\n3   17.50632 2.001131 POINT (747480.4 714803.1)\n4   10.88570 1.137811 POINT (747588.4 717210.4)\n5   19.12422 2.593422 POINT (752887.2 713523.5)\n6   15.94537 2.135047 POINT (745803.6 715810.5)\n7   15.75018 3.389682   POINT (745637 716264.8)\n8   13.27067 2.204342 POINT (750037.2 717080.5)\n9   13.76263 3.044975 POINT (754858.7 720205.4)\n10  13.48186 1.332920 POINT (749901.1 716873.7)\n\n\nObie uzyskane zmienne można wyświetlić z użyciem pakietu tmap.\n\nlibrary(tmap)\n\ntm_shape(sk_punkty) +\n        tm_dots(col = c(\"var1.pred\", \"var1.var\"),\n                size = 1, \n                style = \"cont\", \n                palette = list(\"-Spectral\")) +\n        tm_layout(legend.frame = TRUE)",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geostatystyczne metody estymacji</span>"
    ]
  },
  {
    "objectID": "16_pola_estymacja.html#przykład-2-estymacja-w-siatce",
    "href": "16_pola_estymacja.html#przykład-2-estymacja-w-siatce",
    "title": "12  Geostatystyczne metody estymacji",
    "section": "12.4 Przykład 2: Estymacja w siatce",
    "text": "12.4 Przykład 2: Estymacja w siatce\n\n12.4.1 Dane\n\nlibrary(terra)\n#punkty przekształcone do warstwy geoprzestrzennej typu sf\npunkty = read.csv(\"data/punkty.csv\")\npunkty = st_as_sf(punkty, coords = c(\"x\", \"y\"), crs = \"EPSG:2180\", remove = FALSE)\npunkty = punkty[!is.na(punkty$temp), ]\n\n#granica obszaru wczytana jako obiekt klasy SpatVector z pakietu terra \ngranica = vect(\"data/granica.gpkg\")\n\n\n\n12.4.2 Tworzenie siatki\nDo stworzenia siatki można wykorzystać funkcję rast() z pakietu terra, która tworzy nowy obiekt rastrowy klasy SpatRaster o zdefiniowanym zasięgu oraz rozdzielczości. W poniższym przykładzie utworzono siatkę dla zasięgu zdefiniowanego przez obwiednię (zasięg) warstwy wektorowej granica oraz o rozdzielczości 50m.\n\nsiatka = rast(ext = granica, res = 50, crs = crs(granica))\nsiatka\n\nclass       : SpatRaster \nsize        : 172, 229, 1  (nrow, ncol, nlyr)\nresolution  : 50, 50  (x, y)\nextent      : 745541.7, 756991.7, 712651.6, 721251.6  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \n\n\n\n\n12.4.3 Analiza i modelowanie semiwariogramu\nWykonanie krigingu wymaga kilku kroków, w których zostanie stworzony semiwariogram empiryczny oraz dopasowany do niego model:\n\nv = variogram(temp~1, punkty, cutoff = 5000)\nmodel_sph = vgm(psill = 10, model = \"Sph\", range = 4000, nugget =0.5)\nmodel = fit.variogram(v, model_sph)\n\n\n\n12.4.4 Estymacja: Kriging prosty\nWykonanie interpolacji w siatce wymaga stworzenia obiektu klasy gstat z parametrami krigingu. Obiekt ten tworzony jest wykorzystując funkcję gstat().\n\nsk_param = gstat(formula = temp ~ 1, \n            locations = punkty,\n            model = model,\n            beta = 15)\n\n\nsk_param\n\ndata:\nvar1 : formula = temp`~`1 ; data dim = 247 x 7 beta = 15\nvariograms:\n        model      psill    range\nvar1[1]   Nug  0.6557973    0.000\nvar1[2]   Sph 15.1551913 6188.152\n\n\nW drugim kroku należy wykonać interpolację w siatce wykorzystując funkcję interpolate(). Funkcja interpolate() wymaga podania 3 argumentów:\n\nobiekt siatki (w przykładzie siatka)\nobiekt klasy gstat z parametrami krigingu (w przykładzie sk_param)\nfunkcję pozwalająca na wykonanie interpolacji w oparciu o obiekt klasy gstat (funkcja interpolate_gstat())\n\n\ninterpolate_gstat = function(model, x, ...) {\n  v = st_as_sf(x, coords = c(\"x\", \"y\"), crs = st_crs(model$data[[1]]$data))\n  p = predict(model, v, ...)\n  st_drop_geometry(p)\n}\n\n\nsk = interpolate(siatka, sk_param, fun = interpolate_gstat)\n\n[using simple kriging]\n[using simple kriging]\n\n\nWyniki estymacji można wyświetlić podając nazwę wynikowego obiektu.\n\nsk\n\nclass       : SpatRaster \nsize        : 172, 229, 2  (nrow, ncol, nlyr)\nresolution  : 50, 50  (x, y)\nextent      : 745541.7, 756991.7, 712651.6, 721251.6  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \nsource(s)   : memory\nnames       : var1.pred, var1.var \nmin values  :  8.369086, 0.966972 \nmax values  : 24.305490, 9.434752 \n\n\nDo wyświetlenia wyników krigingu można uzyć funkcji podstawowej plot(). Funkcja mask() przycina obszar tylko do granicy obszaru analizy.\n\nplot(mask(sk, granica))\n\n\n\n\n\n\n\n\nDo wizualizacji wyników estymacji można wykorzystać także pakiet tmap. Obiekt zawierający wyniki estymacji można wyświetlić używając funkcji tm_shape() and tm_raster().\n\ntm_shape(sk) +\n  tm_raster(col = c(\"var1.pred\"), style = \"cont\", palette = \"-Spectral\")\n\n\n\n\n\n\n\n\n\ntm_shape(sk) +\n tm_raster(col = c(\"var1.var\"), style = \"cont\", palette = \"viridis\") \n\n\n\n\n\n\n\n\nZwróć uwagę, że siatka została utworzona używając zasięgu (bounding box) warstwy wektorowej granica. Obiekt można przyciąć do granic obszaru używając funkcji crop() z pakietu terra.\n\nsk_crop = crop(sk, granica, mask=TRUE)\n\n\ntm_shape(sk_crop) +\n  tm_raster(col = c(\"var1.pred\"), style = \"cont\", palette = \"-Spectral\")\n\n\n\n\n\n\n\n\n\n\n12.4.5 Kriging zwykły\nW krigingu zwykłym średnia traktowana jest jako wartość nieznana. Metoda ta uwzględnia lokalne fluktuacje średniej poprzez stosowanie ruchomego okna. Parametry ruchomego okna można określić za pomocą jednego z dwóch argumentów:\n\nnmax - użyta zostanie określona liczba najbliższych obserwacji.\nmaxdist - użyte zostaną jedynie obserwacje w zadanej odległości.\n\nTak jak w przypadku krigingu prostego wykonanie krigingu zwykłego składa się z dwóch etapów: stworzenia obiektu klasy gstat zawierającego parametry krigingu oraz wykonanie interpolacji wykorzystując funkcję interpolate(). W poniższym przykładzie do obliczenia średniej w ruchomym oknie zostanie wykorzystanych 30 najbliższych punktów (argument nmax = 30).\n\nok_param = gstat(formula = temp ~ 1, \n            locations = punkty,\n            model = model,\n            nmax = 30)\n\n\nok = interpolate(siatka, ok_param, fun = interpolate_gstat)\n\n[using ordinary kriging]\n[using ordinary kriging]\n\n\n\nok_crop = crop(ok, granica, mask=TRUE)\n\nPodobnie jak w przypadku krigingu prostego, można przyjrzeć się wynikom estymacji podając nazwę wynikowego obiektu oraz wyświetlić je używając funkcji tm_shape() and tm_raster().\n\ntm_shape(ok_crop) +\n  tm_raster(col = c(\"var1.pred\"), style = \"cont\", palette = \"-Spectral\") \n\n\n\n\n\n\n\n\n\ntm_shape(ok_crop) +\n  tm_raster(col = c(\"var1.var\"), style = \"cont\", palette = \"viridis\") \n\n\n\n\n\n\n\n\n\nWykonaj kriging zwykły wykorzystując jedynie punkty położone w odległości do 1500m (podpowiedź: tworząc obiekt klasy gstat należy wykorzystać parametr maxdist, zamiast nmax). Dotnij wynik do granicy obszaru. Zwizualizuj wyniki.\n\n\n\n12.4.6 Kriging z trendem\nKriging z trendem, określany również jako kriging z wewnętrznym trendem, do estymacji wykorzystuje (oprócz zmienności wartości wraz z odległością) położenie analizowanych punktów.\n\n12.4.6.1 Przygotowanie danych\nPrzy wykonaniu krigingu z trendem istotne są także współrzędne punktów. Wykorzystamy w tym celu obiekt punkty_df, który zawiera wczytane z pliku tekstowego dane.\n\npunkty_df = read.csv(\"data/punkty.csv\")\npunkty_df = punkty_df[!is.na(punkty_df$temp), ]\nhead(punkty_df)\n\n      srtm clc      temp      ndvi      savi        x        y\n1 175.7430   1 13.852222 0.6158061 0.4189449 750298.0 716731.6\n2 149.8111   1 15.484209 0.5558816 0.3794864 753482.9 717331.4\n3 272.8583  NA 12.760814 0.6067462 0.3745572 747242.5 720589.0\n4 187.2777   1 14.324648 0.3756170 0.2386246 755798.9 718828.1\n5 260.1366   1 15.908549 0.4598393 0.3087599 746963.5 717533.5\n6 160.1416   2  9.941118 0.5600288 0.3453627 756801.6 720474.1\n\n\nDrugim plikiem potrzebnym do wykonania analizy będzie granica obszaru wczytana jako obiekt klasy SpatVect używając biblioteki terra.\n\ngranica = vect(\"data/granica.gpkg\")\n\nWykonanie estymacji metodą krigingu wymaga także utworzenia siatki. Siatkę utworzymy używając funkcji rast() z pakietu terra.\n\nsiatka = rast(ext = granica, res = 50, crs = crs(granica))\n\n\n\n12.4.6.2 Analiza i modelowanie struktury przestrzennej\nW funkcji variogram() pierwszy argument musi przyjąć postać temp ~ x + y, co oznacza, że uwzględniamy liniowy trend zależny od współrzędnej x oraz y. Argument data pozwala na podanie ramki danych zawierającej wartość zmiennej oraz współrzędne, argument locations definiuje, które zmienne zawierają współrzędne punktów.\n\nlibrary(gstat)\nvario_kzt = variogram(temp ~ x + y, data = punkty_df, locations = ~x + y)\n\nNastępnym etapem jest dopasowanie modelu do semiwariogramu empirycznego.\n\nfitted_kzt = fit.variogram(vario_kzt, vgm(model = \"Sph\", nugget = 1))\nfitted_kzt\n\n  model     psill  range\n1   Nug 0.2822454    0.0\n2   Sph 6.9271803 2193.7\n\n\nKolejnym etapem jest wykonanie estymacji. W tym celu należy utworzyć obiekt klasy gstat zawierający parametry krigingu oraz wykonać interpolacji wykorzystując funkcję interpolate(). Należy tutaj pamiętać, aby formuła (w przykładzie temp ~ x + y) był taki sam podczas budowania semiwariogramu, jak i estymacji.\n\nkt_param = gstat(formula = temp ~ x + y, \n                 data = punkty_df,\n                 locations = ~ x+ y,\n                 model = fitted_kzt)\n\n\nkt = interpolate(siatka, kt_param)\n\n[using universal kriging]\n[using universal kriging]\n\n\n\nkt\n\nclass       : SpatRaster \nsize        : 172, 229, 2  (nrow, ncol, nlyr)\nresolution  : 50, 50  (x, y)\nextent      : 745541.7, 756991.7, 712651.6, 721251.6  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \nsource(s)   : memory\nnames       : var1.pred,  var1.var \nmin values  :   8.15150, 0.5110511 \nmax values  :  24.55763, 8.4636071 \n\n\nDo wyświetlenia wyników krigingu można także uzyć funkcji podstawowej plot(). Funkcja mask() przycina obszar tylko do granicy obszaru analizy.\n\nplot(mask(kt, granica))",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Geostatystyczne metody estymacji</span>"
    ]
  },
  {
    "objectID": "17_pola_ocena.html",
    "href": "17_pola_ocena.html",
    "title": "13  Ocena jakości estymacji",
    "section": "",
    "text": "13.1 Walidacja podzbiorem\nNa jakość estymacji wykonywanych metodą krigingu wpływa dopasowanie modelu oraz wybór metody estymacji. W celu wybrania najbardziej optymalnego modelu do analizy lub porównania wyników estymacji otrzymanych różnymi metodami (np. kriging prosty vs. kriging zwykły) stosuje się jedną z dwóch metod walidacji wyników estymacji:\nObie metody pozwalają na uzyskanie dla wybranych lokalizacji dwóch wartości analizowanej zmiennej: wartości obserwowanej oraz wartości estymowanej. Porównując wartość oberwowaną oraz estymowaną możemy obliczyć błąd estymacji. Błąd estymacji jest najprostszą formą oceny jakości estymacji obliczaną jako różnica między wartością obserwowaną oraz wartością estymacji. Błąd estymacji można analizować za pomocą mapy lub wykresów.\nDo porównania wartości obserowanych oraz estymowanych można także wykorzystać szereg statystyk jakości estymacji. Do podstawowych statystyk ocen jakości estymacji należą:\nWalidacja podzbiorem polega na podziale zbioru danych na dwa podzbiory:\nZaletą tego podejścia jest stosowanie danych niezależnych od estymacji do oceny jakości modelu. Wadą natomiast jest konieczność posiadania (relatywnie) dużego zbioru danych.\nWalidacja podzbiorem składa się z kilku kroków:",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ocena jakości estymacji</span>"
    ]
  },
  {
    "objectID": "17_pola_ocena.html#walidacja-podzbiorem",
    "href": "17_pola_ocena.html#walidacja-podzbiorem",
    "title": "13  Ocena jakości estymacji",
    "section": "",
    "text": "treningowy - wykorzystywany do stworzenia semiwariogramu empirycznego, zbudowania modelu (zawiera więcej obserwacji)\ntestowy - estymację wykonuje się dla punktów ze zbioru testowego, a następnie porównuje się wynik estymowany oraz rzeczywisty (zawiera mniej obserwacji).\n\n\n\n\nPodział zbioru danych na zbiór treningowy oraz testowy.\nStworzenie semiwariogramu oraz dopasowanie modelu na podstawie punktów ze zbioru treningowego.\nWykonanie estymacji dla punktów ze zbioru testowego.\n\n\n13.1.1 Podział zbioru danych na zbiór treningowy oraz testowy.\n\nlibrary(sf)\n\nLinking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE\n\npunkty = read.csv(\"data/punkty.csv\")\npunkty = st_as_sf(punkty, coords = c(\"x\", \"y\"), crs = \"EPSG:2180\")\npunkty = na.omit(punkty)\n\nNa poniższym przykładzie zbiór danych dzielony jest używając funkcji initial_split() z pakietu rsample. Zap pomocą tej funkcji zostanie utworzony obiekt zawierający wydzielony podzbiór treningowy i testowy. Ważną zaletą funkcji initial_split() jest to, iż w zbiorze treningowym i testowym zachowane są podobne rozkłady wartości. W przykładzie użyto argumentu prop = 0.75, który oznacza, że 75% danych będzie należało do zbioru treningowego, a 25% do zbioru testowego. Następnie, korzystając ze stworzonego obiektu, budowane są dwa zbiory danych - treningowy (train) oraz testowy (test).\n\nlibrary(rsample)\nset.seed(224)\npunkty_podzial = initial_split(punkty, prop = 0.75, strata = temp)\ntrain = training(punkty_podzial)\ntest = testing(punkty_podzial)\n\nNa poniższej mapie kolorem niebieskim zaznaczono 75% obserwacji tworzących zbiór treningowy, a kolorem czerwonym 25% obserwacji tworzących zbiór testowy.\n\nlibrary(tmap)\ntm_shape(train) + \n  tm_symbols(fill = \"blue\") +\ntm_shape(test) + \n  tm_symbols(fill = \"red\") +\ntm_layout(legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n13.1.2 Stworzenie semiwariogramu oraz dopasowanie modelu na podstawie punktów ze zbioru treningowego.\nW kolejnym kroku zbiór treningowy (train) zostanie wykorzystany do utworzenia semiwariogramu empirycznego oraz dopasowania modelu. Semiwariogram empiryczny tworzony jest z wykorzystaniem funkcji variogram() z pakietu gstat. Do określenia typu modelu oraz jego podstawowych parametrów używa się funkcji vgm() z pakietu gstat. Funkcja fit.variogram() pozwala na automatyczne dopasowanie modelu w oparciu o wstępnie podane parametry.\n\nlibrary(gstat)\nvario = variogram(temp ~ 1, data = train)\nmodel = vgm(10, model = \"Sph\", range = 4000, nugget = 0.5)\nfitted = fit.variogram(vario, model)\nplot(vario, fitted)\n\n\n\n\n\n\n\n\n\n\n13.1.3 Wykonanie estymacji dla punktów ze zbioru testowego.\nKolejnym krokiem jest wykonanie estymacji dla punktów ze zbioru testowego (test) używając modelu zbudowanego na podstawie punktów ze zbioru treningowego. W przykładzie wykorzystamy metodę krigingu zwykłego (oridinary kriging).\nW pierwszym etapie zostanie utworzony obiekt klasy gstat zawierający parametry krigingu. Jako argument locations podajemy zbiór treningowy (train) - tj. zbiór danych na podstawie, którego tworzony był model.\n\nlibrary(gstat)\nok_param = gstat(formula = temp ~ 1, \n            locations = train,\n            model = fitted,\n            nmax = 30)\n\nDo wykonania predykcji dla punktów ze zbioru testowego wykorzystuje się funkcję predict(). Funkcja predict() wymaga zdefiniowania dwóch argumentów: obiektu klasy gstat zawierającego parametry krigingu (w przykładzie obiekt ok_param) oraz nazwy zbioru danych zawierającego nowe lokalizacje, dla których ma być wykonana estymacja (w przykładzie obiekt test zawierający lokalizacje punktów w zbiorze testowym).\n\nok_test = predict(ok_param, test)\n\n[using ordinary kriging]\n\n\nObiekt ok_test zawiera wyniki estymacji dla punktów ze zbioru testowego.\n\nok_test\n\nSimple feature collection with 62 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 746342.4 ymin: 712723.3 xmax: 756869.5 ymax: 720852.7\nProjected CRS: ETRF2000-PL / CS92\nFirst 10 features:\n   var1.pred var1.var                  geometry\n1   11.59223 4.134905 POINT (756801.6 720474.1)\n2   13.92553 2.497318 POINT (749399.8 716955.2)\n3   16.24785 2.203199   POINT (748766.6 713889)\n4   15.75189 3.357429 POINT (749696.7 715940.2)\n5   15.08617 2.831779 POINT (748974.3 720650.5)\n6   22.07659 2.300183 POINT (746703.3 713611.8)\n7   18.85014 2.477391 POINT (755052.4 716591.9)\n8   14.31477 2.074113 POINT (750158.1 714389.2)\n9   17.12052 1.922857 POINT (752022.7 715886.1)\n10  13.10026 4.518624 POINT (756869.5 720852.7)\n\n\n\n\n13.1.4 Błąd estymacji\nW celu obliczenia błędów estymacji do obiektu ok_test dodamy wartość obserwowaną temperatury.\n\nok_test$temp_obs = test$temp\nhead(ok_test)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 746703.3 ymin: 713611.8 xmax: 756801.6 ymax: 720650.5\nProjected CRS: ETRF2000-PL / CS92\n  var1.pred var1.var                  geometry  temp_obs\n1  11.59223 4.134905 POINT (756801.6 720474.1)  9.941118\n2  13.92553 2.497318 POINT (749399.8 716955.2) 12.919225\n3  16.24785 2.203199   POINT (748766.6 713889) 19.247132\n4  15.75189 3.357429 POINT (749696.7 715940.2) 14.522082\n5  15.08617 2.831779 POINT (748974.3 720650.5) 14.707823\n6  22.07659 2.300183 POINT (746703.3 713611.8) 23.677025\n\n\nBłąd estymacji obliczamy poprzez odjęcie od wartości obserwowanej (zmienna temp_obs) wartości estymowanej (zmienna var1.pred w obiekcie ok_test).\n\nok_test$blad_est_ok = ok_test$temp_obs - ok_test$var1.pred\n\nWartości błędu estymacji można zwizualizować na mapie lub na wykresach.\n\n13.1.4.1 Rozkład przestrzenny błędu estymacji\n\ncuts = c(-5, -3, -1, 1, 3, 5)\ntm_shape(ok_test) +\n        tm_symbols(col = \"blad_est_ok\", breaks = cuts, title.col = \"\", palette = \"-RdBu\", size = 1.5) +\n        tm_layout(main.title = \"Błąd estymacji\", legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\nCo oznacza ujemny, a co dodatni błąd estymacji?\n\n\n\n13.1.4.2 Histogram: rozkład wartości błędu estymacji\n\nlibrary(ggplot2)\nggplot(ok_test, aes(blad_est_ok)) +\n    geom_histogram() + \n    xlab(\"Błąd estymacji\") + \n    ylab(\"Liczebność\") +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n13.1.4.3 Wykres rozrzutu wartości obserwowanych oraz estymowanych\n\nggplot(ok_test, aes(var1.pred, temp_obs)) +\n    geom_point(size = 1.5) +\n    xlab(\"Estymacja\") +\n    ylab(\"Obserwacja\") + \n  xlim(5, 25) + ylim(5,25) + \n  coord_fixed() + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n13.1.5 Statystyki jakości estymacji\n\n13.1.5.1 Średni błąd estymacji\nŚredni błąd estymacji (MPE) można wyliczyć korzystając z poniższego wzoru:\n\\[ MPE=\\frac{\\sum_{i=1}^{n}(v_i - \\hat{v}_i)}{n} \\]\ngdzie \\(v_i\\) to wartość obserwowana a \\(\\hat{v}_i\\) to wartość estymowana.\nOptymalnie wartość średniego błędu estymacji powinna być jak najbliżej 0.\nW R do obliczenia średniego błędu estymacji służy funkcja mean()\n\nMPE = mean(ok_test$blad_est_ok)\nMPE\n\n[1] -0.01898768\n\n\n\n\n13.1.5.2 Pierwiastek błędu średniokwadratowego\nPierwiastek błędu średniokwadratowego (RMSE) jest możliwy do wyliczenia poprzez wzór:\n\\[ RMSE=\\sqrt{\\frac{\\sum_{i=1}^{n}(v_i-\\hat{v}_i)^2}{n}} \\]\ngdzie \\(v_i\\) to wartość obserwowana a \\(\\hat{v}_i\\) to wartość estymowana.\nOptymalnie wartość pierwiastka błędu średniokwadratowego powinna być jak najmniejsza.\n\nRMSE = sqrt(mean((ok_test$temp_obs - ok_test$var1.pred) ^ 2))\nRMSE\n\n[1] 1.434394\n\n\n\n\n13.1.5.3 Współczynnik determinacji\nWspółczynnik determinacji (R2) jest możliwy do wyliczenia poprzez wzór:\n\\[ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (\\hat v_i - v_i)^2}{\\sum_{i=1}^{n} (v_i - \\overline{v_i})^2} \\]\ngdzie \\(v_i\\) to wartość obserwowana, \\(\\hat{v}_i\\) to wartość estymowana, a \\(\\overline{v}\\) średnia arytmetyczna wartości obserwowanych.\nWspółczynnik determinacji przyjmuje wartości od 0 do 1, gdzie model jest lepszy im wartość tego współczynnika jest bliższa jedności.\nW współczynnik determinacji możemy obliczyć obliczenie współczynnika korelacji liniowej Pearsona używając funkcji cor() oraz podsienienie wyniku do kwadratu.\n\nR2 = cor(ok_test$temp_obs, ok_test$var1.pred) ^ 2\nR2\n\n[1] 0.8938915\n\n\n\n\n\n13.1.6 Estymacja w siatce\nW sytuacji, gdy uzyskany model jest wystarczająco dobry, możemy również uzyskać estymację dla całego obszaru z użyciem funkcji interpolate().\n\nlibrary(terra)\n\nterra 1.8.60\n\ngranica = vect(\"data/granica.gpkg\")\nsiatka = rast(ext = granica, res = 50, crs = crs(granica))\nsiatka\n\nclass       : SpatRaster \nsize        : 172, 229, 1  (nrow, ncol, nlyr)\nresolution  : 50, 50  (x, y)\nextent      : 745541.7, 756991.7, 712651.6, 721251.6  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \n\n\n\ninterpolate_gstat = function(model, x, ...) {\n  v = st_as_sf(x, coords = c(\"x\", \"y\"), crs = st_crs(model$data[[1]]$data))\n  p = predict(model, v, ...)\n  st_drop_geometry(p)\n}\n\n\nok = interpolate(siatka, ok_param, fun = interpolate_gstat)\n\n[using ordinary kriging]\n[using ordinary kriging]\n\n\n\nok_crop = crop(ok, granica, mask=TRUE)\n\n\ntm_shape(ok_crop) +\n  tm_raster(col = c(\"var1.pred\"), style = \"cont\", palette = \"-Spectral\")",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ocena jakości estymacji</span>"
    ]
  },
  {
    "objectID": "17_pola_ocena.html#kroswalidacja",
    "href": "17_pola_ocena.html#kroswalidacja",
    "title": "13  Ocena jakości estymacji",
    "section": "13.2 Kroswalidacja",
    "text": "13.2 Kroswalidacja\nW przypadku kroswalidacji te same dane wykorzystywane są do budowy modelu, estymacji, a następnie do oceny prognozy. Procedura kroswalidacji LOO (ang. leave-one-out cross-validation) składa się z poniższych kroków:\n1 Zbudowanie matematycznego modelu z dostępnych obserwacji. 2. Dla każdej znanej obserwacji następuje:\n\nUsunięcie jej ze zbioru danych.\nUżycie modelu do wykonania estymacji w miejscu tej obserwacji.\nWyliczenie reszty (ang. residual), czyli różnicy pomiędzy znaną wartością a estymacją.\n\n\nPodsumowanie otrzymanych wyników.\n\nW pierwszej kolejności używając wszystkich danych budowany jest semiwariogram empiryczny oraz dopasowywany jest model.\n\nvario = variogram(temp ~ 1, data = punkty)\nmodel = vgm(model = \"Sph\", nugget = 0.5)\nfitted = fit.variogram(vario, model)\nplot(vario, model = fitted)\n\n\n\n\n\n\n\n\nDo wykonania kroswalidacji w R służy funkcja krige.cv() z pakietu gstat.\n\ncv_ok = krige.cv(temp ~ 1,\n                 locations = punkty,\n                 model = fitted,\n                 nmax = 30)\n\nW wyniku krowalidacji otrzymujemy obiekt zawierający wartości estymacji (var1.pred), wariancji krigingowej (var1.var), wartości obserwowane (observed), wartość błędu estymacji (residuals).\n\nhead(cv_ok)\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 746963.5 ymin: 716731.6 xmax: 756801.6 ymax: 720474.1\nProjected CRS: ETRF2000-PL / CS92\n  var1.pred var1.var  observed   residual      zscore fold\n1  14.98472 2.663551 13.852222 -1.1325010 -0.69391785    1\n2  14.12990 1.984469 15.484209  1.3543097  0.96138160    2\n3  12.59702 2.291935 14.324648  1.7276257  1.14116524    3\n4  15.40495 1.348087 15.908549  0.5035949  0.43373286    4\n5  11.08843 2.829173  9.941118 -1.1473123 -0.68210610    5\n6  13.35676 2.515963 13.514751  0.1579941  0.09960671    6\n                   geometry\n1   POINT (750298 716731.6)\n2 POINT (753482.9 717331.4)\n3 POINT (755798.9 718828.1)\n4 POINT (746963.5 717533.5)\n5 POINT (756801.6 720474.1)\n6   POINT (752698 718623.6)\n\n\nPodstawowe statystyki dla wartości obserwowanych, estymowanych oraz błędu estymacji.\n\nsummary(cv_ok)\n\n   var1.pred         var1.var        observed         residual        \n Min.   : 8.897   Min.   :1.154   Min.   : 7.883   Min.   :-5.097073  \n 1st Qu.:12.299   1st Qu.:1.812   1st Qu.:11.953   1st Qu.:-0.962364  \n Median :14.737   Median :2.183   Median :14.937   Median : 0.009709  \n Mean   :15.259   Mean   :2.259   Mean   :15.223   Mean   :-0.036638  \n 3rd Qu.:17.463   3rd Qu.:2.582   3rd Qu.:17.584   3rd Qu.: 0.861203  \n Max.   :24.213   Max.   :5.656   Max.   :24.945   Max.   : 4.388040  \n     zscore               fold                 geometry  \n Min.   :-3.559661   Min.   :  1.00   POINT        :242  \n 1st Qu.:-0.647834   1st Qu.: 61.25   epsg:2180    :  0  \n Median : 0.005204   Median :121.50   +proj=tmer...:  0  \n Mean   :-0.012242   Mean   :121.50                      \n 3rd Qu.: 0.599458   3rd Qu.:181.75                      \n Max.   : 3.129498   Max.   :242.00                      \n\n\n\n13.2.1 Statystyki jakości estymacji\nWykorzystując wyniki kroswalidacji można obliczyć statystyki jakości estymacji.\n\nśredni błąd kwadratowy\n\n\nMPE = mean(cv_ok$residual)\nMPE\n\n[1] -0.03663776\n\n\n\npierwiastek średniego błędu kwadratowego\n\n\nRMSE = sqrt(mean((cv_ok$residual) ^ 2))\nRMSE\n\n[1] 1.418484\n\n\n\nwspółczynnik determinacji\n\n\nR2 = cor(cv_ok$observed, cv_ok$var1.pred) ^ 2\nR2\n\n[1] 0.8709762\n\n\n\n\n13.2.2 Rozkład przestrzenny błędu estymacji\nMożna także przeanalizować rozkład przestrzenny błędu estymacji.\n\ncuts = c(-5, -3, -1, 1, 3, 5)\ntm_shape(cv_ok) +\n        tm_symbols(col = \"residual\", breaks = cuts, title.col = \"\", palette = \"-RdBu\", size = 1) +\n        tm_layout(main.title = \"Błąd estymacji\", legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n13.2.3 Rozkład wartości błędu estymacji\n\nlibrary(ggplot2)\nggplot(cv_ok, aes(residual)) +\n    geom_histogram() + \n    xlab(\"Błąd estymacji\") + \n    ylab(\"Liczebność\") +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\nJaki rozkład ma błąd estymacji?\n\n\n\n13.2.4 Wykres rozrzutu\n\nggplot(cv_ok, aes(var1.pred, observed)) +\n    geom_point() +\n    xlab(\"Estymacja\") +\n    ylab(\"Obserwacja\") + \n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n13.2.5 Estymacja w siatce\nPodobnie jak w walidacji podzbiorem, gdy uzyskany model jest wystarczająco dobry, estymację dla całego obszaru uzyskuje się z użyciem funkcji interpolate()\n\nlibrary(terra)\ngranica = vect(\"data/granica.gpkg\")\nsiatka = rast(ext = granica, res = 50, crs = crs(granica))\nsiatka\n\nclass       : SpatRaster \nsize        : 172, 229, 1  (nrow, ncol, nlyr)\nresolution  : 50, 50  (x, y)\nextent      : 745541.7, 756991.7, 712651.6, 721251.6  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \n\n\n\ninterpolate_gstat = function(model, x, ...) {\n  v = st_as_sf(x, coords = c(\"x\", \"y\"), crs = st_crs(model$data[[1]]$data))\n  p = predict(model, v, ...)\n  st_drop_geometry(p)\n}\n\n\nok_param = gstat(formula = temp ~ 1, \n            locations = punkty,\n            model = fitted,\n            nmax = 30)\n\n\nok = interpolate(siatka, ok_param, fun = interpolate_gstat)\n\n[using ordinary kriging]\n[using ordinary kriging]\n\n\n\nok_crop = crop(ok, granica, mask=TRUE)\n\n\ntm_shape(ok_crop) +\n  tm_raster(col = c(\"var1.pred\"), style = \"cont\", palette = \"-Spectral\")",
    "crumbs": [
      "DANE POWIERZCHNIOWE CIĄGŁE PRZESTRZENNIE",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Ocena jakości estymacji</span>"
    ]
  },
  {
    "objectID": "18_zadania_samodzielne.html",
    "href": "18_zadania_samodzielne.html",
    "title": "14  Zadania samodzielne",
    "section": "",
    "text": "14.1 Dane przestrzenne w R\nProszę zapoznać się z treścią rozdziału 3 oraz rozwiązać poniższe zadania.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zadania samodzielne</span>"
    ]
  },
  {
    "objectID": "18_zadania_samodzielne.html#dane-przestrzenne-w-r",
    "href": "18_zadania_samodzielne.html#dane-przestrzenne-w-r",
    "title": "14  Zadania samodzielne",
    "section": "",
    "text": "Zadanie 1\n\nWykorzystując aplikację Google Maps zczytaj współrzędne swojego miejsca zamieszkania (zaokrąglij je do 6 miejsc po przecinku).\nNa podstawie współrzędnych utwórz obiekt wektorowy punktowy i nadaj mu układ współrzędnych WGS84 (EPSG: 4326).\nUtwórz w R obiekt p2 poprzez przekształcenie utworzonego punktu do PUWG1992 (EPSG: 2180).\nWyznacz 1km strefę bufforową wokół wyznaczonego punktu p2.\nZwizualizuj punkt oraz strefę bufforową używając pakietu tmap. Zapisz mapę do pliku png.\n\n\n\nZadanie 2\n\nWczytaj dane z pliku punkty.csv oraz wyselekcjonuj tylko punkty z temperaturą powyżej 20C (obiekt punkty_sel).\nPrzetwórz obiekt punkty_sel do postaci obiektu przestrzennego punkty20.\nWyznacz obwiednię (bounding box) dla obiektu punkty20 i przypisz ją do obiektu punkty20_bb.\nWczytaj dane z pliku clc.tif do R jako obiekt clc.\nUtwórz obiekt clc20 poprzez przycięcie obiektu clc do granic obiektu punkty20_bb. Zapisz obiekt clc20 do pliku.\n\n\nJakiego rodzaju jest wynikowy obiekt?\nJaki typ danych on przechowuje? Jakie ma wymiary?\nIle ma on atrybutów (zmiennych)?\nJaki ma on układ współrzędnych?\n\n\nStwórz mapę pokazującą punkty z obiektu punkty20. W tle tej mapy przedstaw obiekt clc20. Zapisz mapę w pliku png.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zadania samodzielne</span>"
    ]
  },
  {
    "objectID": "18_zadania_samodzielne.html#eksploracyjna-analiza-danych",
    "href": "18_zadania_samodzielne.html#eksploracyjna-analiza-danych",
    "title": "14  Zadania samodzielne",
    "section": "14.2 Eksploracyjna analiza danych",
    "text": "14.2 Eksploracyjna analiza danych\nProszę zapoznać się z treścią rozdziału 4 oraz rozwiązać poniższe zadania.\n\nDane\nW ćwiczeniu zostaną wykorzystane dane dotyczące stopy bezrobocia w powiatach w Polsce w 2023 roku. Dane zostały pobrane z Banku Danych Lokalnych.\n\nPlik bezrobocie_pl.csv zawiera identyfikator powiatu, nazwę powiatu oraz dane dotyczące stopy bezrobocia dla lat 2004, 2010, 2021, 2023 (odpowiednio SB2004, SB2010, SB2021, SB2023).\nPlik powiaty_klasyfikacje.csv zawiera informacje o przynależności powiatów do regionów NUTS3, makroregionów wg klasyfikacji NUTS1 oraz typologi miejsko-wiejskiej. W przypadku typologii miejsko-wiejskiej kod 1 to regiony regiony przeważająco miejskie (80% ludności mieszka w klastrach miejskich), 2 oznacza regiony pośrednie (od 50 % do 80 % ludności mieszka w „klastrach miejskich”), a 3 oznacza regiony przeważająco wiejskie (co najmniej 50 % ludności mieszka w „komórkach siatki obszarów wiejskich”).\nPlik powiaty_pl.gpkg zawiera granice powiatów w Polsce.\nPlik wojewodztwa.gpkg zawiera granice województw.\n\n\n\nPrzygotowanie danych\n\nUtworzyć obiekt bezrobocie na podstawie pliku bezrobocie_pl.csv\nUtwórz obiekt klasyfikacje na podstawie pliku powiaty_klasyfikacje.csv. Zmodyfikuj pole TERYT poprzez dodanie ‘000’ na końcu.\nUtwórz obiekt powiaty_attr poprzez połączenie informacji znajdujących się w obiektach bezrobocie oraz klasyfikacje.\nUtworzyć obiekt powiaty_granica na podstawie pliku powiaty_pl.gpkg. Dodaj pole “Kod”, które będzie miało przypisane ID powiatu w takiej samej postaci w jakiej jest zapisane w kolumnie Kod w obiekcie powiaty_attr.\nUtworzyć obiekt woj_granica na podstawie pliku wojewodztwa.gpkg.\nUtwórz obiekt powiaty poprzez dołączenie danych z obiektu powiaty_attr do obiektu powiaty_granica.\nZapisz obiekt powiaty do geopaczki powiaty_attr.gpkg\n\n\n\nZadanie\nCelem ćwiczenia jest wykonanie eksploracyjnej nieprzestrzennej analizy danych.\n\nOblicz podstawowe statystyki dla zmiennej SB2023. Scharakteryzuj na ich podstawie wartość stopy bezrobocia w 2023 roku.\nJaki jest rozkład zmiennej SB2023? Czy w zbiorze danych istnieją wartości globalnie odstające. Jeśli tak - zidentyfikuj nazwy powiatów, dla których takie wartości występują.\nCzy stopa bezrobocia różni się między makroregionami NUTS1 (zmienna MAKROREGION).\n\n\nOblicz statystyki opisowe (średnią, medianę, wartość minimalmną i maksymalną) zmiennej SB2023 dla makroregionów.\nWykonaj wykres pudełkowy.\nWykonaj odpowiedni test określający czy istnieją statystycznie istotne różnice wartości stopy bezrobocia w 2023 roku między makroregionami. Określ także między, którymi regionami istnieją statystycznie istotne różnice.\n\n\nCzy stopa bezrobocia różni się między obszarami sklasyfikownami jako regiony przeważająco miejskie, pośrednie, przeważająco wiejskie (zmienna URBN_TYPE)?\n\n\nOblicz statystyki opisowe (średnią, medianę, wartość minimalmną i maksymalną) zmiennej SB2023 dla poszczególnych typów regionów\nWykonaj wykres pudełkowy.\nWykonaj odpowiedni test określający czy istnieją statystycznie istotne różnice wartości stopy bezrobocia w 2023 roku między różnymi typami regionów.\n\n\nStwórz mapę pokazującą rozkład przestrzenny zmiennej SB2023 w powiatach w Polsce. Dodaj także granice województw. Zapisz mapę do pliku png.\nStwórz mapę pokazującą rozkład przestrzenny zmiennej SB2023 w powiatach w Polsce. Dodaj także granice województw. Dodatkowo zaznacz punktami (kolor czerwony, wielkość punktu 1) 5% powiatów o najwyższej stopie bezrobocia w Polsce w 2023 roku. Zapisz mapę do pliku png.\nStwórz mapę pokazującą rozkład przestrzenny zmiennej URBN_TYPE. Przypisz następujące kolory: klasa 1 (obszary przeważająco miejskie) - czerwony, klasa 2 (regiony pośrednie) - pomarańczowy, klasa 3 (obszary przeważająco wiejskie) - zielony. Zapisz mapę do pliku png.\nStwórz mapę zmian pokazująca jak zmieniła się stopa bezrobocia między 2004 a 2023 rokiem. Jaką paletę wybierzesz do przedstawienia zmian? Zapisz mapę do pliku png.\n\n\n\nRozwiązanie\nJako rozwiązanie należy przedstawić:\n\nraport składający się z uzyksanych wykresów, map oraz krótkiego (pół strony) opisu oraz podsumowania wyników. Raport zapisz w pliku bezrobocie_eksploracja.html\nplik .qmd zawierający kod z rozwiązaniem zadania\nutworzone geopaczki\nutworzone pliki png zawierające mapy.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zadania samodzielne</span>"
    ]
  },
  {
    "objectID": "18_zadania_samodzielne.html#analiza-rozkładu-przestrzennego-danych-punktowych",
    "href": "18_zadania_samodzielne.html#analiza-rozkładu-przestrzennego-danych-punktowych",
    "title": "14  Zadania samodzielne",
    "section": "14.3 Analiza rozkładu przestrzennego danych punktowych",
    "text": "14.3 Analiza rozkładu przestrzennego danych punktowych\nProszę zapoznać się z treścią rozdziałów 5-7 oraz rozwiązać poniższe zadania.\n\nCel\nAnaliza rozkładu przestrzennego szpitali, przedszkoli oraz kościołów w obszarze miasta Poznania.\n\nJaki jest rozkład przestrzenny analizowanych danych? (czy wyniki wskazują na rozkład losowy, istnienie skupień w danych itp.)\nCzym się te rozkłady różnią?\n\n\n\nDane\nGeopaczka poznan_budynki.gpkg zawiera 4 warstwy w PUWG1992:\n\nwarstwę poligonową z lokalizacją budynków przedszkoli (nazwa warstwy: przedszkole),\nwarstwę poligonową z lokalizacją budynków kościołów (nazwa warstwy: kosciol),\nwarstwę poligonową z lokalizacją budynków szpitali (nazwa warstwy: szpital),\nwarstwę poligonową z granicą miasta Poznania (nazwa warstwy: poznan),.\n\n\n\nPrzygotowanie danych\n\nWczytać warstwy poligonowe do R. Aby wczytać jedną wartswę w geopaczki należy użyć argumentu layer.\n\n\nlibrary(sf)\npzn = read_sf(\"data/poznan_budynki.gpkg\", layer = 'poznan')\n\n\nlibrary(sf)\nkoscioly = read_sf(\"data/poznan_budynki.gpkg\", layer = 'kosciol')\n\n\nProszę zamienić warstwy poligonowe z lokalizacją przedszkoli oraz kościołów na warstwę punktową poprzez wyznaczenie centroidów budynków.\n\n\nkoscioly_punkty = st_centroid(koscioly)\n\n\nZapisać warstwy punktowe (koscioly_punkty, szpitale_punkty oraz przedszkola_punkty) w geopaczce punkty_poznan.\n\n\nwrite_sf(koscioly_punkty, \"data/punkty_poznan.gpkg\", layer = 'kosciol4')\n\n\nDla każdego typu obiektów proszę utworzyć obiekt klasy ppp. Jako obszar analizy proszę wskazać granicę miasta Poznania.\n\n\nlibrary(spatstat)\n#przekształcenie obiektu sf na obiekt owin definiujący okno (obszar analizy) dla obiektów ppp\npzn_owin = as.owin(pzn)\n#przekształcenie daych punktowych na obiekt ppp. \nkoscioly_ppp = as.ppp(st_geometry(koscioly_punkty), W = pzn_owin)\n\n\n\nZadanie 1: Statystyki centrograficzne\nDla każdego z typów obiektów (przedszkola, kościoły, szpitale):\n\nOblicz statystyki centrograficzne (średnią centralną, odległość standardową, elipsę odchylenia standardowego).\nZwizualizuj statystyki centrograficzne i lokalizacje obiektów punktowych na mapie.\n\n\n\nZadanie 2: Metody oparte na odległości\nDla każdego z typów obiektów (przedszkola, kościoły, szpitale):\n\nOblicz statystyki opisowe do najbliższego sąsiada. Przedstaw odległości na wykresie w postaci dystrybuanty. Zintrepretuj wyniki.\nOblicz wskaźnik Clarka - Evansa. Zinterpretuj wynik.\nWykorzystaj funkcję G do testowania istotności hipotezy dotyczącej rozkładu punktów. Przedstaw wyniki na wykresie. Zinterpretuj wyniki.\n\n\n\nZadanie 3: Metody oparte na analizie intensywności\nDla każdego z typów obiektów (przedszkola, kościoły, szpitale):\n\nOblicz średnią intensywność występowania każdego z typów obiektów na km2. Jako pole odniesienia (window) wykorzystaj granicę miasta Poznania.\n\nPodpowiedź: Dane są w PUWG1992. Aby obliczyć średnią intensywność na km2 obiekt klasy ppp należy przeskalować z m na km\n\n\n\nkonscioly_ppp2 = rescale(koscioly_ppp, 1000)\n\n\nWykorzystaj test zliczania w kwadratach do sprawdzenia hipotezy, czy lokalizacje mają rozkład losowy. Zintrepretuj wyniki testu.\nPrzedstaw na mapie w jaki sposób zmienia się intensywność lokalizacji punktów. Wykorzystaj taką samą liczbę kwadratów jak w teście zliczania w kwadratach.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zadania samodzielne</span>"
    ]
  },
  {
    "objectID": "18_zadania_samodzielne.html#dane-obszarowe",
    "href": "18_zadania_samodzielne.html#dane-obszarowe",
    "title": "14  Zadania samodzielne",
    "section": "14.4 Dane obszarowe",
    "text": "14.4 Dane obszarowe\nProszę zapoznać się z treścią rozdziału 8 i 9 oraz rozwiązać poniższe zadania.\n\nCel\nWykorzystanie miar autokorelacji do analizy wyników wyborów prezydenckich z 2015 roku.\n\n\nDane\nDo poniższych zadań zostanie użyty zbiór danych pol_pres15 z pakietu spDataLarge. Dane pol_pres15 zawierają wyniki wyborów prezydenckich z 2015 roku na poziomie gmin.\n\n#instalacja pakietu spDataLarge\ninstall.packages(\"spDataLarge\", repos = \"https://geocompr.r-universe.dev\")\n\n\nlibrary(spDataLarge)\ndata(pol_pres15, package = \"spDataLarge\")\n?pol_pres15\n\n\n\nZadanie\nPrzed wykonaniem zadania ustaw set.seed(1111)\n\nStwórz obiekt przestrzenny o nazwie wybory, zawierający zmienną name, II_turnout, oraz geometry. W dalszej części ćwiczenia dodaj do tego obiektu lokalne miary autokorelacji przestrzennej obliczone dla poszczególnych gmin. Wynikowy obiekt zapisz jako wybory.gpkg\nKolumna II_turnout zawiera informacje o frekwencji wyborczej w II turze wyborów prezydenckich w 2015 roku. Stwórz mapę przedstawiającą wartości tej zmiennej.\nStwórz obiekt pol_pres15_lw klasy \"listw\" zawierający listę sąsiedztwa z wagami przestrzennymi używając sąsiedztwa queen i binarnych wag.\nOkreśl czy tą zmienną charakteryzuje autokorelacja przestrzenna. Zastosuj do tego globalny test I Morana oraz test C Geary’ego.\nZlokalizuj hot i cold spoty zmiennej II_turnout wykorzystując w tym celu lokalną statystyke G Getisa & Orda. Zwizualizuj i zinterpretuj wyniki.\nZlokalizuj przestrzenne zgrupowania podobnych wartości zmiennej II_turnout wykorzystując w tym celu lokalny test Morana. Zwizualizuj wartości lokalnej statystyki Morana i zinterpretuj wyniki.\nUżywając wyników z poprzedniego zadania określ klastry “niskie-niskie”, “niskie-wysokie”, “wysokie-niskie”, oraz “wysokie-wysokie”. Zwizualizuj i zinterpretuj wyniki.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zadania samodzielne</span>"
    ]
  },
  {
    "objectID": "18_zadania_samodzielne.html#metody-interpolacji",
    "href": "18_zadania_samodzielne.html#metody-interpolacji",
    "title": "14  Zadania samodzielne",
    "section": "14.5 Metody interpolacji",
    "text": "14.5 Metody interpolacji\nProszę zapoznać się z treścią rozdziału 10 oraz rozwiązać poniższe zadania.\n\nDane\nW zadaniu zostaną wykorzystane dane z pliku punkty.csv.\n\n\nZadanie 1\nTabela zawiera zestawienie wartości punktów pomiarowych oraz odległości punktu od nieznanej lokalizacji.\n\n\n\nOdleglość d\nWartość zi\n\n\n\n\n100\n10\n\n\n150\n15\n\n\n300\n15\n\n\n400\n20\n\n\n500\n10\n\n\n600\n20\n\n\n1000\n10\n\n\n\nWykorzystując poniższą funkcję oblicz wartość komórki na podstawie wartości punktów pomiarowy zestawionych w tabeli:\n\nWykorzystując 3 najbliższe punkty i stosując wykładnik potęgowy p = 0\nWykorzystując 3 najbliższe punkty i stosując wykładnik potęgowy p = 1\nWykorzystując 3 najbliższe punkty i stosując wykładnik potęgowy p = 2\nWykorzystując 5 najbliższych punktów i stosując wykładnik potęgowy p = 0\nWykorzystując 5 najbliższych punktów i stosując wykładnik potęgowy p = 1\nWykorzystując 5 najbliższych punktów i stosując wykładnik potęgowy p = 2\n\n\nidw &lt;- function(wartosc, d, power = 2) {\n\n  # Obliczanie wag\n  wagi &lt;- 1 / (d^power)\n  # Obliczanie wartości z\n  estymowana_wartosc &lt;- sum(wagi * wartosc) / sum(wagi)\n\n  return(estymowana_wartosc)\n}\n\n\n\nZadanie 2\n\nStwórz siatkę interpolacyjną o rozdzielczości 200 metrów dla obszaru Suwalskiego Parku Krajobrazowego.\nKorzystając z danych punkty wykonaj interpolację zmiennej srtm używając:\n\n\nPoligonów Woronoja\nMetody IDW\nFunkcji wielomianowych\nFunkcji sklejanych\n\n\nPorównaj uzyskane wyniki poprzez ich wizualizację. Czym różnią się powyższe metody?\nWykonaj interpolację zmiennej temp metodą IDW sprawdzając różne parametry argumentu idp. W jaki sposób wpływa on na uzyskaną interpolację?",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zadania samodzielne</span>"
    ]
  },
  {
    "objectID": "18_zadania_samodzielne.html#geostatystyka-analiza-i-modelowanie-autokorelacji-przestrzennej",
    "href": "18_zadania_samodzielne.html#geostatystyka-analiza-i-modelowanie-autokorelacji-przestrzennej",
    "title": "14  Zadania samodzielne",
    "section": "14.6 Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej",
    "text": "14.6 Geostatystyka: Analiza i modelowanie autokorelacji przestrzennej\nProszę zapoznać się z treścią rozdziału 11 oraz rozwiązać poniższe zadania.\n\nCel analizy\nCelem ćwiczenia jest wykonanie analizy i modelowania struktury przestrzennej zmiennej srtm.\n\n\nDane\nW ćwiczeniu zostaną wykorzystane dane z pliku punkty_pref.gpkg\n\n\nCzęść 1: Analiza struktury przestrzennej\n\nStwórz wykres rozrzutu z przesunięciem dla zmiennej srtm dla odstępów od 0 do 5000 metrów co 625 metry. Co można odczytać z otrzymanego wykresu?\nWylicz odległość oraz wartość semiwariancji dla zmiennej srtm pomiędzy pierwszym i drugim, pierwszym i trzecim, oraz drugim i trzecim punktem. Zwizualizuj te trzy punkty. W jaki sposób można zinterpretować otrzymane wyniki wartości semiwariancji?\nStwórz chmurę semiwariogramu dla zmiennej srtm. W jaki sposób można zaobserwować na niej wartości odstające? Co one oznaczają?\nJaka jest liczba par obserwacji, średnia powierzchnia zajmowana przez jedną próbkę (w km2) oraz średnia odległość między punktami (w km)?\nStwórz semiwariogram zmiennej srtm. Ile par punktów posłużyło do wyliczenia pierwszego odstępu?\nZmodyfikuj powyższy semiwariogram, żeby jego maksymalny zasięg wynosił 3,5 kilometra.\n\n\n\nCzęść 2: Modelowanie struktury przestrzennej\n\nZbuduj modele semiwariogramu zmiennej srtm wykorzystując model sferyczny. Dopasuj model używając zarówno ręcznie ustalonych parametrów oraz funkcji fit.variogram(). Porównaj graficznie uzyskane modele.\nZbuduj modele semiwariogramu zmiennej srtm używając modelu nuggetowego, sferycznego, wykładniczego, gausowskiego i potęgowego. Porównaj graficznie uzyskane modele.\nStwórz złożony model semiwariogramu zmiennej srtm używając modelu nuggetowego i sferycznego.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zadania samodzielne</span>"
    ]
  },
  {
    "objectID": "18_zadania_samodzielne.html#geostatytyka-estymacja",
    "href": "18_zadania_samodzielne.html#geostatytyka-estymacja",
    "title": "14  Zadania samodzielne",
    "section": "14.7 Geostatytyka: Estymacja",
    "text": "14.7 Geostatytyka: Estymacja\nProszę zapoznać się z treścią rozdziału 12 oraz rozwiązać poniższe zadania.\n\nDane\nW ćwiczeniu zostaną wykorzystane dane zapisane w pliku punkty_pref.gpkg.\nNa podstawie pliku punkty_pref stwórz trzy obiekty\n\npunkty_pref1 zawierający wszystkie punkty,\npunkty_pref2 zawierający losowe 100 punktów,\npunkty_pref3 zawierający losowe 30 punktów.\n\nUwaga! Ustaw set.seed(123)\n\nlibrary(sf)\nset.seed(123)\npunkty_pref = read_sf(\"data/punkty_pref.gpkg\")\npunkty_pref1 = punkty_pref\npunkty_pref2 = punkty_pref[sample(nrow(punkty_pref), 100), ]\npunkty_pref3 = punkty_pref[sample(nrow(punkty_pref), 30), ]\n\n\n\nZadanie\n\nZbuduj optymalne modele semiwariogramu zmiennej srtm wykorzystując model sferyczny oraz nuggetowy dla trzech zbiorów danych - punkty_pref1, punkty_pref2, punkty_pref3. Porównaj graficznie uzyskane modele.\nStwórz siatkę interpolacyjną o rozdzielczości 200 metrów dla obszaru Suwalskiego Parku Krajobrazowego.\nWykorzystaj utworzoną siatkę interpolacyjną oraz modele semiwariogramu uzyskane na podstawie zbiorów danych punkty_pref1, punkty_pref2, punkty_pref3 do wykonania estymacji metodą krigingu prostego zmiennej srtm. Porównaj graficznie zarówno mapy estymacji jak i mapy wariancji. Opisz zaobserwowane różnice.\nWykorzystaj utworzoną siatkę interpolacyjną oraz model semiwariogramu uzyskany na podstawie zbioru danych punkty_pref1 do wykonania estymacji metodą krigingu zwykłego zmiennej srtm. Sprawdź jak wygląda wynik estymacji uwzględniając:\n\n\n10 najbliższych obserwacji,\n30 najbliższych obserwacji,\nobserwacje w odległości do 2 km.\n\n\nPorównaj graficznie zarówno mapy estymacji jak i mapy wariancji dla krigingu prostego oraz zwykłego dla danych punkty_pref1. Jakie można zauważyć podobieństwa a jakie różnice?\n\n\n\nRozwiązanie\nJako rozwiązanie zadania należy oddać:\n\nplik qmd zawierający rozwiązanie. Plik powinien składać się z następujących sekcji:\n\nrozwiązanie zadania (wraz z kodem) zawierające odpowiednie zestawienia numeryczne i graficzne\npodsumowanie zawierające interpretację wyników\n\nplik html z wygenerowanym raportem zawierającym zestawienia numeryczne, mapy, wykresy wraz z krótkim komentarzem uzyskanych wyników (zawierający także kod).",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Zadania samodzielne</span>"
    ]
  },
  {
    "objectID": "19_projekt.html",
    "href": "19_projekt.html",
    "title": "15  Projekt",
    "section": "",
    "text": "15.1 Opis projektu\nProjekt wykorzystuje dane o przestępczości oraz dane społeczno-ekonomiczne dla wybranego hrabstwa w Stanach Zjednoczonych dla 2020 roku.\nProjekt składa się z 3 części:",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Projekt</span>"
    ]
  },
  {
    "objectID": "19_projekt.html#opis-projektu",
    "href": "19_projekt.html#opis-projektu",
    "title": "15  Projekt",
    "section": "",
    "text": "Eksploracyjna analiza danych ma na celu sprawdzenie Czy istnieje zależność między poziomem przestępczości, a zmiennymi społeczno-ekonomicznymi?\nAnaliza rozkładu przestrzennego danych o przestępczości ma na celu określenie jaki rozkład charakteryzuje lokalizacje o przestępczości (czy jest to rozkład losowy? czy istnieją skupienia punktów?)\nAnaliza podobieństwa/różnic wartości wybranych zmiennych społeczno-ekonomicznych ma na celu określenie czy w analizowanych danych istnieje autokorelacja przestrzenna,gdzie znajdują się obszary o wysokich oraz niskich wartościach zmiennych? Czy tworzą one skupienia?",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Projekt</span>"
    ]
  },
  {
    "objectID": "19_projekt.html#rozwiązanie-projektu",
    "href": "19_projekt.html#rozwiązanie-projektu",
    "title": "15  Projekt",
    "section": "15.2 Rozwiązanie projektu",
    "text": "15.2 Rozwiązanie projektu\n\nRozwiązanie zadań wykonanych w ramach projektu należy przedstawić w formie pisemnego raportu zawierającego wyniki (wykresy, mapy, statystyki) wraz z krótkim podsumowaniem uzyskanych wyników.\nRaport proszę przygotować w dokumencie Quarto oraz wyeksportować do pliku html.\nProszę zastosować opcję ukrycia kodu (code-fold: true).\n\n\nformat:\n  html:\n    code-fold: true\n\n\n15.2.1 Struktura raportu\n\nTytuł raportu: Jak sformułujesz tytuł raportu?\nAutor raportu\nWprowadzenie\nDane\n\nrozdział można podzielić na 2 podrozdziały: Dane o przestępczości oraz Dane społeczon-ekonomiczne\nkrótki opis wykorzystanych danych\njakie były źródła danych?\ndla jakiego obszaru są analizowane dane?\njakie zmienne społeczno-ekonomiczne zostały uwzględnione w analizie?\njaki typ przestępczości jest analizowany?\n\nWpływ wskaźników społeczno-ekonomicznych na poziom przestępczości\n\nrozdział ten zawiera wyniki części 1: eksploracyjna analiza danych\njaki jest rozkład danych o przestępczości?\njaka jest zależność między poziomem przestępczości a zmiennymi społeczno-ekonimicznymi?\njak różnicuje się poziom przestępczości dla zmiennych skategoryzowanych określających ceny domów, wiek zabudowy oraz dochód?\n\nAnaliza rozkładu przestrzennego danych o przestępczości\n\nrozdział ten zawiera wyniki z części 2.\njaki jest rozkład danych o przestępczości?\n\nAnaliza podobieństwa/różnic wartości wybranych zmiennych społeczno-ekonomicznych\n\nrozdział ten zawiera wyniki z części 3.\n\nPodsumowanie\n\nczego dowiedzieliśmy się nt. analizowanego hrabstwa na podstawie wykonanych analiz w ramach części 1, 2 oraz 3.\n\n\n\n\n15.2.2 Ryciny i tabele w raporcie\n\nKażdy wykres, mapa oraz tabela zamieszczona w raporcie musi być podpisana.\n\nTabele podpisujemy nad tabelą, np. Tabela 1. Statystyki opisowe dla zmiennych społeczno-ekonomicznych.\nRyciny (wykresy i mapy) podpisujemy pod ryciną. Np. Ryc. 1. Rozkład lokalizacji włamań w obszarze hrabstwa Cook w 2020 roku.\n\nDo każdego wykresu, mapy oraz tabeli zamieszczonej w raporcie musimy się odwołać w tekście raportu.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Projekt</span>"
    ]
  },
  {
    "objectID": "19_projekt.html#dane",
    "href": "19_projekt.html#dane",
    "title": "15  Projekt",
    "section": "15.3 Dane",
    "text": "15.3 Dane\nUwaga! W projekcie zostaną wykorzystane dane z 2020 roku. Dane o przestępczości oraz dane społeczno-ekonomiczne należy pobrać dla roku 2020.\nPoniższa tabela zestawia nazwy miast (kolumna CITY) używane do pobrania danych o przestępczości oraz nazwy hrabstw (COUNTY) oraz kod hrabstwa (COUNTY_CODE) używane do pobrania danych społeczno-ekonomicznych.\n\ncities = read.csv(\"projekt/cities.csv\", colClasses = rep(\"character\", 5))\ncities\n\n             CITY          STATE STATE_CODE    COUNTY_NAME COUNTY_CODE\n1          Austin          Texas         48         Travis       48453\n2        Chicago        Illinois         17           Cook       17031\n3      Louisville       Kentucky         21      Jefferson       21111\n4   San Francisco     California         06  San Francisco       06075\n5         Memphis      Tennessee         47         Shelby       47157\n6         Seattle     Washington         53           King       53033\n7          Tucson        Arizona         04           Pima       04019\n8         Detroit       Michigan         26          Wayne       26163\n9     Kansas City       Missouri         29        Jackson       29095\n10    Los Angeles     California         06    Los Angeles       06037\n11      Nashville      Tennessee         47       Davidson       47037\n12 Virginia Beach       Virginia         51 Virginia Beach       51819\n13       St Louis       Missouri         29      St. Louis       29519\n14       New York       New York         36       New York       36061\n15       New York       New York         36          Bronx       36005\n16       New York       New York         36         Queens       36081\n17       New York       New York         36          Kings       36041\n18        Boston   Massachusetts         25        Suffolk       25025\n19       Houston           Texas         48         Harris       48201\n20    Minneapolis      Minnesota         27       Hennepin       27053\n21      Charlotte North Carolina         37    Mecklenburg       37119\n\n\n\n15.3.1 Dane o przestępczości\nPakiet crimedata udostępnia publicznie dostępne, zarejestrowane przez policję, otwarte dane o przestępczości w dużych miastach w Stanach Zjednoczonych, które są zawarte w Otwartej Bazie Danych o Przestępstwach (Crime Open Database, https://osf.io/zyaqn/).\n\nProszę pobrać dane dla wybranego miasta. Dostępne miasta znajdują się w powyższej tabeli w kolumnie CITY.\nProszę pobrać dane jako ‘tbl’.\nProszę wyselekcjonować dane dla jednego z poniższych typów przestępczości:\n\noffense_type: murder and nonnegligent manslaughter\noffense_type: residential burglary/breaking & entering\noffense_type: personal robbery\noffense_type: motor vehicle theft\n\nProszę pamiętąć, że po wyselekcjonowaniu danych należy użyć funkcji droplevels, aby usunąć dla zmiennych czynnikowych (factor) nie występujące w zbiorze poziomy zmiennej.\nWyselekcjonowane dane należy przekształcić na obiekt klasy sf.\nUżywając obiektu klasy sf, proszę wyselekcjonować wyłącznie dane dla wskazanego hrabstwa. Warstwa poligonowa zawierająca granice hrabstw znajduje się w pliku hrabstwa.gpkg. Plik projekt.xlsx zawiera zestawienie nazw miast dla których dostępne są dane wraz z nazwą hrabstwa stanowiącego centralną część miasta.\nWybrane dane zapisać w geopaczce o nazwie [miasto][hrabstwo][offense_type].gpkg, np. chicago_cook_burglary.gpkg\n\nDo pobrania i przygotowania danych można użyć poniższy kod.\n\nlibrary(crimedata)\nlibrary(sf)\nlibrary(dplyr)\n\ncrime &lt;- get_crime_data(\n    years = 2020, \n    cities = \"Chicago\", # TU PODAĆ NAZWĘ WYBRANEGO MIASTA. Patrz tabela powyżej. \n    type = \"core\",\n    quiet = TRUE,\n    output = \"tbl\"\n  )\n  \n#selekcja danych dla jednej z wybranych kategorii, np. \ncrime_sel &lt;- crime %&gt;% \n  filter(offense_type == 'residential burglary/breaking & entering') %&gt;% \n  droplevels()\n\n#przekształcenie danych na obiekt klasy sf\nlibrary(sf)\ncrime_sf  = st_as_sf(burglary, coords = c(\"longitude\", \"latitude\"), crs = \"EPSG:4326\")\n\ncounty = read_sf(\"data/projekt/hrabstwo.gpkg\") #podać sciezke do pliku zawierajacego granice hrabstwa. Patrz tabela powyżej - tabela zestawia nazwy miast oraz hrabstw\n#wyselekcjonowanie danych dla danego hrabstwa\ncrime_sf_hrabstwo = crime_sf[county, ]\n\n#zapisanie geopaczki\nwrite_sf(crime_sf_hrabstwo, \"chicago_burglary.gpkg\")\n\n\n\n15.3.2 Dane społeczno-ekonomiczne\nPakiet tidycensus umożliwia pobranie danych American Community Survey dla wybranego hrabstwa. W celu pobrania zestawu danych społeczno-ekonomicznych można wykorzystać skrypt “Projekt_przygotowanie_danych.qmd”\n\nProszę przygotować zestaw danych społeczno-ekonomicznych dla wskazanego roku dla wybranego hrabstwa wykorzystując 5-letnie dane American Community Survey (ACS).\nProszę pobrać dane dla tego samego hrabstwa, dla którego zostały pobrane dane o przestępczości.\nPobrane dane należy zapisać do geopaczki. (Skrypt “Projekt_przygotowanie_danych.qmd” pobiera, przygotowuje oraz zapisuje dane do geopaczki). W skrypcie wystarczy zdefiniować 3 parametry:\n\n\nstate_code = \"IL\" #PODAC KOD STANU. Patrz tabela powyżej (kolumna STATE_CODE)\ncounty_code = \"17031\" #PODAC KOD HRABSTWA Patrz tabela powyżej (kolumna COUNTY_CODE)\nyear = 2020 #pozostaje rok 2020.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Projekt</span>"
    ]
  },
  {
    "objectID": "19_projekt.html#zadania",
    "href": "19_projekt.html#zadania",
    "title": "15  Projekt",
    "section": "15.4 Zadania",
    "text": "15.4 Zadania",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Projekt</span>"
    ]
  },
  {
    "objectID": "19_projekt.html#część-1-eksploracja-danych",
    "href": "19_projekt.html#część-1-eksploracja-danych",
    "title": "15  Projekt",
    "section": "Część 1: Eksploracja danych",
    "text": "Część 1: Eksploracja danych\n\nCel analizy\nCelem analizy jest znalezienie czynników społeczno-ekonomicznych wpływających na poziom przestępczości dla wybranej kategorii przestępstw. Czy istnieje zależność między poziomem przestępczości, a zmiennymi społeczno-ekonomicznymi?\nW tym celu musimy przygotować warstwę poligonową z zestawem danych społeczno-ekonomicznych oraz osetkiem przestępstw w każdym obszarze spisowym.\n\n\nDane\n\nwczytać z geopaczki warstwę punktową z lokalizacją przestępstw oraz przypisać ją do obiektu crime_data\nwczytać z geopaczki warstwę poligonową zawierająca zestaw danych społeczno-ekonomicznych oraz przypisać ją do obiektu socio_data\n\n\nPrzygotowanie danych społeczno-ekonomicznych\nProszę do zestawu danych społeczno-ekonomicznych dodać 3 zmienne jakościowe:\n\nINCOME_CAT zawierającą 3 kategorie wyróżnione używając zmiennej MEDIAN_INCOME:\n\nlow - wartości dochodów niższe od kwartyla 1.\nmedium - wartości dochodów między 1 i 3 kwartylem.\nhigh - wartości dochodów wyższe do 3. kwartyla.\n\nHOME_VALUES_CAT zawierającą 3 kategorie wyróżnione używając zmiennej MEDIAN_HOME_VALUE:\n\nlow - wartości cen domów niższe od kwartyla 1.\nmedium - wartości cen domów między 1 i 3 kwartylem.\nhigh - wartości cen domów wyższe do 3. kwartyla.\n\nBUILT_YEAR_CAT zawierającą 3 kategorie wyróżnione używając zmiennej MEDIAN_BUILT_YEAR:\n\nold - wybudowane przed 1950,\nmedium - wybudowane między 1950 a 2000,\nnew - wybudowane po 2000.\n\n\nPoniższy kod można wykorzystać do przygotowania 3 zmiennych jakościowych.\n\n# INCOME_CAT\nq1mi = quantile(socio_data$MEDIAN_INCOME, probs = 0.25, na.rm = TRUE)\nq3mi = quantile(socio_data$MEDIAN_INCOME, probs = 0.75, na.rm = TRUE)\nq1mhv = quantile(socio_data$MEDIAN_HOME_VALUE, probs = 0.25, na.rm = TRUE)\nq3mhv = quantile(socio_data$MEDIAN_HOME_VALUE, probs = 0.75, na.rm = TRUE)\n\nsocio_data  &lt;- socio_data %&gt;% \n  mutate(INCOME_CAT=cut(MEDIAN_INCOME, breaks=c(-Inf, q1mi, q3mi, Inf), labels=c(\"low\",\"medium\",\"high\")),\n         HOME_VALUES_CAT=cut(MEDIAN_HOME_VALUE, breaks=c(-Inf, q1mvh, q3mhv, Inf), labels=c(\"low\",\"medium\",\"high\")),\n         BUILT_YEAR_CAT=cut(MEDIAN_BUILT_YEAR, breaks=c(-Inf,1950, 2000, Inf), labels=c(\"old\",\"medium\",\"new\")))\n\n\n\nPrzygotowanie danych do analizy\n\nujednolicenie układów współrzędnych dla obu warstw\n\nLokalizacja przestępstw jest w układzie WGS84, warstwę trzeba przekształcić do układu warstwy z danymi społeczno-ekonomicznymi.\n\nz warstwy zawierającej dane społeczno-ekonomiczne należy wyselekcjonować obszary spisowe, w których zlokalizowane są przestępstwa.\nzliczenie przestępstw w obszarach spisowych\nobliczenie odsetka przestępstw na 1000 mieszkańców (ang. crime rates)\n\nW wyniku przygotowania danych powstanie warstwa zawierająca informacje społeczno-ekonomiczne oraz odsetek przestępstw na 1000 mieszkańców w obszarach spisowych. Warstwę tą należy zapisac jako socio_[hrabstwo]*[offense_type].gpkg, np. socio_cook_burglary.gpkg\n\n\n\nEksploracyjna analiza danych przestrzennych\n\nEksploracyjna nieprzestrzenna analiza danych\nWykorzystując materiały zawarte w przykładzie 2 w dokumencie “Eksploracyjna analiza danych przestrzennych” proszę przeprowadzić ekspoloracyjną analizę danych.\n\nProszę obliczyć statystyki opisowe\nProszę scharakteryzować rozkład zmiennej CRIME_RATE - odsetek przestępczości na 1000 mieszkańców.\nProszę przeanalizować czy istnieje zależność między odsetkiem przestępczości (zmienna CRIME_RATE), a zmiennymi społeczno-ekonomicznymi.\n\nProszę obliczyć współczynnik korelacji między zmiennymi ilościowymi a odsetkiem przestępczości. Proszę wybrać zmienne dla których współczynnik osiąga wartość większą od +/- 0,5 oraz zwizualizować zależność dla tych zmiennych w postaci macierzy wykresów rozrzutu. Jakie to są zmienne?\nProszę przeanalizować czy wartość odsetka przestępczości (CRIME_RATE) zmienia się w klasach zmiennych INCOME_CAT, HOME_VALUES_CAT, oraz BUILT_YEAR_CAT.\n\n\n\n\nEksploracyjna przestrzenna analiza danych\nProszę przedstawić na mapie odsetek przestępczości, odsetek poszczególnych grup rasowo-etnicznych, poziom wykształcenia, oraz zmienne skategoryzowane dotyczące poziomu dochodów, wieku zabudowy, oraz cen domów.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Projekt</span>"
    ]
  },
  {
    "objectID": "19_projekt.html#część-2.-analiza-rozkładu-przestrzennego-danych-o-przestępczości",
    "href": "19_projekt.html#część-2.-analiza-rozkładu-przestrzennego-danych-o-przestępczości",
    "title": "15  Projekt",
    "section": "Część 2. Analiza rozkładu przestrzennego danych o przestępczości",
    "text": "Część 2. Analiza rozkładu przestrzennego danych o przestępczości\nProszę przeprowadzić analizę rozkładu danych o przestępczości w wybranym obszarze.\n\nOblicz statystyki centrograficzne (średnią centralną, odległość standardową, elipsę odchylenia standardowego) dla danych o przestępczości. Zwizualizuj statystyki centrograficzne i lokalizacje przestępstw na mapie.\nOblicz statystyki opisowe do najbliższego sąsiada. Przedstaw odległości na wykresie w postaci dystrybuanty. Zintrepretuj wyniki.\nOblicz wskaźnik Clarka - Evansa. Zinterpretuj wynik.\nWykorzystaj funkcję G do testowania istotności hipotezy dotyczącej rozkładu punktów. Przedstaw wyniki na wykresie. Zinterpretuj wyniki.\nOblicz średnią intensywność przestępstw. Jako pole odniesienia (window) wykorzystaj granicę hrabstwa.\nOblicz średnią intensywność przestępstw. Jako pole odniesienia (window) wykorzystaj zasięg (bounding box) warstwy z punktami. Jak zmieniła się wartość intensywności w zależności od przyjętej granicy obszaru?\nWykorzystaj test zliczania w kwadratach do sprawdzenia hipotezy, czy lokalizacje przestępczości mają rozkład losowy. Zintrepretuj wyniki testu.\nPrzedstaw na mapie w jaki sposób zmienia się intensywność lokalizacji przestępstw. Wykorzystaj taką samą liczbę kwadratów jak w teście zliczania w kwdratach.\n\nCzego dowiedzieliśmy się na temat rozkładu przestrzennego przestępczości z wykonanych analiz (czy wyniki wskazują na rozkład losowy, istnenie skupień w danych itp.)",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Projekt</span>"
    ]
  },
  {
    "objectID": "19_projekt.html#część-3.-analiza-podobieństwaróżnic-wartości-wybranych-zmiennych-społeczno-ekonomicznych",
    "href": "19_projekt.html#część-3.-analiza-podobieństwaróżnic-wartości-wybranych-zmiennych-społeczno-ekonomicznych",
    "title": "15  Projekt",
    "section": "Część 3. Analiza podobieństwa/różnic wartości wybranych zmiennych społeczno-ekonomicznych",
    "text": "Część 3. Analiza podobieństwa/różnic wartości wybranych zmiennych społeczno-ekonomicznych\n\nStwórz obiekt przestrzenny zawierający 3 wybrane zmienne społeczno-ekonomiczne.\nStwórz obiekt socio_lw klasy “listw” zawierający listę sąsiedztwa z wagami przestrzennymi używając sąsiedztwa queen i binarnych wag.\nOkreśl czy wybrane zmienne charakteryzuje autokorelacja przestrzenna. Zastosuj do tego globalny test I Morana oraz test C Geary’ego.\nZlokalizuj hot i cold spoty dla 3 analizowanych zmiennych wykorzystując w tym celu lokalną statystyke G Getisa & Orda. Zwizualizuj i zinterpretuj wyniki.\nZlokalizuj przestrzenne zgrupowania podobnych wartości dla 3 analizowanych zmiennych wykorzystując w tym celu lokalny test Morana. Zwizualizuj i zinterpretuj wyniki.\nUżywając wyników z poprzedniego zadania określ klastry “niskie-niskie”, “niskie-wysokie”, “wysokie-niskie”, oraz “wysokie-wysokie”. Zwizualizuj i zinterpretuj wyniki dla 3 analizowanych zmiennych.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Projekt</span>"
    ]
  },
  {
    "objectID": "20_projekt_geostatystyka.html",
    "href": "20_projekt_geostatystyka.html",
    "title": "16  Projekt 2: Geostatytyka",
    "section": "",
    "text": "16.1 Zadanie\nProjekt ma na celu wykonanie estymacji geostatystycznych oraz ocenę ich jakości. W ćwiczeniu zostaną wykorzystane dane zapisane w pliku punkty_pref.gpkg.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Projekt 2: Geostatytyka</span>"
    ]
  },
  {
    "objectID": "20_projekt_geostatystyka.html#zadanie",
    "href": "20_projekt_geostatystyka.html#zadanie",
    "title": "16  Projekt 2: Geostatytyka",
    "section": "",
    "text": "Podziel obiekt punkty_pref w taki sposób aby 70% danych należało zbioru treningowego, a 30 % danych do zbioru testowego. Zwizualizuj oba nowe zbiory danych.\nStwórz optymalne model zmiennej srtm na podstawie punktów ze zbioru treningowego.\nWykorzystując zbiór testowy wykonaj estymacje 3 metodami:\n\n\nmetodą kriginu prostego\nmetodą krigingu zwykłego używając 30 najbliższych punktów,\nmetodą krigingu zwykłego używając punktów położonych w odległości do 2km.\n\n\nPorównaj wyniki estymacji dla 3 metod korzystając ze statystyk jakości estymacji oraz wizualizacji jakości estymacji. Który ze stworzonych modeli można uznać za najlepszy? Dlaczego?\nWybierz najlepszą metodę estymacji oraz wykonaj tą metodą estymację dla siatki obejmującej obszar analizy w rozdzielczości 50m.",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Projekt 2: Geostatytyka</span>"
    ]
  },
  {
    "objectID": "20_projekt_geostatystyka.html#rozwiązanie",
    "href": "20_projekt_geostatystyka.html#rozwiązanie",
    "title": "16  Projekt 2: Geostatytyka",
    "section": "16.2 Rozwiązanie",
    "text": "16.2 Rozwiązanie\nJako rozwiązanie zadania należy oddać:\n\nplik qmd zawierający rozwiązanie. Plik powinien składać się z następujących sekcji:\n\nrozwiązanie zadania (wraz z kodem) zawierające odpowiednie zestawienia numeryczne i graficzne\npodsumowanie zawierające interpretację wyników\n\nplik html z wygenerowanym raportem zawierającym zestawienia numeryczne, mapy, wykresy wraz z krótkim komentarzem uzyskanych wyników (zawierający także kod).",
    "crumbs": [
      "ZADANIA",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Projekt 2: Geostatytyka</span>"
    ]
  }
]